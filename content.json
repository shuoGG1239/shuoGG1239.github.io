{"posts":[{"title":"axios的ES module (esm)","text":"背景 刚好遇到某个场景需要用到es module, axios用习惯了, 不过axios官方没有esm版本 https://github.com/axios/axios/issues/1879 解决 可以用第三方: https://github.com/bundled-es-modules/axios 使用起来很简单, 直接用里面的axios.js即可 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ESM-test&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script type=&quot;module&quot;&gt; import axios from './axios.js' axios.get('http://127.0.0.1:8888/shuogg').then(resp =&gt; { console.log(resp.data) }).catch(e =&gt; { console.log(e) })&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2020/06/05/axio_esm/"},{"title":"Jetbrains Clion官方支持了Stm32的项目搭建, 说下感想","text":"背景 得知Clion 2019.1之后的版本官方直接支持Stm32项目的创建, 遂怀揣激动之心准备一试... 吐槽 照着别人的教程, 一顿操作猛如虎, 一会捣鼓OpenOCD, 一会捣鼓arm-none-eabi-gcc... ...说实话, 过程挺麻烦的, 会遇到一些坑 手头上只有一块老stm32的核心板还有一个Jlink, 烧写调试也只能靠Jlink. 结果捣鼓了老半天, Jlink这块没办法打通, 即没办法用Jlink愉快地Debug, 遂放弃 结论 现阶段还不完善, 该用keil的还是得用keil (当然也有可能只是我的搭建姿势有问题, 望指教) 期待未来某一天Clion能够拳打Keil脚踩IAR 参考教程 Clion下开发STM32 用clion自带的嵌入式开发功能和stm32cubeMX开发stm32!!!","link":"/2020/05/05/clion_stm32/"},{"title":"Py小玩具-简易取色器","text":"简单的拾色器 最近遇到几次取屏幕某处颜色的场景, 用ps去取色又觉得有点麻烦(步骤太多我懒), 索性自己做一个简单的拾色器 功能极简单就是取屏幕某处的色号, 按下空格把颜色记录下来... 效果 colorcatcher.gif 思路 定时截取屏幕然后取下鼠标位置的像素颜色 1234567891011121314def catch(self): x = QCursor.pos().x() y = QCursor.pos().y() pixmap = QGuiApplication.primaryScreen().grabWindow(QApplication.desktop().winId(), x, y, 1, 1) if not pixmap.isNull(): image = pixmap.toImage() if not image.isNull(): if (image.valid(0, 0)): color = QColor(image.pixel(0, 0)) r, g, b, _ = color.getRgb() self.nowColor = color self.ui.lineEditMove.setText('(%d, %d, %d) %s' % (r, g, b, color.name().upper())) self.ui.lineEditMove.setStyleSheet('QLineEdit{border:2px solid %s;}' % (color.name())) 依赖 PyQt5 代码 https://github.com/shuoGG1239/ColorCatcher","link":"/2019/06/09/color_catcher/"},{"title":"Py小玩具-简易截图","text":"简介 有时没开微信或QQ的时候想立即截个图啥的挺蛋疼的, 故自己捣鼓一个 简单干净快速, 无界面, 运行即截图 使用方式 运行easyshot.py直接开启区域截图, 截完图自动保存到桌面, 然后退出进程 截图的时候框选完区域后可以双击或Enter完成截图, 中途可以按Esc放弃截图 环境 python3 pyQt5 代码 https://github.com/shuoGG1239/EasyScreenShot","link":"/2020/05/10/easyscreenshot/"},{"title":"gh-ost源码分析","text":"简述 之前用到了gh-ost做大表改表工具, 回过来看看源码, 本篇为阅读源码的笔记 源码信息 源码版本: 源码仓库: https://github.com/github/gh-ost gh-ost原理 gh-ost 首先连接到主库上，根据 alter 语句创建幽灵表，然后作为一个\"备库\"连接到其中一个真正的备库上(默认设置,想连到master也行) 一边在主库上拷贝已有的数据到幽灵表，一边从备库上拉取增量数据的 binlog，然后不断的把 binlog 应用回主库 图中 cut-over 是最后一步，锁住主库的源表，等待 binlog 应用完毕，然后替换 gh-ost 表为源表 gh-ost 在执行中，会在原本的 binlog event 里面增加以下 hint 和心跳包，用来控制整个流程的进度，检测状态等 gh-ost的改表流程 检查有没有外键和触发器。 检查表的主键信息。 检查是否主库或从库，是否开启log_slave_updates，以及binlog信息 检查gho和del结尾的临时表是否存在 创建ghc结尾的表，存数据迁移的信息，以及binlog信息等 初始化stream的连接,添加binlog的监听 创建gho结尾的临时表，执行DDL在gho结尾的临时表上 开启事务，按照主键id把源表数据写入到gho结尾的表上，再提交，以及binlog apply。 lock源表，rename 表：rename 源表 to 源_del表，gho表 to 源表。(这个过程叫cut-over) 清理ghc表。 关于v1.1.6修复的时区问题 在_gho表执行sql的session和binlog读取时指定的时区不一致导致 从binlogEvent读取的时间结构体是带了时区的, 该时区是由BinlogParser.timestampStringLocation指定, 在转换成query时会用timestamp结合时区生成时间String 强调: 不管是mysql底层还是binlog中, timestamp是不带时区的, 就是4个bytes; github.com/go-mysql-org/go-mysql在时区强制指定utc, 新版本变成可配置并默认为系统时区, 所以是go-mysql没考虑兼容性导致 源码 /base包: 相当于config, 处理配置信息和日志工具 /sql包: 相当于sql_parser, 处理sql解析的工具包 /mysql包: 相当于mysql相关的util包 /binlog包: 仅仅是对replication.BinlogSyncer的封装, 最后将replication.RowsEvent封装成BinlogEntry塞到EventsStreamer.eventsChannel里面 /logic/server.go 提供接口, 主要用于动态设置一些运行参数 /logic/hooks.go: 执行hook. 按规定的执行程序名字, 将执行程序放入指定目录, 后面会根据事件执行这些执行程序 /logic/streamer.go: 在上面binlog包的基础上再封装一层listener, listener处理上面提到的EventsStreamer.eventsChannel接收的BinlogEntry /logic/inspect.go: 连接slave, 获取实例的基础信息如表结构, 表大小等, 检查改表是否符合迁移条件 /logic/throttler.go: 限流器, 调用throttle()会卡住以实现限流 /logic/applier.go gho和ghc的处理包括cutOver, 提供实现. 调用都在Migrator ApplyDMLEventQueries(dmlEvents ): 将binlogEvent转为query, 然后在_gho表执行 /logic/migrator.go 主流程. 上述各个模块提供的方法会在migrator中使用, 完成整个改表流程","link":"/2024/05/22/gh-ost%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"go module笔记与源码分析","text":"简介 零零散散的关于go module的笔记, 通过源码来理解这些点 go mod源码位置 仓库位置: https://github.com/golang/go/tree/go1.16.6/src/cmd/go/internal/modcmd 相对位置: cmd/go/internal/modcmd cmd/go/internal/modfetch cmd/go/internal/modget cmd/go/internal/modload 关于版本选择 同一个大版本取最大版本号 main依赖了(A1.1.0, B1.2.0), B1.2.0依赖了(A1.0.0), 则最终构建阶段大家都用A1.1.0编译 版本从信息从/@v/list获取, 如果为空则取走@latest获取最新版本号, 在拿最新版本号取拉包 官方版本规则 核心逻辑 1234567891011121314151617181920func (p *proxyRepo) Versions(prefix string) ([]string, error) { data, err := p.getBytes(&quot;@v/list&quot;) // 获取list文件内容 if err != nil { return nil, p.versionError(&quot;&quot;, err) } var list []string for _, line := range strings.Split(string(data), &quot;\\n&quot;) { f := strings.Fields(line) if len(f) &gt;= 1 &amp;&amp; semver.IsValid(f[0]) &amp;&amp; strings.HasPrefix(f[0], prefix) &amp;&amp; !IsPseudoVersion(f[0]) { list = append(list, f[0]) } } SortVersions(list) // 对list里面的版本进行排序 return list, nil}// IsPseudoVersion reports whether v is a pseudo-version.func IsPseudoVersion(v string) bool { return strings.Count(v, &quot;-&quot;) &gt;= 2 &amp;&amp; semver.IsValid(v) &amp;&amp; pseudoVersionRE.MatchString(v)} go private private时不会走proxy和checksumDB 1234567// go/internal/modfetch/sumdb.gofunc useSumDB(mod module.Version) bool { return cfg.GOSUMDB != &quot;off&quot; &amp;&amp; !module.MatchPrefixPatterns(cfg.GONOSUMDB, mod.Path) // cfg.GONOSUMDB里面包含了GOPRIVATE}// cmd/go/internal/cfg/cfg.goGONOSUMDB = envOr(&quot;GONOSUMDB&quot;, GOPRIVATE) @latest v0.0.5 &gt; v0.0.5-alpha &gt; v0.0.4 123456789101112131415161718192021222324// 版本比较的源码: src/cmd/vendor/golang.org/x/mod/semver/semver.gofunc Compare(v, w string) int { pv, ok1 := parse(v) pw, ok2 := parse(w) if !ok1 &amp;&amp; !ok2 { return 0 } if !ok1 { return -1 } if !ok2 { return +1 } if c := compareInt(pv.major, pw.major); c != 0 { return c } if c := compareInt(pv.minor, pw.minor); c != 0 { return c } if c := compareInt(pv.patch, pw.patch); c != 0 { return c } return comparePrerelease(pv.prerelease, pw.prerelease)} indirect main依赖了A, 但是A依赖了B但go.mod里没有require B, 则A的go.mod会自动加上B indirect 此时main的go.mod强行require上B, 则B indirect将消失 exclude 跳过某个版本 (之后一般gomod会自动使用比跳过版本更高的版本) 例如: \"require github.com/google/uuid v1.1.0\", 最后tidy后require里自动变成\"github.com/google/uuid v1.1.1\" 跟replace一样, 仅main module时生效 不大实用, 一般直接用replace即可 url GET \\(base/\\)module/@v/list https://goproxy.io/github.com/gin-gonic/gin/@v/v1.4.0.mod https://goproxy.io/github.com/gin-gonic/gin/@v/ https://goproxy.io/github.com/gin-gonic/gin/@v/list //latest是根据这个list拉取最大的版本 https://goproxy.io/github.com/gin-gonic/gin/@latest go.sum https://sum.golang.org/lookup/golang.org/x/sync@v0.0.0-20181221193216-37e7f081c4d4 checksum源码: go/src搜索 checkModSum(mod, hash) checksumDB源码: go/src搜索 checkSumDB(mod, h) commit version go get github.com/pingcap/parser@659821e go get github.com/pingcap/parser@latest go get github.com/pingcap/parser@feature-lstest","link":"/2021/06/09/go%20module%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"golang&#x2F;net包与epoll","text":"net包与epoll linux下go的网络包底层如tcp也是采用epoll来实现, 你可以从Accept方法一路追下去, 追到尽头你会看到internal/poll/fd_poll_runtime.go里面这些在runtime实现的方法: 123456789func runtime_pollServerInit()func runtime_pollOpen(fd uintptr) (uintptr, int)func runtime_pollClose(ctx uintptr)func runtime_pollWait(ctx uintptr, mode int) intfunc runtime_pollWaitCanceled(ctx uintptr, mode int) intfunc runtime_pollReset(ctx uintptr, mode int) intfunc runtime_pollSetDeadline(ctx uintptr, d int64, mode int)func runtime_pollUnblock(ctx uintptr)func runtime_isPollServerDescriptor(fd uintptr) bool 此时到src/runtime/netpoll.go就能看到上述这些方法的实现, 再往下追下去就可以看到各个平台的具体实现了, 如netpoll_epoll.go netpoll_kqueue.go netpoll_windows.go, 看到netpoll_epoll.go里面的epollcreate, epollctl, epollwait了吧, 多么熟悉的几个函数! goroutine与epoll 虽然net包底层用epoll实现了, 但是实际我们在用tcp还是开goroutine来serve net包就是推荐我们用goroutine来玩tcp, 应对大部分场景妥妥的 面对比较变态的场景并发量贼高时, goroutine尽管只有消耗2k~8k的栈空间, 连接一多还是耗不起, 此时就只能用一些黑魔法来使用epoll了 具体怎么玩可以参照 https://github.com/mailru/easygo","link":"/2020/06/07/go_and_epoll/"},{"title":"go源码阅读:database&#x2F;sql包","text":"简介 database/sql最主要还是实现了连接池逻辑 go源码版本: v1.16 源码仓库: https://github.com/golang/go/tree/go1.16.6/src/database/sql sql.DB的一些关键逻辑 Open会返回DB对象并开启一条connectionOpener线程 connectionOpener主要处理下面提到的\"当前连接数大于maxOpen会陷入等待\"的连接资源请求 DB的核心方法: conn(ctx context.Context, strategy connReuseStrategy) (*driverConn, error) 优先返回连接池里的连接 当前连接数大于maxOpen会陷入等待, 等待取决于ctx, 也即由调用方控制 其余情况则返回一个新连接 DB的连接池: freeConn []*driverConn DB的核心方法: putConnDBLocked(conn, err) bool 如果当前连接数大于maxOpen则直接返回false, 上层看到false则直接关闭该连接conn 优先满足正在等待连接资源请求的(connRequests是一个map,可以看出请求连接资源并无优先级的说法) 如果MaxIdleConns &lt; len(freeConn), 即连接池满了, 则直接返回false, 上层看到false则直接关闭该连接conn 否则丢到连接池freeConn中 清理线程(connectionCleaner): SetConnMaxLifetime,SetConnMaxIdleTime时才会去起唯一的一条清理线程 清理线程定期清理连接池freeConn的连接, 根据maxLifetime和createAt清理, 也根据maxIdleTime和returnedAt清理 有趣的是清理线程并不理会MaxOpen和MaxIdleConns是多少, 只关注MaxLifetime和MaxIdleTime, 反正过期了就清理 sql执行方法如Query,Exec,Ping等, 都会先去调conn获取连接, 再用其连接执行sql, 最后将putConnDBLocked(conn), (query是等rows全部scan完close再putConnDBLocked(conn)) 关于resetSession最终的去处, 总的来说就是reset了个寂寞, 最终居然只是conn.SetReadDeadline(time.Time{})? 123456789101112131415161718192021222324252627282930313233343536373839// go-sql-driver/mysql/connection.go里面的// Write packet buffer 'data'func (mc *mysqlConn) writePacket(data []byte) error { pktLen := len(data) - 4 if pktLen &gt; mc.maxAllowedPacket { return ErrPktTooLarge } // Perform a stale connection check. We only perform this check for // the first query on a connection that has been checked out of the // connection pool: a fresh connection from the pool is more likely // to be stale, and it has not performed any previous writes that // could cause data corruption, so it's safe to return ErrBadConn // if the check fails. if mc.reset { mc.reset = false conn := mc.netConn if mc.rawConn != nil { conn = mc.rawConn } var err error // If this connection has a ReadTimeout which we've been setting on // reads, reset it to its default value before we attempt a non-blocking // read, otherwise the scheduler will just time us out before we can read if mc.cfg.ReadTimeout != 0 { err = conn.SetReadDeadline(time.Time{}) } if err == nil &amp;&amp; mc.cfg.CheckConnLiveness { err = connCheck(conn) } if err != nil { errLog.Print(&quot;closing bad idle connection: &quot;, err) mc.Close() return driver.ErrBadConn } } ... ...} 关于statement 关于query带?的sql的逻辑: func (db *DB) queryDC: 对于带了args的query, 且InterpolateParams=false, 则driver会返回driver.ErrSkip, 此时会走到si, err = ctxDriverPrepare(ctx, dc.ci, query), 做完prepare与ctxDriverStmtQuery, 将si放到Rows里面, Rows读完Close后si会跟着Close","link":"/2022/03/27/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_database-sql%E5%8C%85/"},{"title":"go源码阅读:net&#x2F;http的Transport","text":"简介 http.Transport这样的高频使用模块, 源码肯定得看看 go源码版本: v1.16 源码仓库: https://github.com/golang/go/blob/go1.16.6/src/net/http/transport.go http.Transport的关键逻辑 Transport是RoundTripper接口的实现: func RoundTrip(req Request) (Response, error) Transport对外只提供方法: func RoundTrip(req Request) (Response, error) Transport的内部对象idleLRU connLRU写得不错, 简单实现了LRU http.Client就是在Tranport上简单封装一层 Transport就是一个连接池, 池子里面放着persistConn连接对象(idleConn map[connectMethodKey][]*persistConn) queueForIdleConn: 根据请求的connectMethodKey从t.idleConn获取一个[]*persistConn切片， 并从切片中，根据算法获取一个有效的空闲连接。如果未获取到空闲连接，则将wantConn结构体放入t.idleConnWait[w.key]等待队列 连接释放逻辑在 (t Transport) tryPutIdleConn(pconn persistConn) 哪些情况才回去调 tryPutIdleConn: 大部分的异常情况 responseBody read完: 代码详细见 case bodyEOF := &lt;-waitForBodyRead dialConnFor: 会调用t.dialConn获取一个真正的*persistConn。并将这个连接传递给w, 如果w已经获取到了连接，则会传递失败，此时调用t.putOrCloseIdleConn将连接放回空闲连接池。 dialConn: 调用t.dial(ctx, \"tcp\", cm.addr())创建TCP连接并将其赋予刚new的persistConn 如果是https的请求，则对请求建立安全的tls传输通道 为persistConn创建读写buffer，如果用户没有自定义读写buffer的大小，读写bufffer的大小默认为4096 执行go pconn.readLoop()和go pconn.writeLoop()开启读写循环然后返回连接 dialConn里面这段代码是开启http2的核心 12345678910111213// 当client和server都支持http2时，s.NegotiatedProtocol的值为&quot;h2&quot;且s.NegotiatedProtocolIsMutual的值为true// pconn.tlsState是在pconn.addTLS中附加的// 所以是否支持http2是在tls握手时得知的, 因此http2时强制要求httpsif s := pconn.tlsState; s != nil &amp;&amp; s.NegotiatedProtocolIsMutual &amp;&amp; s.NegotiatedProtocol != &quot;&quot; { if next, ok := t.TLSNextProto[s.NegotiatedProtocol]; ok { alt := next(cm.targetAddr, pconn.conn.(*tls.Conn)) if e, ok := alt.(erringRoundTripper); ok { // pconn.conn was closed by next (http2configureTransports.upgradeFn). return nil, e.RoundTripErr() } return &amp;persistConn{t: t, cacheKey: pconn.cacheKey, alt: alt}, nil }} 关于persistConn persistConn是在给\"conn net.Conn\"包一层 readLoop: for循环, 不停等待新的requestAndChan(由roundTrip发起), response塞回requestAndChan(rc.ch &lt;- responseAndError{res: resp}); 确认读完之后调tryPutIdleConn放回Transport的连接池 只有当调用方完整的读取了响应，该连接才能够被复用。 因此在http1.1中，1个连接上的请求，只有等前一个请求处理完之后才能继续下一个请求。 如果前面的请求处理较慢， 则后面的请求必须等待， 这就是http1.1中的线头阻塞 所以就算你不关心response的body, 也必须把body读完以保持连接的复用, 可以如下处理 io.CopyN(ioutil.Discard, resp.Body, 2 &lt;&lt; 10) resp.Body.Close() writeLoop: for循环, 不停等待新的writeRequest, 写完发信号给pc.writeErrCh和wr.ch, 出错了会关闭该persistConn并结束writeLoop的循环, 否则继续等待新的writeRequest roundTrip: 在for循环里面等待本次roundTrip的各种信号, 如来自writeLoop的写完成信号, pcClosed, cancelChan, response结果信号等, 收到response或者出错则结束循环. 只有在出错的时候才会pc.close; roundTrip里面没有处理连接池的逻辑 关于http.Response http/response.go: 核心函数 func ReadResponse(r bufio.Reader, req Request) (*Response, error) 关于HeaderTimeout, 源码可看出是读完所有header才算header读结束了(blank line), HeaderTimeout是header读结束的timeout response.wroteHeader: writeHeader里面会将wroteHeader置为true func (cw *chunkWriter) writeHeader(p []byte)的最后一行是: w.conn.bufw.Write(crlf) response的flush本质就是writeHeader(nil), 也就是最后也会w.conn.bufw.Write(crlf) 关于http.DefaultTransport 123456789101112var DefaultTransport RoundTripper = &amp;Transport{ Proxy: ProxyFromEnvironment, // &quot;HTTP_PROXY&quot;,&quot;http_proxy&quot;,&quot;HTTPS_PROXY&quot;,&quot;https_proxy&quot;,&quot;NO_PROXY&quot;,&quot;no_proxy&quot; DialContext: (&amp;net.Dialer{ Timeout: 30 * time.Second, KeepAlive: 30 * time.Second, }).DialContext, ForceAttemptHTTP2: true, MaxIdleConns: 100, IdleConnTimeout: 90 * time.Second, TLSHandshakeTimeout: 10 * time.Second, ExpectContinueTimeout: 1 * time.Second,}","link":"/2022/03/20/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_net-http-transport/"},{"title":"go源码阅读:TLS handshake","text":"tls handshake tls/handshake_server.go: serverHandshakeState tls/common.go: Conn.PeerCertificates tls/handshake_client.go: doFullHandshake: handshake读取到的certificateMsg最终会解析后赋值给peerCertificates rfc5246-Handshake Protocol tls/handshake_client.go: clientHandshake makeClientHello loadSession : 会生成sessionID writeRecord : 将ClientHello发送给server readHandshake: 等server回复 tls/handshake_server.go: serverHandshake readClientHello processClientHello pickCipherSuite doFullHandshake establishKeys readFinished sendSessionTicket sendFinished tls.Conn: handlePostHandshakeMessage/handleKeyUpdate: KeyUpdate指的就是tls最后的那个对称密钥(变量名叫trafficSecret) tls/conn.go tls.Conn实现了net.Conn接口, 在Write和Read包裹了一层handShake, 后面的读写也是带了加密的(readRecordOrCCS) net/conn.go 12345678910type Conn interface { Read(b []byte) (n int, err error) Write(b []byte) (n int, err error) Close() error LocalAddr() Addr RemoteAddr() Addr SetDeadline(t time.Time) error SetReadDeadline(t time.Time) error SetWriteDeadline(t time.Time) error}","link":"/2023/04/25/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_tls-handshake/"},{"title":"grpc的goAway和keepalive","text":"简介 虽是Http2的东西, 但也可以通过grpc的源码来侧面加深下理解 GoAway 告诉客户端, 服务端准备关闭了, 本连接不要发新请求过来了 (发一半的请求还是会处理完的) 当client收到这个包之后就会主动关闭连接。下次需要发送数据时，就会重新建立连接 流程: client收到Goaway -&gt; client主动关闭http2连接 -&gt; channel变为IDLE -&gt; 用户发起新请求 -&gt; 创建新连接 -&gt; channel变为CONNECTING 另外GoAway是实现优雅关闭的基石, 因为是client主动关闭(不同于服务端关闭), 可以避免很多无效的请求 源码: google.golang.org/grpc/clientconn.go: errConnDrain (drain是接收到goaway后的连接状态) google.golang.org/grpc/internal/transport/http2_client.go: func (t *http2Client) Close(err error) google.golang.org/grpc/internal/transport/http2_client.go: func (t http2Client) handleGoAway(f http2.GoAwayFrame) 里面的 t.onClose(t.goAwayReason)的onClose在\"grpc/clientconn.go/addrConn createTransport\"里面定义的: 123456789101112131415161718192021222324252627282930313233343536373839func (ac *addrConn) createTransport(ctx context.Context, addr resolver.Address, copts transport.ConnectOptions, connectDeadline time.Time) error { addr.ServerName = ac.cc.getServerName(addr) hctx, hcancel := context.WithCancel(ctx) // hctx会深入到http2Client,见newHTTP2Client的信号&quot;&lt;-newClientCtx.Done()&quot; // 客户端收到goAway的反馈: 关闭连接 + state变IDLE onClose := func(r transport.GoAwayReason) { ac.mu.Lock() defer ac.mu.Unlock() // adjust params based on GoAwayReason ac.adjustParams(r) if ctx.Err() != nil { // Already shut down or connection attempt canceled. tearDown() or // updateAddrs() already cleared the transport and canceled hctx // via ac.ctx, and we expected this connection to be closed, so do // nothing here. return } hcancel() // 关闭http2连接! if ac.transport == nil { // We're still connecting to this address, which could error. Do // not update the connectivity state or resolve; these will happen // at the end of the tryAllAddrs connection loop in the event of an // error. return } ac.transport = nil // Refresh the name resolver on any connection loss. ac.cc.resolveNow(resolver.ResolveNowOptions{}) // Always go idle and wait for the LB policy to initiate a new // connection attempt. ac.updateConnectivityState(connectivity.Idle, nil) } connectCtx, cancel := context.WithDeadline(ctx, connectDeadline) defer cancel() copts.ChannelzParentID = ac.channelzID newTr, err := transport.NewClientTransport(connectCtx, ac.cc.ctx, addr, copts, onClose) ... ...} 关于keepalive keepalive在server端是默认开启的, client端默认关闭 keepalive.ServerParameters.MaxConnectionIdle: 如果一个client空闲超过15s, 发送一个 GOAWAY grpc-keepalive-guide RPC客户端长连接机制实现及keepalive分析 源码: 结构: google.golang.org/grpc/internal/transport/http2_server.go: http2Server.kp 结构: google.golang.org/grpc/internal/transport/http2_client.go: http2Client.kp 实现: google.golang.org/grpc/internal/transport/http2_server.go: func (t *http2Server) keepalive() 实现: google.golang.org/grpc/internal/transport/http2_client.go: func (t *http2Client) keepalive()","link":"/2023/08/02/grpc%E7%9A%84goAway%E5%92%8Ckeepalive/"},{"title":"Py小玩具-简单好用的OCR","text":"效果如下 截图识别 图片识别 代码 https://github.com/shuoGG1239/Image2Text 介绍 本来一开始是用谷歌的tesseract, 也搞来了据说比较靠谱的trainedData, 但实际识别准确率实在不行, 于是放弃了, 不过这种好处就是可以离线识图, 只要能搞到靠谱好用的trainedData肯定是比在线识图要好啦 这个小工具最终是用了百度AI的OCR接口, 识别率不错, 特别是中文 百度AI的普通文字识别的接口一天最多只能免费调用500次/账号, 也不保证并发量, 想变强就充钱吧 识图的核心代码在上面代码仓库的ocr_util.py, 要注意的是API_KEY和SECRET_KEY的这两个变量要填上自己的key, 这俩key获取直接得到百度AI去拿, 直接登陆控制台--&gt;鼠标移到右上角头像上--&gt;安全认证--&gt; AccessKey, 当然也可以直接用我的Key, 我都直接丢代码里, 但尽量用自己的吧, 说不定某天我不小心把AccessKey删了呢 :P Gui用的是PyQt5, 只支持python3, 所以python2.7的同学可以无视Gui部分...","link":"/2018/07/21/image2text/"},{"title":"Py小玩具-罗马转假名","text":"罗马音转日文假名 罗马音转假名(平假片假), 简单好用... 属于个人需求, 偶尔要敲一段假名, (日文输入法太笨重污染桌面干净的右下角) 算是用的比较频繁的一个自制玩具 效果 japinput_1 使用 无需联网 除了长音符用了yy, 其余输入规则和主流的日文输入法差不多 例子 1234567ka かwa わi いlo ぉ (小お)le ぇ (小え)nn んyy ー 实现 纯PyQt5开发 代码 https://github.com/shuoGG1239/JapInput","link":"/2019/06/09/japinput/"},{"title":"markdown锚点跳转的坑","text":"背景 写markdown有这样的需求: 点击某个词跳转到markdown文章的某个位置(某个锚点), 但是写完发现有些点了跳不过去 原因就是跳转锚点的格式没写对, 格式见下面 锚点title需要注意的格式 必须全小写 空格用'-'代替 '_' '()'需要去掉 错误例子 123[点我跳转1](/shuogg/article.html#如何取一个好的ID) [点我跳转2](/shuogg/article.html#game system搭建)[点我跳转3](/shuogg/article.html#game_system搭建) 正确例子 123[点我跳转1](/shuogg/article.html#如何取一个好的id) [点我跳转2](/shuogg/article.html#game-system搭建)[点我跳转3](/shuogg/article.html#gamesystem搭建)","link":"/2019/11/13/markdown_note/"},{"title":"mongodb官方备份工具, MongoDump源码分析","text":"简介 因为工作需要魔改该模块, 所以详细看了源码, 这里顺便做下笔记 本文前几小节讲MongoDump的运转流程, 后几小节讲代码方面的技术细节 源码版本: 100.6.1 源码仓库: https://github.com/mongodb/mongo-tools/blob/100.6.1/mongodump 0. 备份总览: MongoDump.Dump 备份逻辑就在MongoDump的Dump方法 (mongodump/mongodump.go/MongoDump.Dump) Dump主要分4步: 各种初始化工作 备份metaData, index, users, roles, version 等基础数据 备份collections 备份oplog 后面会经常提到Intent, 这是MongoDump自己的一个抽象概念, 可以简单理解为备份任务单元, 例如一个collection的备份对应一个Intent, oplog的备份对应一个Intent等等; 在阅读源码时你可以将Intent在脑海里替换成Task. 关于Intent详见本文后面章节 核心逻辑见以下源码及注释(为了方便阅读, 这里我删减了些不关键的逻辑): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258// Dump是MongoDump的一个方法type MongoDump struct { ToolOptions *options.ToolOptions InputOptions *InputOptions OutputOptions *OutputOptions SkipUsersAndRoles bool ProgressManager progress.Manager SessionProvider *db.SessionProvider // 就是mongoClient manager *intents.Manager // 备份单元管理, 核心组件 query bson.D oplogCollection string oplogStart primitive.Timestamp oplogEnd primitive.Timestamp isMongos bool storageEngine storageEngineType authVersion int archive *archive.Writer // InputOptions.Output非&quot;-&quot;时往这里写入 shutdownIntentsNotifier *notifier OutputWriter io.Writer // InputOptions.Output为&quot;-&quot;时往这里写入 Logger *log.ToolLogger}// OutputOptions出现频繁所以贴一下type OutputOptions struct { Out string `long:&quot;out&quot; value-name:&quot;&lt;directory-path&gt;&quot; short:&quot;o&quot; description:&quot;output directory, or '-' for stdout (default: 'dump')&quot;` Gzip bool `long:&quot;gzip&quot; description:&quot;compress archive or collection output with Gzip&quot;` Oplog bool `long:&quot;oplog&quot; description:&quot;use oplog for taking a point-in-time snapshot&quot;` Archive string `long:&quot;archive&quot; value-name:&quot;&lt;file-path&gt;&quot; optional:&quot;true&quot; optional-value:&quot;-&quot; description:&quot;dump as an archive to the specified path. If flag is specified without a value, archive is written to stdout&quot;` DumpDBUsersAndRoles bool `long:&quot;dumpDbUsersAndRoles&quot; description:&quot;dump user and role definitions for the specified database&quot;` ExcludedCollections []string `long:&quot;excludeCollection&quot; value-name:&quot;&lt;collection-name&gt;&quot; description:&quot;collection to exclude from the dump (may be specified multiple times to exclude additional collections)&quot;` ExcludedCollectionPrefixes []string `long:&quot;excludeCollectionsWithPrefix&quot; value-name:&quot;&lt;collection-prefix&gt;&quot; description:&quot;exclude all collections from the dump that have the given prefix (may be specified multiple times to exclude additional prefixes)&quot;` NumParallelCollections int `long:&quot;numParallelCollections&quot; short:&quot;j&quot; description:&quot;number of collections to dump in parallel&quot; default:&quot;4&quot; default-mask:&quot;-&quot;` ViewsAsCollections bool `long:&quot;viewsAsCollections&quot; description:&quot;dump views as normal collections with their produced data, omitting standard collections&quot;`}func (dump *MongoDump) Dump() (err error) { defer dump.SessionProvider.Close() /* 1. 阶段1: 各种初始化工作 */ // 检查下dump.ToolOptions.Namespace.DB和dump.ToolOptions.Namespace.Collection是否存在 exists, err := dump.verifyCollectionExists() if err != nil { return fmt.Errorf(&quot;error verifying collection info: %v&quot;, err) } if !exists { return nil } // 初始化shutdownIntentsNotifier; 本质就是一个shutdown chan dump.shutdownIntentsNotifier = newNotifier() // 初始化dump.query; 是针对指定了过滤条件的情况, 一般不会用到 if dump.InputOptions.HasQuery() { content, err := dump.InputOptions.GetQuery() if err != nil { return err } var query bson.D err = bson.UnmarshalExtJSON(content, false, &amp;query) if err != nil { return fmt.Errorf(&quot;error parsing query as Extended JSON: %v&quot;, err) } dump.query = query } // 连源MongoDB获取authSchemaVersion, 版本小于3则不支持备份用户和角色, 直接返回错误 if !dump.SkipUsersAndRoles &amp;&amp; dump.OutputOptions.DumpDBUsersAndRoles { // dump.SessionProvider就是mongoClient dump.authVersion, err = auth.GetAuthVersion(dump.SessionProvider) if err == nil { err = auth.VerifySystemAuthVersion(dump.SessionProvider) } if err != nil { return fmt.Errorf(&quot;error getting auth schema version for dumpDbUsersAndRoles: %v&quot;, err) } if dump.authVersion &lt; 3 { return fmt.Errorf(&quot;backing up users and roles is only supported for &quot;+ &quot;deployments with auth schema versions &gt;= 3, found: %v&quot;, dump.authVersion) } } // 初始化dump.archive, dump.archive是个高度封装的Writer if dump.OutputOptions.Archive != &quot;&quot; { var archiveOut io.WriteCloser // 根据dump.OutputOptions.Archive获取对应输出文件的ioWriter, 当值为&quot;-&quot;时输出将到OutputWriter而不是文件 archiveOut, err = dump.getArchiveOut() if err != nil { return err } dump.archive = &amp;archive.Writer{ Out: archiveOut, Mux: archive.NewMultiplexer(archiveOut, dump.shutdownIntentsNotifier), } go dump.archive.Mux.Run() // 备份结束后的一些释放工作 defer func() { // The Mux runs until its Control is closed close(dump.archive.Mux.Control) muxErr := &lt;-dump.archive.Mux.Completed archiveOut.Close() if muxErr != nil { if err != nil { err = fmt.Errorf(&quot;archive writer: %v / %v&quot;, err, muxErr) } else { err = fmt.Errorf(&quot;archive writer: %v&quot;, muxErr) } dump.Logger.Logvf(log.DebugLow, &quot;%v&quot;, err) } else { dump.Logger.Logvf(log.DebugLow, &quot;mux completed successfully&quot;) } }() } // 源Mongodb连通性检测 session, err := dump.SessionProvider.GetSession() if err != nil { return fmt.Errorf(&quot;error getting a client session: %v&quot;, err) } err = session.Ping(context.Background(), nil) if err != nil { return fmt.Errorf(&quot;error connecting to host: %v&quot;, err) } // 创建备份Intents switch { case dump.ToolOptions.DB == &quot;&quot; &amp;&amp; dump.ToolOptions.Collection == &quot;&quot;: err = dump.CreateAllIntents() case dump.ToolOptions.DB != &quot;&quot; &amp;&amp; dump.ToolOptions.Collection == &quot;&quot;: err = dump.CreateIntentsForDatabase(dump.ToolOptions.DB) case dump.ToolOptions.DB != &quot;&quot; &amp;&amp; dump.ToolOptions.Collection != &quot;&quot;: err = dump.CreateCollectionIntent(dump.ToolOptions.DB, dump.ToolOptions.Collection) } if err != nil { return fmt.Errorf(&quot;error creating intents to dump: %v&quot;, err) } // 如果需要备份Oplog, 则创建备份Oplog的Intents if dump.OutputOptions.Oplog { err = dump.CreateOplogIntents() if err != nil { return err } } // 如果需要备份Users和Roles, 则创建备份Users和Role的Intents if !dump.SkipUsersAndRoles &amp;&amp; dump.OutputOptions.DumpDBUsersAndRoles &amp;&amp; dump.ToolOptions.DB != &quot;admin&quot; { err = dump.CreateUsersRolesVersionIntentsForDB(dump.ToolOptions.DB) if err != nil { return err } } /* 2. 阶段2: 备份metaData, index, users, roles, version 等基础数据 */ err = dump.DumpMetadata() // intent.MetadataFile.Write(json.Marshal(metadata)) if err != nil { return fmt.Errorf(&quot;error dumping metadata: %v&quot;, err) } if dump.OutputOptions.Archive != &quot;&quot; { serverVersion, err := dump.SessionProvider.ServerVersion() if err != nil { dump.Logger.Logvf(log.Always, &quot;warning, couldn't get version information from server: %v&quot;, err) serverVersion = &quot;unknown&quot; } dump.archive.Prelude, err = archive.NewPrelude(dump.manager, dump.OutputOptions.NumParallelCollections, serverVersion, dump.ToolOptions.VersionStr) if err != nil { return fmt.Errorf(&quot;creating archive prelude: %v&quot;, err) } err = dump.archive.Prelude.Write(dump.archive.Out) if err != nil { return fmt.Errorf(&quot;error writing metadata into archive: %v&quot;, err) } } // 备份users, roles if !dump.SkipUsersAndRoles { if dump.ToolOptions.DB == &quot;admin&quot; || dump.ToolOptions.DB == &quot;&quot; { err = dump.DumpUsersAndRoles() if err != nil { return fmt.Errorf(&quot;error dumping users and roles: %v&quot;, err) } } if dump.OutputOptions.DumpDBUsersAndRoles { dump.Logger.Logvf(log.Always, &quot;dumping users and roles for %v&quot;, dump.ToolOptions.DB) if dump.ToolOptions.DB == &quot;admin&quot; { dump.Logger.Logvf(log.Always, &quot;skipping users/roles dump, already dumped admin database&quot;) } else { err = dump.DumpUsersAndRolesForDB(dump.ToolOptions.DB) if err != nil { return fmt.Errorf(&quot;error dumping users and roles: %v&quot;, err) } } } } // 设置dump.oplogStart 和 dump.oplogCollection if dump.OutputOptions.Oplog { // set dump.oplogCollection, &quot;oplog.rs&quot;或&quot;oplog.$main&quot; err := dump.determineOplogCollectionName() if err != nil { return fmt.Errorf(&quot;error finding oplog: %v&quot;, err) } dump.Logger.Logvf(log.Info, &quot;getting most recent oplog timestamp&quot;) dump.oplogStart, err = dump.getOplogCopyStartTime() if err != nil { return fmt.Errorf(&quot;error getting oplog start: %v&quot;, err) } } /* 3. 阶段3: 备份collections */ if err := dump.DumpIntents(); err != nil { return err } /* 4. 阶段4: 备份oplog */ if dump.OutputOptions.Oplog { dump.oplogEnd, err = dump.getCurrentOplogTime() if err != nil { return fmt.Errorf(&quot;error getting oplog end: %v&quot;, err) } // 确认oplog文件是否发生了翻转(Roll over), oplog本身是个环形队列 exists, err := dump.checkOplogTimestampExists(dump.oplogStart) if !exists { return fmt.Errorf( &quot;oplog overflow: mongodump was unable to capture all new oplog entries during execution&quot;) } if err != nil { return fmt.Errorf(&quot;unable to check oplog for overflow: %v&quot;, err) } dump.Logger.Logvf(log.Always, &quot;writing captured oplog to %v&quot;, dump.manager.Oplog().Location) // 备份oplog err = dump.DumpOplogBetweenTimestamps(dump.oplogStart, dump.oplogEnd) if err != nil { return fmt.Errorf(&quot;error dumping oplog: %v&quot;, err) } // 再次确认oplog文件是否发生了翻转 dump.Logger.Logvf(log.DebugLow, &quot;checking again if oplog entry %v still exists&quot;, dump.oplogStart) exists, err = dump.checkOplogTimestampExists(dump.oplogStart) if !exists { return fmt.Errorf( &quot;oplog overflow: mongodump was unable to capture all new oplog entries during execution&quot;) } if err != nil { return fmt.Errorf(&quot;unable to check oplog for overflow: %v&quot;, err) } } // 备份完成 dump.Logger.Logvf(log.DebugLow, &quot;finishing dump&quot;) return err} 1. 备份metadata 备份metadata的逻辑比较简单, 就是将Metadata jsonMarshal后写入intent.MetadataFile (io) 源码逻辑如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192type Metadata struct { Options bson.M `bson:&quot;options,omitempty&quot;` Indexes []bson.D `bson:&quot;indexes&quot;` UUID string `bson:&quot;uuid,omitempty&quot;` CollectionName string `bson:&quot;collectionName&quot;` Type string `bson:&quot;type,omitempty&quot;`}func (dump *MongoDump) dumpMetadata(intent *intents.Intent, buffer resettableOutputBuffer) (err error) { // 1. 填充Metadata, 值取自入参`intent` meta := Metadata{ Indexes: []bson.D{}, } meta.Options = intent.Options meta.UUID = intent.UUID meta.CollectionName = intent.C if intent.Type != &quot;&quot; { meta.Type = intent.Type } session, err := dump.SessionProvider.GetSession() if err != nil { return err } // 获取源端的index并set进meta.Indexes if dump.OutputOptions.ViewsAsCollections || intent.IsView() { dump.Logger.Logvf(log.DebugLow, &quot;not dumping indexes metadata for '%v' because it is a view&quot;, intent.Namespace()) } else { // get the indexes indexesIter, err := db.GetIndexes(session.Database(intent.DB).Collection(intent.C)) if err != nil { return err } if indexesIter == nil { dump.Logger.Logvf(log.Always, &quot;the collection %v appears to have been dropped after the dump started&quot;, intent.Namespace()) return nil } defer indexesIter.Close(context.Background()) ctx := context.Background() for indexesIter.Next(ctx) { indexOpts := &amp;bson.D{} err := indexesIter.Decode(indexOpts) if err != nil { return fmt.Errorf(&quot;error converting index: %v&quot;, err) } meta.Indexes = append(meta.Indexes, *indexOpts) } if err := indexesIter.Err(); err != nil { return fmt.Errorf(&quot;error getting indexes for collection `%v`: %v&quot;, intent.Namespace(), err) } } // 2. 把Metadata写入intent.MetadataFile /* 后面就是将meta jsonMarshal后写入intent.MetadataFile 而已*/ jsonBytes, err := bson.MarshalExtJSON(meta, true, false) if err != nil { return fmt.Errorf(&quot;error marshalling metadata json for collection `%v`: %v&quot;, intent.Namespace(), err) } err = intent.MetadataFile.Open() if err != nil { return err } defer func() { closeErr := intent.MetadataFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() var f io.Writer f = intent.MetadataFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } _, err = f.Write(jsonBytes) if err != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 2. 备份collections 源码如下, 就是从intent的manager中不断取intent分配给n条线程进行备份 一个intent对应一个collection的备份任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 并发备份collections, NumParallelCollections条线程func (dump *MongoDump) DumpIntents() error { resultChan := make(chan error) // 线程数 = jobs = dump.OutputOptions.NumParallelCollections jobs := dump.OutputOptions.NumParallelCollections if numIntents := len(dump.manager.Intents()); jobs &gt; numIntents { jobs = numIntents } // 设置intents的Pop顺序策略 if jobs &gt; 1 { dump.manager.Finalize(intents.LongestTaskFirst) } else { dump.manager.Finalize(intents.Legacy) } // 多线程从dump.manager中Pop出Intent, 进行dump for i := 0; i &lt; jobs; i++ { go func(id int) { buffer := dump.getResettableOutputBuffer() dump.Logger.Logvf(log.DebugHigh, &quot;starting dump routine with id=%v&quot;, id) for { intent := dump.manager.Pop() if intent == nil { dump.Logger.Logvf(log.DebugHigh, &quot;ending dump routine with id=%v, no more work to do&quot;, id) resultChan &lt;- nil return } if intent.BSONFile != nil { err := dump.DumpIntent(intent, buffer) if err != nil { resultChan &lt;- err return } } dump.manager.Finish(intent) } }(i) } // 等待所有intents dump完 for i := 0; i &lt; jobs; i++ { if err := &lt;-resultChan; err != nil { return err } } return nil} 3. 备份oplog 备份oplog的逻辑比较简单, 将查询oplog的结果写入oplog对应的intent.BSONFile 如果对oplog不熟悉可以看下官方文档: https://www.mongodb.com/zh-cn/docs/manual/core/replica-set-oplog/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 查询start与end之间的oplog, 写入 dump.manager.Oplog().BSONFilefunc (dump *MongoDump) DumpOplogBetweenTimestamps(start, end primitive.Timestamp) error { session, err := dump.SessionProvider.GetSession() if err != nil { return err } queryObj := bson.M{&quot;$and&quot;: []bson.M{ {&quot;ts&quot;: bson.M{&quot;$gte&quot;: start}}, {&quot;ts&quot;: bson.M{&quot;$lte&quot;: end}}, }} oplogQuery := &amp;db.DeferredQuery{ // &quot;local.oplog.rs&quot;(replset)或&quot;local.oplog.$main&quot;(m/s) Coll: session.Database(&quot;local&quot;).Collection(dump.oplogCollection), Filter: queryObj, LogReplay: true, } // 执行上面的`oplogQuery`, 将结果写入`dump.manager.Oplog().BSONFile` (dump.manager.Oplog()是oplog的intent) oplogCount, err := dump.dumpValidatedQueryToIntent(oplogQuery, dump.manager.Oplog(), dump.getResettableOutputBuffer(), oplogDocumentValidator) if err == nil { dump.Logger.Logvf(log.Always, &quot;\\tdumped %v oplog %v&quot;, oplogCount, util.Pluralize(int(oplogCount), &quot;entry&quot;, &quot;entries&quot;)) } return err}// 把`query`的查询结果写入`intent.BSONFile` (这是一个公共函数, 上边引用到了就顺便贴下, 不关键)func (dump *MongoDump) dumpValidatedQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer, validator documentValidator) (dumpCount int64, err error) { err = intent.BSONFile.Open() if err != nil { return 0, err } defer func() { closeErr := intent.BSONFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() // don't dump any data for views being dumped as views if intent.IsView() &amp;&amp; !dump.OutputOptions.ViewsAsCollections { return 0, nil } total, err := dump.getCount(query, intent) if err != nil { return 0, err } dumpProgressor := progress.NewCounter(total) if dump.ProgressManager != nil { dump.ProgressManager.Attach(intent.Namespace(), dumpProgressor) defer dump.ProgressManager.Detach(intent.Namespace()) } var f io.Writer f = intent.BSONFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } cursor, err := query.Iter() if err != nil { return } // 将cursor查询结果写入f err = dump.dumpValidatedIterToWriter(cursor, f, dumpProgressor, validator) dumpCount, _ = dumpProgressor.Progress() if err != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 备份单元: Intent 备份任务单元, 可以简单理解1个collection的备份任务就叫intent, 拆分是为了多线程执行 Intent相关的结构和关键方法: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177type Intent struct { // Destination namespace info DB string C string // collection // File locations as absolute paths BSONFile file BSONSize int64 MetadataFile file // Indicates where the intent will be read from or written to Location string MetadataLocation string // Collection options Options bson.M // UUID (for MongoDB 3.6+) as a big-endian hex string UUID string // File/collection size, for some prioritizer implementations. // Units don't matter as long as they are consistent for a given use case. Size int64 // Either view or timeseries. Empty string &quot;&quot; is a regular collection. Type string}// 查询collection(intent.C) 的数据, 写入intent.BSONFile, 仅此而已func (dump *MongoDump) DumpIntent(intent *intents.Intent, buffer resettableOutputBuffer) error { session, err := dump.SessionProvider.GetSession() if err != nil { return err } intendedDB := session.Database(intent.DB) var coll *mongo.Collection if intent.IsTimeseries() { coll = intendedDB.Collection(&quot;system.buckets.&quot; + intent.C) } else { coll = intendedDB.Collection(intent.C) } isView := true collInfo, err := db.GetCollectionInfo(coll) if err != nil { return err } else if collInfo != nil { isView = collInfo.IsView() } // 推断并设置dump.storageEngine if dump.storageEngine == storageEngineUnknown &amp;&amp; !isView { if err != nil { return err } dump.storageEngine = storageEngineModern isMMAPV1, err := db.IsMMAPV1(intendedDB, intent.C) if err != nil { dump.Logger.Logvf(log.Always, &quot;failed to determine storage engine, an mmapv1 storage engine could result in&quot;+ &quot; inconsistent dump results, error was: %v&quot;, err) } else if isMMAPV1 { dump.storageEngine = storageEngineMMAPV1 } } findQuery := &amp;db.DeferredQuery{Coll: coll} switch { case len(dump.query) &gt; 0: if intent.IsTimeseries() { metaKey, ok := intent.Options[&quot;timeseries&quot;].(bson.M)[&quot;metaField&quot;].(string) if !ok { return fmt.Errorf(&quot;could not determine the metaField for %s&quot;, intent.Namespace()) } for i, predicate := range dump.query { splitPredicateKey := strings.SplitN(predicate.Key, &quot;.&quot;, 2) if splitPredicateKey[0] != metaKey { return fmt.Errorf(&quot;cannot process query %v for timeseries collection %s. &quot;+ &quot;mongodump only processes queries on metadata fields for timeseries collections.&quot;, dump.query, intent.Namespace()) } if len(splitPredicateKey) &gt; 1 { dump.query[i].Key = &quot;meta.&quot; + splitPredicateKey[1] } else { dump.query[i].Key = &quot;meta&quot; } } } findQuery.Filter = dump.query case dump.storageEngine == storageEngineMMAPV1 &amp;&amp; !dump.InputOptions.TableScan &amp;&amp; !isView &amp;&amp; !intent.IsSpecialCollection() &amp;&amp; !intent.IsOplog(): autoIndexId, found := intent.Options[&quot;autoIndexId&quot;] if !found || autoIndexId == true { findQuery.Hint = bson.D{{&quot;_id&quot;, 1}} } } var dumpCount int64 if dump.OutputOptions.Out == &quot;-&quot; { // 初始化阶段有 &quot;intent.BSONFile = &amp;stdoutFile{Writer: dump.OutputWriter}&quot;, 可以搜下源码 // 所以这里虽然也是写到intent.BSONFile, 但实际写到dump.OutputWriter了 dump.Logger.Logvf(log.Always, &quot;writing %v to stdout&quot;, intent.DataNamespace()) dumpCount, err = dump.dumpQueryToIntent(findQuery, intent, buffer) if err == nil { // on success, print the document count dump.Logger.Logvf(log.Always, &quot;dumped %v %v&quot;, dumpCount, docPlural(dumpCount)) } return err } // 将findQuery查到的写入intent.BSONFile if dumpCount, err = dump.dumpQueryToIntent(findQuery, intent, buffer); err != nil { return err } return nil}// 将query查到的写入intent.BSONFilefunc (dump *MongoDump) dumpQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer) (dumpCount int64, err error) { return dump.dumpValidatedQueryToIntent(query, intent, buffer, nil)}func (dump *MongoDump) dumpValidatedQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer, validator documentValidator) (dumpCount int64, err error) { err = intent.BSONFile.Open() if err != nil { return 0, err } defer func() { closeErr := intent.BSONFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() // don't dump any data for views being dumped as views if intent.IsView() &amp;&amp; !dump.OutputOptions.ViewsAsCollections { return 0, nil } total, err := dump.getCount(query, intent) if err != nil { return 0, err } dumpProgressor := progress.NewCounter(total) if dump.ProgressManager != nil { dump.ProgressManager.Attach(intent.Namespace(), dumpProgressor) defer dump.ProgressManager.Detach(intent.Namespace()) } var f io.Writer f = intent.BSONFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } cursor, err := query.Iter() if err != nil { return } // 将cursor查到的东西写入f err = dump.dumpValidatedIterToWriter(cursor, f, dumpProgressor, validator) dumpCount, _ = dumpProgressor.Progress() if err != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 多路读写模块: archive.Writer/Reader 这是MongoDump唯一比较复杂的模块, 因为只讲备份, 所以只讲archive.Writer, archive.Reader是恢复时用到, 原理一样 多路读写核心Multiplexer, 里面有个核心组件MuxIn, 这东西其实就是上面提到的BSONFile的实现, 每次BSONFile.Open就相当于New一个NuxIn然后塞给Multiplexer管理, MuxIn就是多路读写里面的路 多路读写怎么实现? 其实本质是多路In, 一路Out, 上面提到的MuxIn是实现多路In, 而一路Out的关键逻辑在Multiplexer.formatBody, 这里可以看看下面的源码, 其实就是利用写入header和namespace来做数据隔离, 配合Multiplexer的select channel这样就实现了多路读写. 这个思想是值得学习的 概念那么多是不是看了头晕? 我们将所有概念都关联起来捋一下: 在1次备份中, 只有1个archive.Writer, 也意味着只有1个Multiplexer, 1个Multiplexer管理了n个MuxIn, n又等于Intent的个数, Intent有多少个? Intent的个数为len(collections) + 1 + 1, 这里的两个1分别是metadata和oplog Multiplexer源码的几个核心方法: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150type Writer struct { Out io.WriteCloser Prelude *Prelude Mux *Multiplexer}type Multiplexer struct { Out io.WriteCloser Control chan *MuxIn Completed chan error shutdownInputs notifier // ins and selectCases are correlating slices ins []*MuxIn selectCases []reflect.SelectCase currentNamespace string}type notifier interface { Notify()}func NewMultiplexer(out io.WriteCloser, shutdownInputs notifier) *Multiplexer { mux := &amp;Multiplexer{ Out: out, Control: make(chan *MuxIn), Completed: make(chan error), shutdownInputs: shutdownInputs, ins: []*MuxIn{ nil, // There is no MuxIn for the Control case }, } // 反射实现channel select, 非常少见的玩法! mux.selectCases = []reflect.SelectCase{ { Dir: reflect.SelectRecv, Chan: reflect.ValueOf(mux.Control), Send: reflect.Value{}, }, } return mux}// 核心事件循环: 处理MuxIn的增删事件和来自MuxIn的写数据事件func (mux *Multiplexer) Run() { var err, completionErr error for { // select的反射玩法, 学到了 index, value, notEOF := reflect.Select(mux.selectCases) EOF := !notEOF if index == 0 { // index 0 为 mux.Control, 用于接收新的MuxIn if EOF { log.Logvf(log.DebugLow, &quot;Mux finish&quot;) mux.Out.Close() if completionErr != nil { mux.Completed &lt;- completionErr } else if len(mux.selectCases) != 1 { mux.Completed &lt;- fmt.Errorf(&quot;Mux ending but selectCases still open %v&quot;, len(mux.selectCases)) } else { mux.Completed &lt;- nil } return } muxIn, ok := value.Interface().(*MuxIn) if !ok { mux.Completed &lt;- fmt.Errorf(&quot;non MuxIn received on Control chan&quot;) // one for the MuxIn.Open return } log.Logvf(log.DebugLow, &quot;Mux open namespace %v&quot;, muxIn.Intent.DataNamespace()) mux.selectCases = append(mux.selectCases, reflect.SelectCase{ Dir: reflect.SelectRecv, Chan: reflect.ValueOf(muxIn.writeChan), Send: reflect.Value{}, }) mux.ins = append(mux.ins, muxIn) } else { // index &gt; 0 为 MuxIn.writeChan, 用于接收MuxIn.Write的data if EOF { mux.ins[index].writeCloseFinishedChan &lt;- struct{}{} err = mux.formatEOF(index, mux.ins[index]) if err != nil { mux.shutdownInputs.Notify() mux.Out = &amp;nopCloseNopWriter{} completionErr = err } log.Logvf(log.DebugLow, &quot;Mux close namespace %v&quot;, mux.ins[index].Intent.DataNamespace()) mux.currentNamespace = &quot;&quot; mux.selectCases = append(mux.selectCases[:index], mux.selectCases[index+1:]...) mux.ins = append(mux.ins[:index], mux.ins[index+1:]...) } else { bsonBytes, ok := value.Interface().([]byte) if !ok { mux.Completed &lt;- fmt.Errorf(&quot;multiplexer received a value that wasn't a []byte&quot;) return } // format bsonBytes, 然后 mux.Out.Write(bsonBytes) err = mux.formatBody(mux.ins[index], bsonBytes) if err != nil { mux.shutdownInputs.Notify() mux.Out = &amp;nopCloseNopWriter{} completionErr = err } } } }}// 核心逻辑, 这个里的header用于隔离不同namespace的数据, 已达到多路的效果, 恢复的时候也是根据header来恢复的// mux.Out.Write header和bsonBytes, 这里的Out其实就是dump.archive.Outfunc (mux *Multiplexer) formatBody(in *MuxIn, bsonBytes []byte) error { var err error var length int defer func() { in.writeLenChan &lt;- length }() if in.Intent.DataNamespace() != mux.currentNamespace { // Handle the change of which DB/Collection we're writing docs for // If mux.currentNamespace then we need to terminate the current block if mux.currentNamespace != &quot;&quot; { l, err := mux.Out.Write(terminatorBytes) if err != nil { return err } if l != len(terminatorBytes) { return io.ErrShortWrite } } header, err := bson.Marshal(NamespaceHeader{ Database: in.Intent.DB, Collection: in.Intent.DataCollection(), }) if err != nil { return err } l, err := mux.Out.Write(header) if err != nil { return err } if l != len(header) { return io.ErrShortWrite } } mux.currentNamespace = in.Intent.DataNamespace() length, err = mux.Out.Write(bsonBytes) if err != nil { return err } return nil} MuxIn源码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273type MuxIn struct { writeChan chan []byte writeLenChan chan int writeCloseFinishedChan chan struct{} buf []byte hash hash.Hash64 Intent *intents.Intent Mux *Multiplexer}func (muxIn *MuxIn) Read([]byte) (int, error) { return 0, nil}func (muxIn *MuxIn) Pos() int64 { return 0}// 关闭muxIn内部的所有chan, 最后multiplexer会收到关闭信号并返回formatEOF, 同时multiplexer也会发信号muxIn.writeCloseFinishedChanfunc (muxIn *MuxIn) Close() error { // the mux side of this gets closed in the mux when it gets an eof on the read log.Logvf(log.DebugHigh, &quot;MuxIn close %v&quot;, muxIn.Intent.DataNamespace()) if bufferWrites { muxIn.writeChan &lt;- muxIn.buf length := &lt;-muxIn.writeLenChan if length != len(muxIn.buf) { return io.ErrShortWrite } muxIn.buf = nil } close(muxIn.writeChan) close(muxIn.writeLenChan) &lt;-muxIn.writeCloseFinishedChan return nil}// 初始化muxIn, 然后把自己发给 muxIn.Muxfunc (muxIn *MuxIn) Open() error { log.Logvf(log.DebugHigh, &quot;MuxIn open %v&quot;, muxIn.Intent.DataNamespace()) muxIn.writeChan = make(chan []byte) muxIn.writeLenChan = make(chan int) muxIn.writeCloseFinishedChan = make(chan struct{}) muxIn.buf = make([]byte, 0, bufferSize) muxIn.hash = crc64.New(crc64.MakeTable(crc64.ECMA)) if bufferWrites { muxIn.buf = make([]byte, 0, db.MaxBSONSize) } muxIn.Mux.Control &lt;- muxIn return nil}// buf写入muxIn.buf, 满了就把muxIn.buf写入muxIn.writeChan, 然后清空muxIn.buffunc (muxIn *MuxIn) Write(buf []byte) (int, error) { if bufferWrites { // 固定true if len(muxIn.buf)+len(buf) &gt; cap(muxIn.buf) { muxIn.writeChan &lt;- muxIn.buf length := &lt;-muxIn.writeLenChan if length != len(muxIn.buf) { return 0, io.ErrShortWrite } muxIn.buf = muxIn.buf[:0] } muxIn.buf = append(muxIn.buf, buf...) } else { muxIn.writeChan &lt;- buf length := &lt;-muxIn.writeLenChan if length != len(buf) { return 0, io.ErrShortWrite } } muxIn.hash.Write(buf) return len(buf), nil}","link":"/2023/04/15/mongoDump%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Mongodb数据迁移:MongoShake源码阅读","text":"简述 近期用到了MongoShake做数据迁移, 顺便看看源码, 本篇为阅读源码的笔记 源码信息 源码版本: v2.8.1 源码仓库: https://github.com/alibaba/MongoShake 架构 mongoshake-arch 一. collector进程入口:main func main为进程的入口, 核心源码如下: (为了只关注核心逻辑, 无关紧要的代码片段会用// ... + 注释代替) 简单来讲做了3件事情 初始化和校验配置参数 加进程锁 使用了第三方工具github.com/nightlyone/lockfile实现的 简单讲就是在conf.Options.PidPath目录下建一个{conf.Options.Id}.pid文件来实现进程锁; 所以你可以扫描该目录下有多少个pid文件来确认当前有多少RedisShake进程在运行 跑数据迁移 留坑,以后填","link":"/2023/04/09/mongoShake%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"OTP动态口令及底层实现","text":"背景 最近用到了OTP, 遂mark一下 我们常用的那种倒计时验证码就是TOTP, 既不是叫OTP也不是叫MFA, 经常听有人这么说所以提一嘴 OTP 动态口令验证可以看作是服务端和客户端之间通过约定相同的算法来实现验证功能, 也即你在客户端看到的动态口令是客户端通过算法生成的无需请求服务端获取 TOTP 平时用的google动态口令用的就是TOTP(Time-based One-Time Password), TOTP基于HOTP 原理: 假设用的是30秒间隔的六位口令, 精简版伪代码: 12345678// secret为密码, timestamp为时间戳, 返回口令GetOTPCode(secret, timestamp) { hs = hmac(secret, timestamp/30) // hsToInt是对hs这个[]byte进行各种&amp;与偏移操作然后转为int intHs = hsToInt(hs) code = intHs % 1000000 return code} HOTP TOTP本质就是一个套皮HOTP, 核心是HOTP 我们从TOTP的视角来理解HOTP做了什么, 见代码和注释: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647type HOTP struct { secret []byte // 密钥, 每个人都有属于自己的一串密钥 digits int // 验证码的位数}/* counter就是`当前时间戳/设定的倒计时长`, return的就是平时看到的那串数字(通常为6位); eg: 假设设定的倒计时长为60: counter为(1600000000 / 60) 返回 519365 counter为((1600000000 + 5) / 60) 返回 519365 counter为((1600000000 + 60) / 60)返回 797425*/func (h HOTP) At(counter uint64) string { counterBytes := make([]byte, 8) binary.BigEndian.PutUint64(counterBytes, counter) hash := hmac.New(sha1.New, h.secret) hash.Write(counterBytes) hs := hash.Sum(nil) offset := hs[19] &amp; 0x0f binCodeBytes := make([]byte, 4) binCodeBytes[0] = hs[offset] &amp; 0x7f binCodeBytes[1] = hs[offset+1] &amp; 0xff binCodeBytes[2] = hs[offset+2] &amp; 0xff binCodeBytes[3] = hs[offset+3] &amp; 0xff binCode := binary.BigEndian.Uint32(binCodeBytes) mod := uint32(1) for i := 0; i &lt; h.digits; i++ { mod *= 10 } code := binCode % mod codeString := strconv.FormatUint(uint64(code), 10) if len(codeString) &lt; h.digits { paddingByteLength := h.digits - len(codeString) paddingBytes := make([]byte, paddingByteLength) for i := 0; i &lt; paddingByteLength; i++ { paddingBytes[i] = '0' } codeString = string(paddingBytes) + codeString } return codeString}// 验证阶段就简单了, 就是拿用户的输入code和上面的At结果对比func (h HOTP) Verify(code string, counter uint64) bool { return h.At(counter) == code} 库 golang-gootp python-pyotp 参考 RFC 4226 (HOTP) RFC 6238 (TOTP) 动态令牌-(OTP,HOTP,TOTP)-基本原理","link":"/2020/06/06/otp/"},{"title":"Py小玩具-绘图转线稿","text":"背景 临摹大佬的作品的时候丰富的颜色会反而产生莫名的干扰, 所以希望能转成干净的线稿! PS的滤镜效果不理想, 也找不到其他合适的工具, 故自己撸一个试试 源码 https://github.com/shuoGG1239/Paint2Sketch 版本一(分支:no-ml) 原理就是直接用opencv各种骚操作, 效果如下图 右下角几个滑条用于调参, 参数主要有canny卷积核和开闭操作的几个阈值参数 效果就是这样...参数不管怎么调都很辣鸡 版本二(分支:master) 版本一的效果太辣鸡, 没啥思路, 弃坑搁置了一年; 无意瞎逛逛到了一个lllyasviel/sketchKeras的项目, 看似效果不错? lllyasviel/sketchKeras是走了神经网络的路线, 开源了test_code和训练好的mod, 总之先抄过来试试 效果如下: 牛逼!机器学习逆天改命! 使用 如果想用的话直接clone下来, 再把release的mod.h5下载完丢根目录, 跑main.py就行了","link":"/2019/06/30/paint2sketch/"},{"title":"Py小玩具-MarkDown图床助手","text":"简介 现在markdown越用越频繁了, md好用是好用, 但就是贴图片的时候有些麻烦: 要截图-&gt;上传图片-&gt;复制图片url, 于是做了个简单的工具: 截图-&gt;传图-&gt;生成图片url三合一, 三...三分之一倍的快乐呀! 效果 截图上传(也可以用快捷键ctrl+shift+alt+F8) img 上传后自动将图片url复制到剪贴板, 直接粘贴即可 img 也可以选择图片上传 img 也可以直接拖到框中上传 img 相关地址 exe下载: https://github.com/shuoGG1239/PicPong/releases 源码仓库: https://github.com/shuoGG1239/PicPong 技术相关 纯pyQt5开发 图床是用了sm.ms, 简单好用 后期也许会加一些其他的图床吧(也许...)","link":"/2018/10/25/picpong/"},{"title":"go-爬点Pixiv画师图","text":"背景 刚好需要某个画师的插画, 故写了个简单无需登录的爬图工具 根据Pixiv画师ID, 爬完直接保存在当前目录的 使用 1234567// Fetch完后自动建一个画师ID的目录然后图片就放里面cli := PixivClient{ Cli: http.Client{}, Proxy: defaultProxy(), // 不需要代理则nil即可}ctx, _ := context.WithTimeout(context.Background(), time.Second*600)cli.Fetch(ctx, &quot;4462&quot;, -1) // 画师ID:4462 完整实现 代码很简单, 就100多行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166package pixivimport ( &quot;context&quot; &quot;encoding/json&quot; &quot;fmt&quot; &quot;golang.org/x/net/proxy&quot; &quot;io&quot; &quot;io/ioutil&quot; &quot;math/rand&quot; &quot;net/http&quot; &quot;os&quot; &quot;strconv&quot; &quot;strings&quot; &quot;sync&quot; &quot;time&quot;)type PixivClient struct { Cli http.Client Proxy proxy.Dialer // 不为nil则走proxy RandSleep bool // 随机sleep, 防止爬太快}// limit为-1则无限func (pcli PixivClient) Fetch(ctx context.Context, userId string, limit int) { wg := sync.WaitGroup{} tasksDone := make(chan struct{}) illusts, err := pcli.ListIllustByUser(userId) if err != nil { return } for i, illust := range illusts { if i &gt; limit &amp;&amp; limit != -1 { break } wg.Add(1) go func(imgUrl, imgId string) { defer wg.Done() if pcli.RandSleep { randSleep(time.Second * 5) } err := pcli.fetchOne(userId, imgUrl, imgId) if err != nil { fmt.Printf(&quot;fetchOne(%s, %s): %s\\n&quot;, userId, imgId, err.Error()) return } fmt.Printf(&quot;%s finish!\\n&quot;, imgId) }(illust.ImageUrls.Large, strconv.Itoa(illust.Id)) } go func() { wg.Wait() tasksDone &lt;- struct{}{} }() select { case &lt;-tasksDone: case &lt;-ctx.Done(): fmt.Println(ctx.Err()) }}type illustInfo struct { Id int `json:&quot;id&quot;` ImageUrls struct { Large string `json:&quot;large&quot;` } `json:&quot;image_urls&quot;`}func (pcli PixivClient) ListIllustByUser(userId string) ([]illustInfo, error) { type pagination struct { Pages int `json:&quot;pages&quot;` Current int `json:&quot;current&quot;` PerPage int `json:&quot;per_page&quot;` Total int `json:&quot;total&quot;` } type imjad struct { Response []illustInfo `json:&quot;response&quot;` Pagination pagination `json:&quot;pagination&quot;` } pcli.Cli.Transport = nil // 国内的api无需代理 getFn := func(pageNo int) (*imjad, error) { u := fmt.Sprintf(&quot;https://api.imjad.cn/pixiv/v1/?type=member_illust&amp;id=%s&amp;page=%d&quot;, userId, pageNo) req, _ := http.NewRequest(&quot;GET&quot;, u, nil) req.Header.Set(&quot;user-agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&quot;) resp, err := pcli.Cli.Do(req) if err != nil { return nil, err } data, err := ioutil.ReadAll(resp.Body) if err != nil { return nil, err } var im imjad err = json.Unmarshal(data, &amp;im) if err != nil { return nil, err } return &amp;im, nil } im, err := getFn(1) if err != nil { return nil, err } illusts := make([]illustInfo, 0) for p := 1; p &lt;= im.Pagination.Pages; p++ { im, err := getFn(p) if err != nil { return nil, err } illusts = append(illusts, im.Response...) } return illusts, nil}func (pcli PixivClient) fetchOne(userId, imgUrl, imgId string) error { u := imgUrl cli := pcli.Cli if pcli.Proxy != nil { cli.Transport = pcli.socks5Transport() } req, _ := http.NewRequest(&quot;GET&quot;, u, nil) req.Header.Set(&quot;user-agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&quot;) req.Header.Set(&quot;Referer&quot;, &quot;http://www.pixiv.net/&quot;) resp, err := cli.Do(req) if err != nil { return err } _ = os.Mkdir(userId, os.ModePerm) idx := strings.LastIndexFunc(imgUrl, func(r rune) bool { if r == '/' { return true } return false }) f, err := os.Create(fmt.Sprintf(&quot;%s/%s&quot;, userId, imgUrl[idx+1:])) if err != nil { return err } defer func() { _ = f.Close() }() if _, err = io.Copy(f, resp.Body); err != nil { return err } return nil}func (pcli PixivClient) socks5Transport() *http.Transport { httpTransport := &amp;http.Transport{} httpTransport.Dial = pcli.Proxy.Dial return httpTransport}func randSleep(max time.Duration) { t := rand.Int63n(int64(max)) time.Sleep(time.Duration(t))}func defaultProxy() proxy.Dialer { d, err := proxy.SOCKS5(&quot;tcp&quot;, &quot;127.0.0.1:1080&quot;, nil, proxy.Direct) if err != nil { panic(err) } return d}","link":"/2020/06/21/pixiv_spider/"},{"title":"总结Pyinstaller的坑及终极解决方法","text":"一. 首先要有个稳定环境 下面是博主经测试的觉得坑比较少的环境搭配 Python3.4 + PyQt5.4 + Pyinstaller3.2.1 Python3.5 + PyQt5.8 + Pyinstaller3.2.1 二. Pyinstaller遇到坑没必要换打包工具 博主好几次用Pyinstaller遇到坑时都有考虑换工具如py2exe或cx-freeze之类的, 依旧无法解决 (最后还是用pyinstaller解决了) 所以没必要换其他工具, pyinstaller就够了 三. 坑1: 打包不了, 连exe都生成不出来 解决方法 直接换Pyinstaller的版本, 即卸掉重装, 推荐用3.2.1 四. 坑2: exe生成了, 但是跑不了 大多数情况都是被坑在这里 解决方法 遇到这种问题不管弹出什么样的错误提示, 在输出exe时参数加个'-d'即debug模式, 然后打开的时候能看到打印的错误信息了, 这招很好用 留意一下程序依赖的一些资源文件, 检查下路径是否正确, 特别是程序里有相对路径的; 还有一些涉及到依赖系统默认资源的如默认字体啥的, 也得留意 换下打包方式, 如onefile模式和onedir模式 (之前出现过onedir打包可以但onefile打包不行的情况) 环境变量PATH中加上PyQt5的plugins的路径 依旧不行则换个Pyinstaller的版本, 即卸掉重装, 推荐用3.2.1 再不行则换操作系统试试, 有win10跑得了但到了win7就跑不了的情况 (弄个虚拟机测下找下问题在哪) 五. 错误码集锦 main return -1 这种错误基本都是自己的问题, 只能在输出exe时参数加个'-d'即debug模式, 然后再查下打印的错误信息 Failed to execute script pyi_rth_pkgres 可以先换Pyinstaller的版本, 这个错误会消失, 但会弹出其他的错误信息, 然并卵 这种错误基本都是自己的问题, 只能在输出exe时参数加个'-d'即debug模式, 然后再查下打印的错误信息 Failed to execute script xxxx 这种错误基本都是自己的问题, 只能在输出exe时参数加个'-d'即debug模式, 然后再查下打印的错误信息 This application failed to start ... Qt platform plugin ... 这种错误先配下PyQt5的plugins的环境变量, 如博主的是C:-packages 不行再换Pyinstaller的版本 (貌似3.0.0这个版本有问题, 后来换3.2.1就没事了)","link":"/2018/07/06/pyinstaller_keng/"},{"title":"快速美化PyQt应用--QCandyUi","text":"QCandy-UI 快速美化PyQt应用 项目地址: https://github.com/shuoGG1239/QCandyUi 使用方法 pip install QCandyUi 仅需在需要美化的窗口类上加上@colorful装饰器即可 也可以调用CandyWindow.creatWindow()返回经美化的QWidget (推荐用这种) 实例 原味窗口 12345678910# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyclass TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝绿色主题的窗口(使用@colorful) 12345678910111213# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyfrom QCandyUi.CandyWindow import colorful@colorful('blueGreen')class TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝色主题的窗口(使用@colorful) 12345678910111213# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyfrom QCandyUi.CandyWindow import colorful@colorful('blue')class TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝色主题的窗口(使用CandyWindow.createWindow) 12345from QCandyUi import CandyWindowmainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow = CandyWindow.createWindow(mainWindow, 'blue')mainWindow.show() Ps: 想自己新增颜色主题可以在theme.json里面配, 按照theme.json里的格式配即可 py模块的安装包在/python-version/dist中","link":"/2018/07/11/qcandyui/"},{"title":"Redis数据迁移:RedisShake源码阅读","text":"简述 近期用到了RedisShake做数据迁移, 源码代码量不多于是看了一遍, 本篇为阅读源码的笔记 本篇重点讲解RedisShake的数据迁移功能, 其他几个功能dump,decode,restore,rump只简单提及 源码信息 源码版本: v1.6 源码仓库: https://github.com/tair-opensource/RedisShake 一. 进程入口:main func main为进程的入口, 核心源码如下: (为了只关注核心逻辑, 无关紧要的代码片段会用// ... + 注释代替) 简单来讲做了3件事情 初始化和校验配置参数 加进程锁 使用了第三方工具github.com/nightlyone/lockfile实现的 简单讲就是在conf.Options.PidPath目录下建一个{conf.Options.Id}.pid文件来实现进程锁; 所以你可以扫描该目录下有多少个pid文件来确认当前有多少RedisShake进程在运行 跑数据迁移 12345678910111213141516171819202122232425262728// main/main.gofunc main() { // ... 各种初始化和校验, 如配置文件, 入参等 // 加进程锁, 进程id为`conf.Options.Id`, 入参时指定 if err = utils.WritePidById(conf.Options.Id, conf.Options.PidPath); err != nil { crash(fmt.Sprintf(&quot;write pid failed. %v&quot;, err), -5) } // 根据需求选择对应的命令, 我们这里只关注数据迁移, 所以走run.CmdSync var runner base.Runner switch *tp { case conf.TypeDecode: runner = new(run.CmdDecode) // 见decode.go case conf.TypeRestore: runner = new(run.CmdRestore) // 见restore.go case conf.TypeDump: runner = new(run.CmdDump) // 见dump.go case conf.TypeSync: runner = new(run.CmdSync) // 见sync.go case conf.TypeRump: runner = new(run.CmdRump) // 见rump.go } // 运行迁移任务 runner.Main()} 二. 数据迁移入口: CmdSync 数据迁移的具体实现在 sync.go/CmdSync.Main, 核心源码如下: 简单来讲做了2件事情 建n个dbSyncer, 分配给m条线程 默认情况下n = m = len(SourceAddressList), 即和源地址相关, 但和源架构无关 对于迁移目标为cluster架构, 所有dbSyncer用同一个dst, 即TargetAddressList 对于迁移目标为standalone架构, 用roundRobin将TargetAddressList分配给各个DbSyncer 用dbSyncer完成数据迁移, 所以dbSyncer才是数据迁移的核心, 下小节讲解 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// sync.go// 这个Main()就是上小节的`runner.Main()`func (cmd *CmdSync) Main() { syncChan := make(chan syncNode, total) // dbSyncer是数据迁移的核心, 相当于worker cmd.dbSyncers = make([]*dbSyncer, total) for i, source := range conf.Options.SourceAddressList { var target []string if conf.Options.TargetType == conf.RedisTypeCluster { target = conf.Options.TargetAddressList } else { // round-robin pick pick := utils.PickTargetRoundRobin(len(conf.Options.TargetAddressList)) target = []string{conf.Options.TargetAddressList[pick]} } nd := syncNode{ id: i, source: source, sourcePassword: conf.Options.SourcePasswordRaw, target: target, targetPassword: conf.Options.TargetPasswordRaw, } syncChan &lt;- nd } var wg sync.WaitGroup wg.Add(len(conf.Options.SourceAddressList)) for i := 0; i &lt; int(conf.Options.SourceRdbParallel); i++ { go func() { for { nd, ok := &lt;-syncChan if !ok { break } ds := NewDbSyncer(nd.id, nd.source, nd.sourcePassword, nd.target, nd.targetPassword, conf.Options.HttpProfile+i) cmd.dbSyncers[nd.id] = ds go ds.sync() &lt;-ds.waitFull // 阻塞直至全量同步完成 wg.Done() } }() } wg.Wait() // 阻塞直至全量同步完成 close(syncChan) // 永远阻塞 select {}} 三. 数据迁移核心: dbSyncer 核心方法为3个: sendPSyncCmd: 读取全量+增量数据(pSync), 返回reader syncRDBFile: 从reader读取全量数据, 并同步(restore key) syncCommand: 从reader读取增量数据, 并同步(回放cmd) 这三个核心方法的具体逻辑如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223// sync.go// 不阻塞// 跑Psync, 跑一条全量+增量同步的goroutine, 用于不断读取全量+增量的数据,// 全量和增量同步获取的实体数据最终都会Pipe到piper, 即return的那个pipe.Readerfunc (ds *dbSyncer) sendPSyncCmd(master, auth_type, passwd string, tlsEnable bool) (pipe.Reader, int64) { // 1. 执行pSync c := utils.OpenNetConn(master, auth_type, passwd, tlsEnable) utils.SendPSyncListeningPort(c, conf.Options.HttpProfile) br := bufio.NewReaderSize(c, utils.ReaderBufferSize) bw := bufio.NewWriterSize(c, utils.WriterBufferSize) // 2. 首次pSync获取runid, offset, nsize, 后面的rdb数据会写入bw, 我们只需关注从br读取即可 runid, offset, wait := utils.SendPSyncFullsync(br, bw) ds.targetOffset.Set(offset) var nsize int64 // nsize为rdb的大小 for nsize == 0 { // 获取nsize为耗时操作, 通过wait管道通知获取(变量名换成waitForRdbSize好些) select { case nsize = &lt;-wait: if nsize == 0 { log.Infof(&quot;dbSyncer[%v] +&quot;, ds.id) } case &lt;-time.After(time.Second): log.Infof(&quot;dbSyncer[%v] -&quot;, ds.id) } } // br -&gt; pipew -&gt; piper(这个返回出去,作为ds.syncRDBFile的reader) piper, pipew := pipe.NewSize(utils.ReaderBufferSize) go func() { defer pipew.Close() p := make([]byte, 8192) // 3. 全量: 读取psync的数据直至读了nsize的数据, 最终写到piper(return出去那个) for rdbsize := int(nsize); rdbsize != 0; { // br -&gt; pipew rdbsize -= utils.Iocopy(br, pipew, p, rdbsize) } // 4. 增量: 不断从psync读数据, 最终写到piper(return出去那个), 是一个死循环(除非异常) for { n, err := ds.pSyncPipeCopy(c, br, bw, offset, pipew) // 正常会在这里永远阻塞 if err != nil { log.PanicErrorf(err, &quot;dbSyncer[%v] psync runid = %s, offset = %d, pipe is broken&quot;, ds.id, runid, offset) } // ... 后面是失败重试相关的操作, 这里不展示 } }() return piper, nsize}// 阻塞至全量同步完成// 从reader(这个就是上面sendPSyncCmd返回的reader)读出BinEntry, 再OpenRedisConn(target), 然后RestoreRdbEntry到target的redis节点func (ds *dbSyncer) syncRDBFile(reader *bufio.Reader, target []string, auth_type, passwd string, nsize int64, tlsEnable bool) { pipe := utils.NewRDBLoader(reader, &amp;ds.rbytes, base.RDBPipeSize) wait := make(chan struct{}) go func() { defer close(wait) var wg sync.WaitGroup wg.Add(conf.Options.Parallel) // restore的时候可以并发, 从pipe里面取, 因为是全量的(按key进行restore), 所以没有顺序之分, 可以并发执行 for i := 0; i &lt; conf.Options.Parallel; i++ { go func() { defer wg.Done() c := utils.OpenRedisConn(target, auth_type, passwd, conf.Options.TargetType == conf.RedisTypeCluster, tlsEnable) defer c.Close() var lastdb uint32 = 0 for e := range pipe { // e是BinEntry, 全量同步的单位数据 // filterDB控制src, targetDB控制dst if filter.FilterDB(int(e.DB)) { } else { // 这里执行selectDB选择同步的目标DB // selectDB, 写这么多只是为了防止重复selectDB浪费性能 if conf.Options.TargetDB != -1 { if conf.Options.TargetDB != int(lastdb) { lastdb = uint32(conf.Options.TargetDB) utils.SelectDB(c, uint32(conf.Options.TargetDB)) } } else { // 如果不指定targetDB, 则源DB是啥, targetDB就是啥 if e.DB != lastdb { lastdb = e.DB utils.SelectDB(c, lastdb) } } // 根据BinEntry的Key进行过滤 if filter.FilterKey(string(e.Key)) == true { // key白名单 ds.ignore.Incr() continue } else { slot := int(utils.KeyToSlot(string(e.Key))) if filter.FilterSlot(slot) == true { // slot白名单 ds.ignore.Incr() continue } } utils.RestoreRdbEntry(c, e) // restore 到 target } } }() } wg.Wait() }() // 这会阻塞至&lt;-wait信号, 即全量同步完成 for done := false; !done; { select { case &lt;-wait: done = true case &lt;-time.After(time.Second): } // ... 后面都是统计和打日志逻辑, 这里不展示 }}// 永远阻塞func (ds *dbSyncer) syncCommand(reader *bufio.Reader, target []string, auth_type, passwd string, tlsEnable bool) { c := utils.OpenRedisConnWithTimeout(target, auth_type, passwd, readeTimeout, writeTimeout, isCluster, tlsEnable) defer c.Close() // ... 一大段FakeSlaveOffset相关逻辑, 给不支持pSync的用的, 这里不展示 // ... 一大段统计相关逻辑, 这里不展示 go func() { var ( lastdb int32 = 0 bypass = false isselect = false scmd string argv, newArgv [][]byte err error reject bool ) decoder := redis.NewDecoder(reader) // 1. 读取reader, 解析出cmdDetail, 然后发送到sendBuf for { ignoresentinel:= false ignorecmd := false isselect = false resp := redis.MustDecodeOpt(decoder) // 这里是我精简后的代码 // 根据scmd做一些过滤逻辑, 以及对当scmd为Select db时, 对target db的一些处理 if scmd, argv, err = redis.ParseArgs(resp); err != nil { } else { if scmd != &quot;ping&quot; { if strings.EqualFold(scmd, &quot;select&quot;) { s := string(argv[0]) n, err := strconv.Atoi(s) bypass = filter.FilterDB(n) isselect = true } else if filter.FilterCommands(scmd) { ignorecmd = true } if strings.EqualFold(scmd, &quot;publish&quot;) &amp;&amp; strings.EqualFold(string(argv[0]), &quot;__sentinel__:hello&quot;){ ignoresentinel = true } if bypass || ignorecmd || ignoresentinel{ ds.nbypass.Incr() continue } } newArgv, reject = filter.HandleFilterKeyWithCommand(scmd, argv) if bypass || ignorecmd || reject { continue } } if isselect &amp;&amp; conf.Options.TargetDB != -1 { if conf.Options.TargetDB != int(lastdb) { lastdb = int32(conf.Options.TargetDB) ds.sendBuf &lt;- cmdDetail{Cmd: &quot;SELECT&quot;, Args: [][]byte{[]byte(strconv.FormatInt(int64(lastdb), 10))}} } continue } ds.sendBuf &lt;- cmdDetail{Cmd: scmd, Args: newArgv} } }() // 2. 从sendBuf读取出cmd, 回放到target (默认5000个cmd flush一次) go func() { var noFlushCount uint var cachedSize uint64 for item := range ds.sendBuf { length := len(item.Cmd) data := make([]interface{}, len(item.Args)) for i := range item.Args { data[i] = item.Args[i] length += len(item.Args[i]) } err := c.Send(item.Cmd, data...) // 回放cmd noFlushCount += 1 if noFlushCount &gt;= conf.Options.SenderCount || cachedSize &gt;= conf.Options.SenderSize || len(ds.sendBuf) == 0 { // 5000 ds in a batch err := c.Flush() noFlushCount = 0 cachedSize = 0 } } }() // 3. 永远阻塞, 每1s做一次统计 for lstat := ds.Stat(); ; { time.Sleep(time.Second) nstat := ds.Stat() var b bytes.Buffer fmt.Fprintf(&amp;b, &quot;dbSyncer[%v] sync: &quot;, ds.id) fmt.Fprintf(&amp;b, &quot; +forwardCommands=%-6d&quot;, nstat.forward-lstat.forward) fmt.Fprintf(&amp;b, &quot; +filterCommands=%-6d&quot;, nstat.nbypass-lstat.nbypass) fmt.Fprintf(&amp;b, &quot; +writeBytes=%d&quot;, nstat.wbytes-lstat.wbytes) log.Info(b.String()) lstat = nstat }} 四. 总结 简单来说, 这个迁移原理就是假装为slave (利用pSync), 接收源redis的rdb然后restore到目标redis实现全量同步, 然后继续接收源redis的增量cmd然后回放到目标redis实现增量同步 附官方架构图cluster版 附官方架构图standalone版","link":"/2022/07/09/redisShake%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Redis源码阅读-事件模型ae","text":"源码文件 src/ae.c 入口函数 src/ae.c下的void aeMain(aeEventLoop *eventLoop)函数; 推荐从这个函数开始阅读 1234567891011121314/* * 事件处理器的主循环 */void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { // 如果有需要在事件处理前执行的函数，那么运行它 if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); // 开始处理事件 aeProcessEvents(eventLoop, AE_ALL_EVENTS); }} 我们着重看下aeMain里面aeProcessEvents(eventLoop, AE_ALL_EVENTS)做了什么; 这里我们留意一下里面的aeApiPoll函数, 该函数用于获取可执行的事件, 获取之后在下面的for循环中处理事件, 执行事件处理器fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask) aeApiPoll函数是ae模块提供的一个接口, 在ae_epoll.c ae_kqueue.c ae_select.c ae_evport.c都做了相应的具体实现, 也是所谓IO多路复用各平台的具体实现, 目的为了兼容不同平台 备注: 也许你会好奇为啥IO多路复用没有iocp的实现难道windows就没人权吗, 其实redis的官方版本是不支持windows的, windows版本在https://github.com/microsoftarchive/redis由微软团队自己维护, 里面就有ae_wsiocp.c即iocp版的实现 1234567891011121314151617181920212223242526int aeProcessEvents(aeEventLoop *eventLoop, int flags) { ... ... // 处理文件事件 numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j &lt; numevents; j++) { // 从已就绪数组中获取事件 aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; // 读事件 if (fe-&gt;mask &amp; mask &amp; AE_READABLE) { // rfired 确保读/写事件只能执行其中一个 rfired = 1; fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask); } // 写事件 if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) { if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask); } processed++; } ... ...} 通常说的redis的reactor模型(反应堆)其实说的就是aeMain的大循环中aeProcessEvents做的那些事情: 监听网络连接的FD的文件事件---&gt; 获取事件---&gt; 执行事件回调 剩下具体细节不多赘述, 顺着思路看源码即可 参考 Redis 和 IO 多路复用 redis的事件模型详解(结合Reactor设计模式)","link":"/2020/05/05/redis_ae/"},{"title":"Py小玩具-Tcp&amp;Udp&amp;串口调试工具","text":"简介 以前捣鼓嵌入式的时候写的, 就是那种网上搜就一大把的UDP+TCP+串口调试工具, 为了方便定制些功能于是自己搞了一个, 平时自己用, 功能没啥问题 效果如下 界面 十分眼熟的那四种模式...Udp+TcpClient+TcpServer+串口通信 代码 https://github.com/shuoGG1239/TcpUdpSerialPortTool 补充 纯PyQt开发 界面右下角的AOP功能目的主要用于拦截发送和接收的数据, 然后在脚本中做额外处理, 一般场景用不到可无视...","link":"/2019/03/31/tcpudp_tool/"},{"title":"博客主题icarus样式魔改","text":"简介 博客主题由next换成icarus, 官方默认样式不大满意, 我做了些调整, 这里记下调整过程 预览(前者官方, 后者魔改) xxx 小组件调整 自带的小组件太杂了, 这里我只保留了总览, 分类, 标签, 近期文章 修改_config.icarus.yml的widgets, 仅保留type为profile, categories, recent_posts, tags这4个, 其他删掉或注释掉即可 代码样式调整 修改_config.icarus.yml的article/highlight/theme, 默认是用atom-one-light, 我改用vs即vscode的样式 标题样式调整 原大标题字体没加粗处理, 跟文章content看起来不好区分, 所以做了加粗处理 修改include/style/article.styl文件, 在h1到h5项分别加上font-weight: 600, 如下: 12345678910111213141516171819202122232425262728293031323334&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size h1 font-weight: 600 font-size: 1.75em h2 font-weight: 600 font-size: 1.5em h3 font-weight: 600 font-size: 1.25em h4 font-weight: 600 font-size: 1.125em h5 font-weight: 600 font-size: 1em 文章宽度和留空区宽度调整 默认的文章宽度太小, 两边的留空区太大, 文章内部就显得拥挤 调整include/style/base.styl的基础变量$gap, 由64px改为32px 12345$gap ?= 32px$tablet ?= 769px$desktop ?= 1088px$widescreen ?= 1280px$fullhd ?= 1472px 调整include/style/responsive.styl里面containter的width, container其实就是文章容器, 这里把默认值2 * $gap改成1 * $gap 12345678910111213+widescreen() .is-1-column .container, .is-2-column .container max-width: $widescreen - 1 * $gap width: $widescreen - 1 * $gap+fullhd() .is-2-column .container max-width: $fullhd - 1 * $gap width: $fullhd - 1 * $gap .is-1-column .container max-width: $desktop - 1 * $gap width: $desktop - 1 * $gap 调整layout/common/widgets.jsx的getColumnSizeClass, 将case 2的is-4-tablet is-4-desktop is-4-widescreen替换为is-2-tablet is-2-desktop is-2-widescreen 123456789function getColumnSizeClass(columnCount) { switch (columnCount) { case 2: return 'is-2-tablet is-2-desktop is-2-widescreen'; case 3: return 'is-3-tablet is-3-desktop is-3-widescreen'; } return '';} 调整layout/layout.jsx, 将'is-8-tablet is-8-desktop is-8-widescreen': columnCount === 2替换为'is-8-tablet is-9-desktop is-9-widescreen': columnCount === 2 12345678&lt;div class={classname({ column: true, 'order-2': true, 'column-main': true, 'is-12': columnCount === 1, 'is-8-tablet is-9-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-9-desktop is-6-widescreen': columnCount === 3})} dangerouslySetInnerHTML={{ __html: body }}&gt;&lt;/div&gt; 侧边栏的profile 由于侧边栏被我调窄了, 头像显得太大, 所以这里调下layout/widget/profile.jsx的figure, 将128x128替换为64x64 1234567891011&lt;div&gt; &lt;figure class=&quot;image is-64x64 mx-auto mb-2&quot;&gt; &lt;img class={'avatar' + (avatarRounded ? ' is-rounded' : '')} src={avatar} alt={author} /&gt; &lt;/figure&gt; {author ? &lt;p class=&quot;title is-size-4 is-block&quot; style={{'line-height': 'inherit'}}&gt;{author}&lt;/p&gt; : null} {authorTitle ? &lt;p class=&quot;is-size-6 is-block&quot;&gt;{authorTitle}&lt;/p&gt; : null} {location ? &lt;p class=&quot;is-size-6 is-flex justify-content-center&quot;&gt; &lt;i class=&quot;fas fa-map-marker-alt mr-1&quot;&gt;&lt;/i&gt; &lt;span&gt;{location}&lt;/span&gt; &lt;/p&gt; : null}&lt;/div&gt;","link":"/2024/11/27/%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98icarus%E6%A0%B7%E5%BC%8F%E9%AD%94%E6%94%B9/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"tls","slug":"tls","link":"/tags/tls/"},{"name":"grpc","slug":"grpc","link":"/tags/grpc/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"Qt","slug":"Qt","link":"/tags/Qt/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"}],"categories":[{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"PC端","slug":"PC端","link":"/categories/PC%E7%AB%AF/"},{"name":"db","slug":"db","link":"/categories/db/"},{"name":"后端","slug":"后端","link":"/categories/%E5%90%8E%E7%AB%AF/"},{"name":"Ai","slug":"Ai","link":"/categories/Ai/"}],"pages":[{"title":"文章分类","text":"","link":"/categories/index.html"},{"title":"文章分类","text":"","link":"/tags/index.html"}]}