{"posts":[{"title":"2020年的绘画之路","text":"绘画之路 今年年初买了正版CSP, 开始了正式的练画之路 完整过了遍美术基础的视频, 从透视到色彩理论, 但感觉没啥卵用, 画画这事还是得靠堆练习量, 从小到大涂涂画画, 咱没科班基础, 但手感还有的 今年涂涂画画, 没画出什么成品, 草稿一堆…糊糊涂涂一年就过去了 总结 虽然完全不及原计划, 但今年还是花了时间去画的了. 明年尝试画点作品, 总不能一直画草稿吧","link":"/2020/12/20/2020%E5%B9%B4%E7%9A%84%E7%BB%98%E7%94%BB%E4%B9%8B%E8%B7%AF/"},{"title":"2021年我都画了些什么鬼?","text":"赶在2021结束前画了只苏芳 今年画的最满意的一只, 我是尽力还原原作的画风了, 堆了30多个图层, 算是大工程了 甚至录下了绘画过程: https://www.bilibili.com/video/BV1TT4y1m7zj 一些杂杂的涂鸦 今年的练习量 今年练的比较勤快, 留下了些杂七杂八的草稿和涂鸦 总结 今年终于是画了些成品作品, 但感觉还是少了, 就那么几幅, 希望明年能多画些作品","link":"/2021/12/29/2021%E5%B9%B4%E6%88%91%E9%83%BD%E7%94%BB%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%E9%AC%BC/"},{"title":"2022年我都画了些什么鬼","text":"设计了只广工酱 红白配色, 头饰为校花紫荆花, 设计元素比较简约 还蛮受欢迎的, 做了梗图 附初稿纪念下 今年的练习量 今年摸鱼了, 这悲惨的练习量…","link":"/2022/12/22/2022%E5%B9%B4%E6%88%91%E9%83%BD%E7%94%BB%E4%BA%86%E4%BA%9B%E4%BB%80%E4%B9%88%E9%AC%BC/"},{"title":"QQ机器人qqbot把玩","text":"关于qqbot的官方sdk 都2024年7月了, 有些sdk的最新更新时间还停留在去年, 最新的也就python版本, 这里我用的是botpy https://github.com/tencent-connect/botpy QQ群发图的问题 botpy目前这个版本还没支持QQ群发base64图的功能, 这意味着必须先把图送到图床才行, 麻烦! 翻阅接口文档我发现其实接口上已经是支持了,但是所有语言的sdk没有一个支持就离谱 这时候需要魔改下botpy源码, 很简单, 找到botpy/api.py, 改下post_group_file这个方法, 加一个file_data的参数即可 123456789101112131415161718192021async def post_group_file( self, group_openid: str, file_type: int, url: str, file_data: str, # 新增该参数即可 srv_send_msg: bool = False,) -&gt; message.Media: &quot;&quot;&quot; 上传/发送群聊图片 Args: group_openid (str): 您要将消息发送到的群的 ID file_type (int): 媒体类型：1 图片png/jpg，2 视频mp4，3 语音silk，4 文件（暂不开放） url (str): 需要发送媒体资源的url srv_send_msg (bool): 设置 true 会直接发送消息到目标端，且会占用主动消息频次 &quot;&quot;&quot; payload = locals() payload.pop(&quot;self&quot;, None) route = Route(&quot;POST&quot;, &quot;/v2/groups/{group_openid}/files&quot;, group_openid=group_openid) return await self._http.request(route, json=payload) 吐槽下QQBot 咱开发这机器人无非就是要和QQ群里面的小伙伴们一起耍, 结果个人开发者开发的机器人只能在沙箱群里玩, 沙箱群最多就20人, 那还玩个蛋? 看官方文档和SDK的更新情况, 感觉这个项目大概是要凉了 希望有朝一日QQBot的产品能把它盘活了, 目前这玩意, 我是不想继续折腾了, 88 最后贴个我的Demo供参考1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import botpyfrom botpy import logging, Intentsfrom botpy.message import GroupMessageclass Bot(botpy.Client): img_gen_running = False app_id: int = 0 app_secret: str = '' def __init__(self, intents: Intents): super().__init__(intents) self.app_id, self.app_secret = load_qqbot_cfg() tc_app_id, tc_app_secret = load_tecentcloud_cfg() tran = lang_translate.LanguageTranslater(tc_app_id, tc_app_secret) if tc_app_id != '' else None self.qq_msg_parser = qq_msg_parser.QQMsgParser(tran) self.ai_chat = ai_chat.AiChat(tc_app_id, tc_app_secret) async def on_group_at_message_create(self, message: GroupMessage): print(message.content) prompt_type, prompt = self.qq_msg_parser.msg_to_prompt(message.content) msg_req = 0 if prompt_type == qq_msg_parser.PROMPT_TYPE_IMG: await self.resolve_img_gen(message, msg_req, prompt) else: # PROMPT_TYPE_TEXT case await self.resolve_chat(message, msg_req, prompt) async def resolve_img_gen(self, message: GroupMessage, msg_req: int, img_prompt: str): if self.img_gen_running: return self.group_reply_txt(message, msg_req, &quot;aris手里的画没画完呐~等画完再发一遍给aris喵~&quot;) msg_req += 1 await self.group_reply_txt(message, msg_req, &quot;aris画得比较慢,要等半分钟喵~&quot;) msg_req += 1 self.img_gen_running = True gen_code, file_or_msg = await img_gen.generate_img(img_prompt) self.img_gen_running = False _log.info(f'generate_img(&quot;{img_prompt}&quot;): {gen_code} {file_or_msg}') if gen_code == 200: jpg_data = await img_gen.get_jpg_from_png(file_or_msg) jpg_data_b64 = base64.standard_b64encode(jpg_data).decode() await self.group_reply_image_data(message, msg_req, jpg_data_b64) else: await self.group_reply_txt(message, msg_req, &quot;aris这张不想画了,换别的吧 T_T&quot;) async def resolve_chat(self, message: GroupMessage, msg_req: int, prompt: str): reply_txt = self.ai_chat.chat(prompt) await self.group_reply_txt(message, msg_req, reply_txt) async def group_reply_txt(self, message: GroupMessage, msg_req: int, txt: str): await self.api.post_group_message( group_openid=message.group_openid, msg_type=0, msg_id=message.id, msg_seq=msg_req, content=txt) async def group_reply_image_data(self, message: GroupMessage, msg_req: int, img_b64: str): upload_media = await self.api.post_group_file( group_openid=message.group_openid, file_type=1, url='', file_data=img_b64, ) await self.api.post_group_message( group_openid=message.group_openid, msg_type=7, msg_id=message.id, msg_seq=msg_req, media=upload_media, ) async def group_reply_image_url(self, message: GroupMessage, msg_req: int, file_url: str): upload_media = await self.api.post_group_file( group_openid=message.group_openid, file_type=1, url=file_url, file_data='', ) await self.api.post_group_message( group_openid=message.group_openid, msg_type=7, msg_id=message.id, msg_seq=msg_req, media=upload_media, )","link":"/2024/07/30/QQ%E6%9C%BA%E5%99%A8%E4%BA%BAqqbot%E6%8A%8A%E7%8E%A9/"},{"title":"ai图手脚崩坏如何修复","text":"先看看修复效果 这是原ai图, 是用sd的一个微调模型生成的: 这是修复后的图: 怎么修复? 重绘? 用sd的inpaint局部重绘? 然后加上controlNet? 各种教程的都教人这么去搞, 然后效果就是一坨! 首先重绘出一只完美的手就要换时间不停重试,好不容易roll出一只好手, 重绘区域却和整体图片有违和感, 对于违和感是0容忍的, 对于一幅插画来说, 这比手脚崩坏严重多了; 在无数次重试,我发现几个小时就过了… 总结, 重绘就是陷阱, 不要陷进去, 至少目前是这样的, 未来不好说 半小时人工修好还费那劲在那roll inpaint? 我的评价是: 直接动笔, 半小时的事情, 效果是完美的","link":"/2023/12/06/ai%E5%9B%BE%E6%89%8B%E8%84%9A%E5%B4%A9%E5%9D%8F%E5%A6%82%E4%BD%95%E4%BF%AE%E5%A4%8D/"},{"title":"axios的ES module (esm)","text":"背景 刚好遇到某个场景需要用到es module, axios用习惯了, 不过axios官方没有esm版本 https://github.com/axios/axios/issues/1879 解决 可以用第三方: https://github.com/bundled-es-modules/axios 使用起来很简单, 直接用里面的axios.js即可12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;ESM-test&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script type=&quot;module&quot;&gt; import axios from './axios.js' axios.get('http://127.0.0.1:8888/shuogg').then(resp =&gt; { console.log(resp.data) }).catch(e =&gt; { console.log(e) })&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"/2020/06/05/axio_esm/"},{"title":"Jetbrains Clion官方支持了Stm32的项目搭建, 说下感想","text":"背景 得知Clion 2019.1之后的版本官方直接支持Stm32项目的创建, 遂怀揣激动之心准备一试… 吐槽 照着别人的教程, 一顿操作猛如虎, 一会捣鼓OpenOCD, 一会捣鼓arm-none-eabi-gcc… …说实话, 过程挺麻烦的, 会遇到一些坑 手头上只有一块老stm32的核心板还有一个Jlink, 烧写调试也只能靠Jlink. 结果捣鼓了老半天, Jlink这块没办法打通, 即没办法用Jlink愉快地Debug, 遂放弃 结论 现阶段还不完善, 该用keil的还是得用keil (当然也有可能只是我的搭建姿势有问题, 望指教) 期待未来某一天Clion能够拳打Keil脚踩IAR 参考教程 Clion下开发STM32 用clion自带的嵌入式开发功能和stm32cubeMX开发stm32!!!","link":"/2020/05/05/clion_stm32/"},{"title":"Py小玩具-简易取色器","text":"简单的拾色器 最近遇到几次取屏幕某处颜色的场景, 用ps去取色又觉得有点麻烦(步骤太多我懒), 索性自己做一个简单的拾色器 功能极简单就是取屏幕某处的色号, 按下空格把颜色记录下来… 效果 思路 定时截取屏幕然后取下鼠标位置的像素颜色 1234567891011121314def catch(self): x = QCursor.pos().x() y = QCursor.pos().y() pixmap = QGuiApplication.primaryScreen().grabWindow(QApplication.desktop().winId(), x, y, 1, 1) if not pixmap.isNull(): image = pixmap.toImage() if not image.isNull(): if (image.valid(0, 0)): color = QColor(image.pixel(0, 0)) r, g, b, _ = color.getRgb() self.nowColor = color self.ui.lineEditMove.setText('(%d, %d, %d) %s' % (r, g, b, color.name().upper())) self.ui.lineEditMove.setStyleSheet('QLineEdit{border:2px solid %s;}' % (color.name())) 依赖 PyQt5 代码 https://github.com/shuoGG1239/ColorCatcher","link":"/2019/06/09/color_catcher/"},{"title":"danbooru爬图","text":"简介 最近ai炼丹嫌找炼丹素材麻烦, 就写了个按danbooru tag爬图的脚本, 没什么第三方依赖, 想用直接copy去跑就行 使用方法 把脚本里面的your_username和your_password替换成你的danbooru账号密码(账号在https://danbooru.donmai.us注册) main里面, 在tag填上要爬取的tag, save_dir为爬取的图片的保存路径 如果需要通过代理爬取, 把use_proxy置为True, 然后改下proxies变量的代理地址即可 脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# -*- coding: utf-8 -*-import osimport urllib.parseimport requestsfrom requests.auth import HTTPBasicAuthuse_proxy = Trueheaders = { 'User-Agent': 'db/v1.0.0', 'Content-Type': 'application/json; charset=utf-8',}proxies = {&quot;http&quot;: &quot;socks5://127.0.0.1:1080&quot;, 'https': 'socks5://127.0.0.1:1080'}auth = HTTPBasicAuth('your_username', 'your_password')session = requests.session()def save_file(file_url, path): response = request_get(file_url) with open(path, 'wb') as f: f.write(response.content) f.flush()def request_get(url): if use_proxy: return session.get(url, headers=headers, proxies=proxies, auth=auth) else: return session.get(url, headers=headers, auth=auth)def get_all_danbooru_items(tag_list): &quot;&quot;&quot; :param tag_list: :return: list of danbooru item &quot;&quot;&quot; items = [] cur_page = 1 while True: page_items = get_danbooru_items(tag_list, cur_page) if len(page_items) == 0: break items.extend(page_items) cur_page += 1 return itemsdef get_danbooru_items(tag_list, page=1, limit=100): &quot;&quot;&quot; API文档 https://danbooru.donmai.us/wiki_pages/api%3Aposts :param tag_list: :param page: 从1开始 :param limit: :return: list of danbooru item &quot;&quot;&quot; clean_tag_list = list(map(lambda x: x.replace(' ', '_'), tag_list)) query = { 'tags': ' '.join(clean_tag_list), 'page': page, 'limit': limit, } posts_url = 'https://danbooru.donmai.us/posts.json?%s' % (urllib.parse.urlencode(query)) print(posts_url) resp = request_get(posts_url) return resp.json()def count_danbooru_items(tag_list): &quot;&quot;&quot; :param tag_list: :return: int &quot;&quot;&quot; clean_tag_list = list(map(lambda x: x.replace(' ', '_'), tag_list)) query = { 'tags': ' '.join(clean_tag_list), } posts_url = 'https://danbooru.donmai.us/counts/posts.json?%s' % (urllib.parse.urlencode(query)) print(posts_url) resp = request_get(posts_url) return resp.json()['counts']['posts']def download_imgs_by_tags(tags, save_dir): items = get_all_danbooru_items(tags) sub_folder_name = '_'.join(tags) folder_path = os.path.join(save_dir, sub_folder_name) if not os.path.exists(folder_path): os.mkdir(folder_path) total = len(items) cnt = 0 print(&quot;downloading&quot;, sub_folder_name, 'total:', total) for item in items: img_id, img_url = item['id'], item.get('file_url') if img_url is None: print('emtpy file_url:', item) continue img_suffix = img_url[img_url.rindex('.'):] img_file_name = '%d%s' % (img_id, img_suffix) img_file_path = os.path.join(folder_path, img_file_name) if os.path.exists(img_file_path): print(img_file_name, 'exists! pass...') continue save_file(img_url, img_file_path) cnt += 1 print('(%d/%d)%s download ok!' % (cnt, total, img_file_name)) print('all done!')# https://danbooru.donmai.us/if __name__ == '__main__': use_proxy = False save_dir = './danbooru_imgs' tag = 'mari_(blue_archive)' cnt = count_danbooru_items([tag]) print(cnt) if cnt &gt; 0: download_imgs_by_tags([tag], save_dir)","link":"/2023/08/13/danbooru%E7%88%AC%E5%9B%BE/"},{"title":"Py小玩具-简易截图","text":"简介 有时没开微信或QQ的时候想立即截个图啥的挺蛋疼的, 故自己捣鼓一个 简单干净快速, 无界面, 运行即截图 使用方式 运行easyshot.py直接开启区域截图, 截完图自动保存到桌面, 然后退出进程 截图的时候框选完区域后可以双击或Enter完成截图, 中途可以按Esc放弃截图 环境 python3 pyQt5 代码 https://github.com/shuoGG1239/EasyScreenShot","link":"/2020/05/10/easyscreenshot/"},{"title":"gh-ost源码分析","text":"简述 之前用到了gh-ost做大表改表工具, 回过来看看源码, 本篇为阅读源码的笔记 源码信息 源码版本: 源码仓库: https://github.com/github/gh-ost gh-ost原理 gh-ost 首先连接到主库上，根据 alter 语句创建幽灵表，然后作为一个”备库”连接到其中一个真正的备库上(默认设置,想连到master也行) 一边在主库上拷贝已有的数据到幽灵表，一边从备库上拉取增量数据的 binlog，然后不断的把 binlog 应用回主库 图中 cut-over 是最后一步，锁住主库的源表，等待 binlog 应用完毕，然后替换 gh-ost 表为源表 gh-ost 在执行中，会在原本的 binlog event 里面增加以下 hint 和心跳包，用来控制整个流程的进度，检测状态等 gh-ost的改表流程 检查有没有外键和触发器。 检查表的主键信息。 检查是否主库或从库，是否开启log_slave_updates，以及binlog信息 检查gho和del结尾的临时表是否存在 创建ghc结尾的表，存数据迁移的信息，以及binlog信息等 初始化stream的连接,添加binlog的监听 创建gho结尾的临时表，执行DDL在gho结尾的临时表上 开启事务，按照主键id把源表数据写入到gho结尾的表上，再提交，以及binlog apply。 lock源表，rename 表：rename 源表 to 源_del表，gho表 to 源表。(这个过程叫cut-over) 清理ghc表。 关于v1.1.6修复的时区问题 在_gho表执行sql的session和binlog读取时指定的时区不一致导致 从binlogEvent读取的时间结构体是带了时区的, 该时区是由BinlogParser.timestampStringLocation指定, 在转换成query时会用timestamp结合时区生成时间String 强调: 不管是mysql底层还是binlog中, timestamp是不带时区的, 就是4个bytes; github.com/go-mysql-org/go-mysql在时区强制指定utc, 新版本变成可配置并默认为系统时区, 所以是go-mysql没考虑兼容性导致 源码 /base包: 相当于config, 处理配置信息和日志工具 /sql包: 相当于sql_parser, 处理sql解析的工具包 /mysql包: 相当于mysql相关的util包 /binlog包: 仅仅是对replication.BinlogSyncer的封装, 最后将replication.RowsEvent封装成BinlogEntry塞到EventsStreamer.eventsChannel里面 /logic/server.go 提供接口, 主要用于动态设置一些运行参数 /logic/hooks.go: 执行hook. 按规定的执行程序名字, 将执行程序放入指定目录, 后面会根据事件执行这些执行程序 /logic/streamer.go: 在上面binlog包的基础上再封装一层listener, listener处理上面提到的EventsStreamer.eventsChannel接收的BinlogEntry /logic/inspect.go: 连接slave, 获取实例的基础信息如表结构, 表大小等, 检查改表是否符合迁移条件 /logic/throttler.go: 限流器, 调用throttle()会卡住以实现限流 /logic/applier.go gho和ghc的处理包括cutOver, 提供实现. 调用都在Migrator ApplyDMLEventQueries(dmlEvents ): 将binlogEvent转为query, 然后在_gho表执行 /logic/migrator.go 主流程. 上述各个模块提供的方法会在migrator中使用, 完成整个改表流程","link":"/2024/05/22/gh-ost%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"go module笔记与源码分析","text":"简介 零零散散的关于go module的笔记, 通过源码来理解这些点 go mod源码位置 仓库位置: https://github.com/golang/go/tree/go1.16.6/src/cmd/go/internal/modcmd 相对位置: cmd/go/internal/modcmd cmd/go/internal/modfetch cmd/go/internal/modget cmd/go/internal/modload 关于版本选择 同一个大版本取最大版本号 main依赖了(A1.1.0, B1.2.0), B1.2.0依赖了(A1.0.0), 则最终构建阶段大家都用A1.1.0编译 版本从信息从/@v/list获取, 如果为空则取走@latest获取最新版本号, 在拿最新版本号取拉包 官方版本规则 核心逻辑 1234567891011121314151617181920func (p *proxyRepo) Versions(prefix string) ([]string, error) { data, err := p.getBytes(&quot;@v/list&quot;) // 获取list文件内容 if err != nil { return nil, p.versionError(&quot;&quot;, err) } var list []string for _, line := range strings.Split(string(data), &quot;\\n&quot;) { f := strings.Fields(line) if len(f) &gt;= 1 &amp;&amp; semver.IsValid(f[0]) &amp;&amp; strings.HasPrefix(f[0], prefix) &amp;&amp; !IsPseudoVersion(f[0]) { list = append(list, f[0]) } } SortVersions(list) // 对list里面的版本进行排序 return list, nil}// IsPseudoVersion reports whether v is a pseudo-version.func IsPseudoVersion(v string) bool { return strings.Count(v, &quot;-&quot;) &gt;= 2 &amp;&amp; semver.IsValid(v) &amp;&amp; pseudoVersionRE.MatchString(v)} go private private时不会走proxy和checksumDB1234567// go/internal/modfetch/sumdb.gofunc useSumDB(mod module.Version) bool { return cfg.GOSUMDB != &quot;off&quot; &amp;&amp; !module.MatchPrefixPatterns(cfg.GONOSUMDB, mod.Path) // cfg.GONOSUMDB里面包含了GOPRIVATE}// cmd/go/internal/cfg/cfg.goGONOSUMDB = envOr(&quot;GONOSUMDB&quot;, GOPRIVATE) @latest v0.0.5 &gt; v0.0.5-alpha &gt; v0.0.4123456789101112131415161718192021222324// 版本比较的源码: src/cmd/vendor/golang.org/x/mod/semver/semver.gofunc Compare(v, w string) int { pv, ok1 := parse(v) pw, ok2 := parse(w) if !ok1 &amp;&amp; !ok2 { return 0 } if !ok1 { return -1 } if !ok2 { return +1 } if c := compareInt(pv.major, pw.major); c != 0 { return c } if c := compareInt(pv.minor, pw.minor); c != 0 { return c } if c := compareInt(pv.patch, pw.patch); c != 0 { return c } return comparePrerelease(pv.prerelease, pw.prerelease)} indirect main依赖了A, 但是A依赖了B但go.mod里没有require B, 则A的go.mod会自动加上B indirect 此时main的go.mod强行require上B, 则B indirect将消失 exclude 跳过某个版本 (之后一般gomod会自动使用比跳过版本更高的版本) 例如: “require github.com/google/uuid v1.1.0”, 最后tidy后require里自动变成”github.com/google/uuid v1.1.1” 跟replace一样, 仅main module时生效 不大实用, 一般直接用replace即可 url GET $base/$module/@v/list https://goproxy.io/github.com/gin-gonic/gin/@v/v1.4.0.mod https://goproxy.io/github.com/gin-gonic/gin/@v/ https://goproxy.io/github.com/gin-gonic/gin/@v/list //latest是根据这个list拉取最大的版本 https://goproxy.io/github.com/gin-gonic/gin/@latest go.sum https://sum.golang.org/lookup/golang.org/x/sync@v0.0.0-20181221193216-37e7f081c4d4 checksum源码: go/src搜索 checkModSum(mod, hash) checksumDB源码: go/src搜索 checkSumDB(mod, h) commit version go get github.com/pingcap/parser@659821e go get github.com/pingcap/parser@latest go get github.com/pingcap/parser@feature-lstest","link":"/2021/06/09/go%20module%E7%AC%94%E8%AE%B0%E4%B8%8E%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"golang&#x2F;net包与epoll","text":"net包与epoll linux下go的网络包底层如tcp也是采用epoll来实现, 你可以从Accept方法一路追下去, 追到尽头你会看到internal/poll/fd_poll_runtime.go里面这些在runtime实现的方法:123456789func runtime_pollServerInit()func runtime_pollOpen(fd uintptr) (uintptr, int)func runtime_pollClose(ctx uintptr)func runtime_pollWait(ctx uintptr, mode int) intfunc runtime_pollWaitCanceled(ctx uintptr, mode int) intfunc runtime_pollReset(ctx uintptr, mode int) intfunc runtime_pollSetDeadline(ctx uintptr, d int64, mode int)func runtime_pollUnblock(ctx uintptr)func runtime_isPollServerDescriptor(fd uintptr) bool 此时到src/runtime/netpoll.go就能看到上述这些方法的实现, 再往下追下去就可以看到各个平台的具体实现了, 如netpoll_epoll.go netpoll_kqueue.go netpoll_windows.go, 看到netpoll_epoll.go里面的epollcreate, epollctl, epollwait了吧, 多么熟悉的几个函数! goroutine与epoll 虽然net包底层用epoll实现了, 但是实际我们在用tcp还是开goroutine来serve net包就是推荐我们用goroutine来玩tcp, 应对大部分场景妥妥的 面对比较变态的场景并发量贼高时, goroutine尽管只有消耗2k~8k的栈空间, 连接一多还是耗不起, 此时就只能用一些黑魔法来使用epoll了 具体怎么玩可以参照 https://github.com/mailru/easygo","link":"/2020/06/07/go_and_epoll/"},{"title":"go源码阅读:database&#x2F;sql包","text":"简介 database/sql最主要还是实现了连接池逻辑 go源码版本: v1.16 源码仓库: https://github.com/golang/go/tree/go1.16.6/src/database/sql sql.DB的一些关键逻辑 Open会返回DB对象并开启一条connectionOpener线程 connectionOpener主要处理下面提到的”当前连接数大于maxOpen会陷入等待”的连接资源请求 DB的核心方法: conn(ctx context.Context, strategy connReuseStrategy) (*driverConn, error) 优先返回连接池里的连接 当前连接数大于maxOpen会陷入等待, 等待取决于ctx, 也即由调用方控制 其余情况则返回一个新连接 DB的连接池: freeConn []*driverConn DB的核心方法: putConnDBLocked(conn, err) bool 如果当前连接数大于maxOpen则直接返回false, 上层看到false则直接关闭该连接conn 优先满足正在等待连接资源请求的(connRequests是一个map,可以看出请求连接资源并无优先级的说法) 如果MaxIdleConns &lt; len(freeConn), 即连接池满了, 则直接返回false, 上层看到false则直接关闭该连接conn 否则丢到连接池freeConn中 清理线程(connectionCleaner): SetConnMaxLifetime,SetConnMaxIdleTime时才会去起唯一的一条清理线程 清理线程定期清理连接池freeConn的连接, 根据maxLifetime和createAt清理, 也根据maxIdleTime和returnedAt清理 有趣的是清理线程并不理会MaxOpen和MaxIdleConns是多少, 只关注MaxLifetime和MaxIdleTime, 反正过期了就清理 sql执行方法如Query,Exec,Ping等, 都会先去调conn获取连接, 再用其连接执行sql, 最后将putConnDBLocked(conn), (query是等rows全部scan完close再putConnDBLocked(conn)) 关于resetSession最终的去处, 总的来说就是reset了个寂寞, 最终居然只是conn.SetReadDeadline(time.Time{})? 123456789101112131415161718192021222324252627282930313233343536373839// go-sql-driver/mysql/connection.go里面的// Write packet buffer 'data'func (mc *mysqlConn) writePacket(data []byte) error { pktLen := len(data) - 4 if pktLen &gt; mc.maxAllowedPacket { return ErrPktTooLarge } // Perform a stale connection check. We only perform this check for // the first query on a connection that has been checked out of the // connection pool: a fresh connection from the pool is more likely // to be stale, and it has not performed any previous writes that // could cause data corruption, so it's safe to return ErrBadConn // if the check fails. if mc.reset { mc.reset = false conn := mc.netConn if mc.rawConn != nil { conn = mc.rawConn } var err error // If this connection has a ReadTimeout which we've been setting on // reads, reset it to its default value before we attempt a non-blocking // read, otherwise the scheduler will just time us out before we can read if mc.cfg.ReadTimeout != 0 { err = conn.SetReadDeadline(time.Time{}) } if err == nil &amp;&amp; mc.cfg.CheckConnLiveness { err = connCheck(conn) } if err != nil { errLog.Print(&quot;closing bad idle connection: &quot;, err) mc.Close() return driver.ErrBadConn } } ... ...} 关于statement 关于query带?的sql的逻辑: func (db *DB) queryDC: 对于带了args的query, 且InterpolateParams=false, 则driver会返回driver.ErrSkip, 此时会走到si, err = ctxDriverPrepare(ctx, dc.ci, query), 做完prepare与ctxDriverStmtQuery, 将si放到Rows里面, Rows读完Close后si会跟着Close","link":"/2022/03/27/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_database-sql%E5%8C%85/"},{"title":"go源码阅读:net&#x2F;http的Transport","text":"简介 http.Transport这样的高频使用模块, 源码肯定得看看 go源码版本: v1.16 源码仓库: https://github.com/golang/go/blob/go1.16.6/src/net/http/transport.go http.Transport的关键逻辑 Transport是RoundTripper接口的实现: func RoundTrip(req *Request) (*Response, error) Transport对外只提供方法: func RoundTrip(req *Request) (*Response, error) Transport的内部对象idleLRU connLRU写得不错, 简单实现了LRU http.Client就是在Tranport上简单封装一层 Transport就是一个连接池, 池子里面放着persistConn连接对象(idleConn map[connectMethodKey][]*persistConn) queueForIdleConn: 根据请求的connectMethodKey从t.idleConn获取一个[]*persistConn切片， 并从切片中，根据算法获取一个有效的空闲连接。如果未获取到空闲连接，则将wantConn结构体放入t.idleConnWait[w.key]等待队列 连接释放逻辑在 (t *Transport) tryPutIdleConn(pconn *persistConn) 哪些情况才回去调 tryPutIdleConn: 大部分的异常情况 responseBody read完: 代码详细见 case bodyEOF := &lt;-waitForBodyRead dialConnFor: 会调用t.dialConn获取一个真正的*persistConn。并将这个连接传递给w, 如果w已经获取到了连接，则会传递失败，此时调用t.putOrCloseIdleConn将连接放回空闲连接池。 dialConn: 调用t.dial(ctx, “tcp”, cm.addr())创建TCP连接并将其赋予刚new的persistConn 如果是https的请求，则对请求建立安全的tls传输通道 为persistConn创建读写buffer，如果用户没有自定义读写buffer的大小，读写bufffer的大小默认为4096 执行go pconn.readLoop()和go pconn.writeLoop()开启读写循环然后返回连接 dialConn里面这段代码是开启http2的核心 12345678910111213// 当client和server都支持http2时，s.NegotiatedProtocol的值为&quot;h2&quot;且s.NegotiatedProtocolIsMutual的值为true// pconn.tlsState是在pconn.addTLS中附加的// 所以是否支持http2是在tls握手时得知的, 因此http2时强制要求httpsif s := pconn.tlsState; s != nil &amp;&amp; s.NegotiatedProtocolIsMutual &amp;&amp; s.NegotiatedProtocol != &quot;&quot; { if next, ok := t.TLSNextProto[s.NegotiatedProtocol]; ok { alt := next(cm.targetAddr, pconn.conn.(*tls.Conn)) if e, ok := alt.(erringRoundTripper); ok { // pconn.conn was closed by next (http2configureTransports.upgradeFn). return nil, e.RoundTripErr() } return &amp;persistConn{t: t, cacheKey: pconn.cacheKey, alt: alt}, nil }} 关于persistConn persistConn是在给”conn net.Conn”包一层 readLoop: for循环, 不停等待新的requestAndChan(由roundTrip发起), response塞回requestAndChan(rc.ch &lt;- responseAndError{res: resp}); 确认读完之后调tryPutIdleConn放回Transport的连接池 只有当调用方完整的读取了响应，该连接才能够被复用。 因此在http1.1中，1个连接上的请求，只有等前一个请求处理完之后才能继续下一个请求。 如果前面的请求处理较慢， 则后面的请求必须等待， 这就是http1.1中的线头阻塞 所以就算你不关心response的body, 也必须把body读完以保持连接的复用, 可以如下处理 io.CopyN(ioutil.Discard, resp.Body, 2 &lt;&lt; 10) resp.Body.Close() writeLoop: for循环, 不停等待新的writeRequest, 写完发信号给pc.writeErrCh和wr.ch, 出错了会关闭该persistConn并结束writeLoop的循环, 否则继续等待新的writeRequest roundTrip: 在for循环里面等待本次roundTrip的各种信号, 如来自writeLoop的写完成信号, pcClosed, cancelChan, response结果信号等, 收到response或者出错则结束循环. 只有在出错的时候才会pc.close; roundTrip里面没有处理连接池的逻辑 关于http.Response http/response.go: 核心函数 func ReadResponse(r *bufio.Reader, req *Request) (*Response, error) 关于HeaderTimeout, 源码可看出是读完所有header才算header读结束了(blank line), HeaderTimeout是header读结束的timeout response.wroteHeader: writeHeader里面会将wroteHeader置为true func (cw *chunkWriter) writeHeader(p []byte)的最后一行是: w.conn.bufw.Write(crlf) response的flush本质就是writeHeader(nil), 也就是最后也会w.conn.bufw.Write(crlf) 关于http.DefaultTransport123456789101112var DefaultTransport RoundTripper = &amp;Transport{ Proxy: ProxyFromEnvironment, // &quot;HTTP_PROXY&quot;,&quot;http_proxy&quot;,&quot;HTTPS_PROXY&quot;,&quot;https_proxy&quot;,&quot;NO_PROXY&quot;,&quot;no_proxy&quot; DialContext: (&amp;net.Dialer{ Timeout: 30 * time.Second, KeepAlive: 30 * time.Second, }).DialContext, ForceAttemptHTTP2: true, MaxIdleConns: 100, IdleConnTimeout: 90 * time.Second, TLSHandshakeTimeout: 10 * time.Second, ExpectContinueTimeout: 1 * time.Second,}","link":"/2022/03/20/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_net-http-transport/"},{"title":"go源码阅读:TLS handshake","text":"tls handshake tls/handshake_server.go: serverHandshakeState tls/common.go: Conn.PeerCertificates tls/handshake_client.go: doFullHandshake: handshake读取到的certificateMsg最终会解析后赋值给peerCertificates rfc5246-Handshake Protocol tls/handshake_client.go: clientHandshake makeClientHello loadSession : 会生成sessionID writeRecord : 将ClientHello发送给server readHandshake: 等server回复 tls/handshake_server.go: serverHandshake readClientHello processClientHello pickCipherSuite doFullHandshake establishKeys readFinished sendSessionTicket sendFinished tls.Conn: handlePostHandshakeMessage/handleKeyUpdate: KeyUpdate指的就是tls最后的那个对称密钥(变量名叫trafficSecret) tls/conn.go tls.Conn实现了net.Conn接口, 在Write和Read包裹了一层handShake, 后面的读写也是带了加密的(readRecordOrCCS) net/conn.go12345678910type Conn interface { Read(b []byte) (n int, err error) Write(b []byte) (n int, err error) Close() error LocalAddr() Addr RemoteAddr() Addr SetDeadline(t time.Time) error SetReadDeadline(t time.Time) error SetWriteDeadline(t time.Time) error}","link":"/2023/04/25/go%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB_tls-handshake/"},{"title":"grpc的goAway和keepalive","text":"简介 虽是Http2的东西, 但也可以通过grpc的源码来侧面加深下理解 GoAway 告诉客户端, 服务端准备关闭了, 本连接不要发新请求过来了 (发一半的请求还是会处理完的) 当client收到这个包之后就会主动关闭连接。下次需要发送数据时，就会重新建立连接 流程: client收到Goaway -&gt; client主动关闭http2连接 -&gt; channel变为IDLE -&gt; 用户发起新请求 -&gt; 创建新连接 -&gt; channel变为CONNECTING 另外GoAway是实现优雅关闭的基石, 因为是client主动关闭(不同于服务端关闭), 可以避免很多无效的请求 源码: google.golang.org/grpc/clientconn.go: errConnDrain (drain是接收到goaway后的连接状态) google.golang.org/grpc/internal/transport/http2_client.go: func (t *http2Client) Close(err error) google.golang.org/grpc/internal/transport/http2_client.go: func (t *http2Client) handleGoAway(f *http2.GoAwayFrame) 里面的 t.onClose(t.goAwayReason)的onClose在”grpc/clientconn.go/addrConn createTransport”里面定义的: 123456789101112131415161718192021222324252627282930313233343536373839func (ac *addrConn) createTransport(ctx context.Context, addr resolver.Address, copts transport.ConnectOptions, connectDeadline time.Time) error { addr.ServerName = ac.cc.getServerName(addr) hctx, hcancel := context.WithCancel(ctx) // hctx会深入到http2Client,见newHTTP2Client的信号&quot;&lt;-newClientCtx.Done()&quot; // 客户端收到goAway的反馈: 关闭连接 + state变IDLE onClose := func(r transport.GoAwayReason) { ac.mu.Lock() defer ac.mu.Unlock() // adjust params based on GoAwayReason ac.adjustParams(r) if ctx.Err() != nil { // Already shut down or connection attempt canceled. tearDown() or // updateAddrs() already cleared the transport and canceled hctx // via ac.ctx, and we expected this connection to be closed, so do // nothing here. return } hcancel() // 关闭http2连接! if ac.transport == nil { // We're still connecting to this address, which could error. Do // not update the connectivity state or resolve; these will happen // at the end of the tryAllAddrs connection loop in the event of an // error. return } ac.transport = nil // Refresh the name resolver on any connection loss. ac.cc.resolveNow(resolver.ResolveNowOptions{}) // Always go idle and wait for the LB policy to initiate a new // connection attempt. ac.updateConnectivityState(connectivity.Idle, nil) } connectCtx, cancel := context.WithDeadline(ctx, connectDeadline) defer cancel() copts.ChannelzParentID = ac.channelzID newTr, err := transport.NewClientTransport(connectCtx, ac.cc.ctx, addr, copts, onClose) ... ...} 关于keepalive keepalive在server端是默认开启的, client端默认关闭 keepalive.ServerParameters.MaxConnectionIdle: 如果一个client空闲超过15s, 发送一个 GOAWAY grpc-keepalive-guide RPC客户端长连接机制实现及keepalive分析 源码: 结构: google.golang.org/grpc/internal/transport/http2_server.go: http2Server.kp 结构: google.golang.org/grpc/internal/transport/http2_client.go: http2Client.kp 实现: google.golang.org/grpc/internal/transport/http2_server.go: func (t *http2Server) keepalive() 实现: google.golang.org/grpc/internal/transport/http2_client.go: func (t *http2Client) keepalive()","link":"/2023/08/02/grpc%E7%9A%84goAway%E5%92%8Ckeepalive/"},{"title":"Py小玩具-简单好用的OCR","text":"效果如下 截图识别 图片识别 代码https://github.com/shuoGG1239/Image2Text 介绍 本来一开始是用谷歌的tesseract, 也搞来了据说比较靠谱的trainedData, 但实际识别准确率实在不行, 于是放弃了, 不过这种好处就是可以离线识图, 只要能搞到靠谱好用的trainedData肯定是比在线识图要好啦 这个小工具最终是用了百度AI的OCR接口, 识别率不错, 特别是中文 百度AI的普通文字识别的接口一天最多只能免费调用500次/账号, 也不保证并发量, 想变强就充钱吧 识图的核心代码在上面代码仓库的ocr_util.py, 要注意的是API_KEY和SECRET_KEY的这两个变量要填上自己的key, 这俩key获取直接得到百度AI去拿, 直接登陆控制台–&gt;鼠标移到右上角头像上–&gt;安全认证–&gt; AccessKey, 当然也可以直接用我的Key, 我都直接丢代码里, 但尽量用自己的吧, 说不定某天我不小心把AccessKey删了呢 :P Gui用的是PyQt5, 只支持python3, 所以python2.7的同学可以无视Gui部分…","link":"/2018/07/21/image2text/"},{"title":"Py小玩具-罗马转假名","text":"罗马音转日文假名 罗马音转假名(平假片假), 简单好用… 属于个人需求, 偶尔要敲一段假名, (日文输入法太笨重污染桌面干净的右下角) 算是用的比较频繁的一个自制玩具 效果 使用 无需联网 除了长音符用了yy, 其余输入规则和主流的日文输入法差不多 例子1234567ka かwa わi いlo ぉ (小お)le ぇ (小え)nn んyy ー 实现 纯PyQt5开发 代码 https://github.com/shuoGG1239/JapInput","link":"/2019/06/09/japinput/"},{"title":"llOnebot实现QQ机器人","text":"QQ机器人的近乎完美方案 之前捣鼓qqbot结果让我非常失望, 各种功能限制, onebot这玩意就靠谱多了 onebot缺点仅仅是平台限定windows 部署 安装QQNT: 安装包地址 在安装目录下找到resources\\app\\app_launcher, 到app_launcher目录下创建文件llob.js, 然后写入一句require(String.raw./LiteLoaderQQNT) 在安装目录下找到resources\\app\\app_launcher\\package.json, 将json里面的main对应的值改为”./app_launcher/llob.js” 下载LiteLoaderQQNT.zip: 下载地址 将解压出来的整个文件夹LiteLoaderQQNT复制到第1步的llob.js相同目录下 在LiteLoaderQQNT文件夹内创建一个plugins文件夹 下载LLOneBot.zip: 下载地址 下载dbghelp_x64.dll: 下载地址 下载完后重命名为dbghelp.dll然后放到和QQ.exe同目录(就是第1步安装的那个QQ) 此时打开第1步装的QQ, 上号, 在设置里面能看到LLOneBot就说明部署ok了 机器人发消息 上面部署完之后就可以测试下了, 发消息的测试脚本如下: 1234567891011121314151617181920212223242526272829303132333435363738import jsonimport requestsheaders = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36', 'Accept': 'application/json, text/plain, */*', 'Upgrade-Insecure-Requests': '1', 'Content-type': 'application/json', 'Sec-Ch-Ua': '&quot;Not_A Brand&quot;;v=&quot;8&quot;, &quot;Chromium&quot;;v=&quot;120&quot;, &quot;Google Chrome&quot;;v=&quot;120&quot;', 'Sec-Ch-Ua-Mobile': '?0', 'Sec-Ch-Ua-Platform': 'Windows',}class OneBotApi(object): def __init__(self): self.addr = 'http://127.0.0.1:3000' def send_text_to_group(self, group_id: int, text: str): req = { &quot;group_id&quot;: group_id, &quot;message_type&quot;: &quot;private&quot;, &quot;message&quot;: [ { &quot;type&quot;: &quot;text&quot;, &quot;data&quot;: { &quot;text&quot;: text } } ] } payload = json.dumps(req) resp = requests.post('%s/send_group_msg' % self.addr, data=payload, headers=headers) print(resp.text)cli = OneBotApi()your_group_id = 1145144444 # 这里填测试用的QQ群号cli.send_text_to_group(your_group_id, 'hello') 其他功能 直接看LLOneBot的官方文档即可, 非常全: LLOneBot 参考资料 LLOnebot教程","link":"/2024/10/24/llOnebot%E5%AE%9E%E7%8E%B0QQ%E6%9C%BA%E5%99%A8%E4%BA%BA/"},{"title":"markdown锚点跳转的坑","text":"背景 写markdown有这样的需求: 点击某个词跳转到markdown文章的某个位置(某个锚点), 但是写完发现有些点了跳不过去 原因就是跳转锚点的格式没写对, 格式见下面 锚点title需要注意的格式 必须全小写 空格用’-‘代替 ‘_’ ‘()’需要去掉 错误例子123[点我跳转1](/shuogg/article.html#如何取一个好的ID) [点我跳转2](/shuogg/article.html#game system搭建)[点我跳转3](/shuogg/article.html#game_system搭建) 正确例子123[点我跳转1](/shuogg/article.html#如何取一个好的id) [点我跳转2](/shuogg/article.html#game-system搭建)[点我跳转3](/shuogg/article.html#gamesystem搭建)","link":"/2019/11/13/markdown_note/"},{"title":"markdown数学公式","text":"一. Markdown公式基础关键字 $ 单行公式 $ $$ 多行公式 $$ \\\\ 公式内换行符 \\! 跟随 \\, 小空格 \\; 中空格 \\ 大空格(约1字符) \\quad 巨大空格(约2字符) \\qquad 超巨大空格(约4字符) _ 下标 ^上标 二. 基础符号 预览$$f(x) \\x \\pm y \\x \\times y \\x \\div y \\x \\cdot y \\x^y \\x_i \\x_{i+1} \\x^i_j \\\\dfrac{x}{y} \\\\sqrt{x} \\x \\gt y \\x \\lt y \\x \\ge y \\x \\le y \\\\cdots \\\\infty \\\\sin \\\\cos \\\\tan \\\\cot \\\\vec{x} \\\\sum_{x}^{y} \\\\prod_{x}^{y} \\\\lim_{x} \\$$ 对应的markdown123456789101112131415161718192021222324252627$$f(x) \\\\x \\pm y \\\\x \\times y \\\\x \\div y \\\\x \\cdot y \\\\x^y \\\\x_i \\\\x_{i+1} \\\\x^i_j \\\\\\dfrac{x}{y} \\\\\\sqrt{x} \\\\x \\gt y \\\\x \\lt y \\\\x \\ge y \\\\x \\le y \\\\\\cdots \\\\\\infty \\\\\\sin \\\\\\cos \\\\\\tan \\\\\\cot \\\\\\vec{x} \\\\\\sum_{x}^{y} \\\\\\prod_{x}^{y} \\\\\\lim_{x} \\\\$$ 三. 希腊字符 预览$$\\alpha \\beta \\gamma \\delta \\epsilon \\varepsilon \\eta \\theta \\kappa \\iota \\zeta \\lambda \\pi \\rho \\xi \\nu \\upsilon \\varphi \\chi \\psi \\omega \\Omega \\Gamma \\Delta \\mu \\Phi \\nabla $$ 对应的markdown1234567891011121314151617181920212223242526272829$$\\alpha \\\\beta \\\\gamma \\\\delta \\\\epsilon \\\\varepsilon \\\\eta \\\\theta \\\\kappa \\\\iota \\\\zeta \\\\lambda \\\\pi \\\\rho \\\\xi \\\\nu \\\\upsilon \\\\varphi \\\\chi \\\\psi \\\\omega \\\\Omega \\\\Gamma \\\\Delta \\\\mu \\\\Phi \\\\nabla \\$$ 四. 矩阵 预览$$ \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \\\\end{matrix}\\right]\\tag{这是注释}$$ 对应的markdown12345678910$$ \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\\\ \\end{matrix} \\right] \\tag{这是注释}$$ 五. 表达式 预览$$\\begin{cases}x + y + z \\2x - 2y + 4z \\-3x + 4y +5z\\end{cases}$$ 对应的markdown1234567$$\\begin{cases}x + y + z \\\\2x - 2y + 4z \\\\-3x + 4y +5z\\end{cases}$$ 经典示例 麦克斯威方程$$\\begin{array}{lll}\\nabla\\times E &amp;=&amp; -;\\frac{\\partial{B}}{\\partial{t}} \\nabla\\times H &amp;=&amp; \\frac{\\partial{D}}{\\partial{t}}+J \\nabla\\cdot D &amp;=&amp; \\rho \\nabla\\cdot B &amp;=&amp; 0 \\end{array} $$","link":"/2022/11/03/markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"},{"title":"mongodb官方备份工具, MongoDump源码分析","text":"简介 最近用到了该模块, 期间看了源码, 这里顺便做下笔记 本文前几小节讲MongoDump的运转流程, 后几小节讲代码方面的技术细节 源码版本: 100.6.1 源码仓库: https://github.com/mongodb/mongo-tools/blob/100.6.1/mongodump 0. 备份总览: MongoDump.Dump 备份逻辑就在MongoDump的Dump方法 (mongodump/mongodump.go/MongoDump.Dump) Dump主要分4步: 各种初始化工作 备份metaData, index, users, roles, version 等基础数据 备份collections 备份oplog 后面会经常提到Intent, 这是MongoDump自己的一个抽象概念, 可以简单理解为备份任务单元, 例如一个collection的备份对应一个Intent, oplog的备份对应一个Intent等等; 在阅读源码时你可以将Intent在脑海里替换成Task. 关于Intent详见本文后面章节 核心逻辑见以下源码及注释(为了方便阅读, 这里我删减了些不关键的逻辑): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258// Dump是MongoDump的一个方法type MongoDump struct { ToolOptions *options.ToolOptions InputOptions *InputOptions OutputOptions *OutputOptions SkipUsersAndRoles bool ProgressManager progress.Manager SessionProvider *db.SessionProvider // 就是mongoClient manager *intents.Manager // 备份单元管理, 核心组件 query bson.D oplogCollection string oplogStart primitive.Timestamp oplogEnd primitive.Timestamp isMongos bool storageEngine storageEngineType authVersion int archive *archive.Writer // InputOptions.Output非&quot;-&quot;时往这里写入 shutdownIntentsNotifier *notifier OutputWriter io.Writer // InputOptions.Output为&quot;-&quot;时往这里写入 Logger *log.ToolLogger}// OutputOptions出现频繁所以贴一下type OutputOptions struct { Out string `long:&quot;out&quot; value-name:&quot;&lt;directory-path&gt;&quot; short:&quot;o&quot; description:&quot;output directory, or '-' for stdout (default: 'dump')&quot;` Gzip bool `long:&quot;gzip&quot; description:&quot;compress archive or collection output with Gzip&quot;` Oplog bool `long:&quot;oplog&quot; description:&quot;use oplog for taking a point-in-time snapshot&quot;` Archive string `long:&quot;archive&quot; value-name:&quot;&lt;file-path&gt;&quot; optional:&quot;true&quot; optional-value:&quot;-&quot; description:&quot;dump as an archive to the specified path. If flag is specified without a value, archive is written to stdout&quot;` DumpDBUsersAndRoles bool `long:&quot;dumpDbUsersAndRoles&quot; description:&quot;dump user and role definitions for the specified database&quot;` ExcludedCollections []string `long:&quot;excludeCollection&quot; value-name:&quot;&lt;collection-name&gt;&quot; description:&quot;collection to exclude from the dump (may be specified multiple times to exclude additional collections)&quot;` ExcludedCollectionPrefixes []string `long:&quot;excludeCollectionsWithPrefix&quot; value-name:&quot;&lt;collection-prefix&gt;&quot; description:&quot;exclude all collections from the dump that have the given prefix (may be specified multiple times to exclude additional prefixes)&quot;` NumParallelCollections int `long:&quot;numParallelCollections&quot; short:&quot;j&quot; description:&quot;number of collections to dump in parallel&quot; default:&quot;4&quot; default-mask:&quot;-&quot;` ViewsAsCollections bool `long:&quot;viewsAsCollections&quot; description:&quot;dump views as normal collections with their produced data, omitting standard collections&quot;`}func (dump *MongoDump) Dump() (err error) { defer dump.SessionProvider.Close() /* 1. 阶段1: 各种初始化工作 */ // 检查下dump.ToolOptions.Namespace.DB和dump.ToolOptions.Namespace.Collection是否存在 exists, err := dump.verifyCollectionExists() if err != nil { return fmt.Errorf(&quot;error verifying collection info: %v&quot;, err) } if !exists { return nil } // 初始化shutdownIntentsNotifier; 本质就是一个shutdown chan dump.shutdownIntentsNotifier = newNotifier() // 初始化dump.query; 是针对指定了过滤条件的情况, 一般不会用到 if dump.InputOptions.HasQuery() { content, err := dump.InputOptions.GetQuery() if err != nil { return err } var query bson.D err = bson.UnmarshalExtJSON(content, false, &amp;query) if err != nil { return fmt.Errorf(&quot;error parsing query as Extended JSON: %v&quot;, err) } dump.query = query } // 连源MongoDB获取authSchemaVersion, 版本小于3则不支持备份用户和角色, 直接返回错误 if !dump.SkipUsersAndRoles &amp;&amp; dump.OutputOptions.DumpDBUsersAndRoles { // dump.SessionProvider就是mongoClient dump.authVersion, err = auth.GetAuthVersion(dump.SessionProvider) if err == nil { err = auth.VerifySystemAuthVersion(dump.SessionProvider) } if err != nil { return fmt.Errorf(&quot;error getting auth schema version for dumpDbUsersAndRoles: %v&quot;, err) } if dump.authVersion &lt; 3 { return fmt.Errorf(&quot;backing up users and roles is only supported for &quot;+ &quot;deployments with auth schema versions &gt;= 3, found: %v&quot;, dump.authVersion) } } // 初始化dump.archive, dump.archive是个高度封装的Writer if dump.OutputOptions.Archive != &quot;&quot; { var archiveOut io.WriteCloser // 根据dump.OutputOptions.Archive获取对应输出文件的ioWriter, 当值为&quot;-&quot;时输出将到OutputWriter而不是文件 archiveOut, err = dump.getArchiveOut() if err != nil { return err } dump.archive = &amp;archive.Writer{ Out: archiveOut, Mux: archive.NewMultiplexer(archiveOut, dump.shutdownIntentsNotifier), } go dump.archive.Mux.Run() // 备份结束后的一些释放工作 defer func() { // The Mux runs until its Control is closed close(dump.archive.Mux.Control) muxErr := &lt;-dump.archive.Mux.Completed archiveOut.Close() if muxErr != nil { if err != nil { err = fmt.Errorf(&quot;archive writer: %v / %v&quot;, err, muxErr) } else { err = fmt.Errorf(&quot;archive writer: %v&quot;, muxErr) } dump.Logger.Logvf(log.DebugLow, &quot;%v&quot;, err) } else { dump.Logger.Logvf(log.DebugLow, &quot;mux completed successfully&quot;) } }() } // 源Mongodb连通性检测 session, err := dump.SessionProvider.GetSession() if err != nil { return fmt.Errorf(&quot;error getting a client session: %v&quot;, err) } err = session.Ping(context.Background(), nil) if err != nil { return fmt.Errorf(&quot;error connecting to host: %v&quot;, err) } // 创建备份Intents switch { case dump.ToolOptions.DB == &quot;&quot; &amp;&amp; dump.ToolOptions.Collection == &quot;&quot;: err = dump.CreateAllIntents() case dump.ToolOptions.DB != &quot;&quot; &amp;&amp; dump.ToolOptions.Collection == &quot;&quot;: err = dump.CreateIntentsForDatabase(dump.ToolOptions.DB) case dump.ToolOptions.DB != &quot;&quot; &amp;&amp; dump.ToolOptions.Collection != &quot;&quot;: err = dump.CreateCollectionIntent(dump.ToolOptions.DB, dump.ToolOptions.Collection) } if err != nil { return fmt.Errorf(&quot;error creating intents to dump: %v&quot;, err) } // 如果需要备份Oplog, 则创建备份Oplog的Intents if dump.OutputOptions.Oplog { err = dump.CreateOplogIntents() if err != nil { return err } } // 如果需要备份Users和Roles, 则创建备份Users和Role的Intents if !dump.SkipUsersAndRoles &amp;&amp; dump.OutputOptions.DumpDBUsersAndRoles &amp;&amp; dump.ToolOptions.DB != &quot;admin&quot; { err = dump.CreateUsersRolesVersionIntentsForDB(dump.ToolOptions.DB) if err != nil { return err } } /* 2. 阶段2: 备份metaData, index, users, roles, version 等基础数据 */ err = dump.DumpMetadata() // intent.MetadataFile.Write(json.Marshal(metadata)) if err != nil { return fmt.Errorf(&quot;error dumping metadata: %v&quot;, err) } if dump.OutputOptions.Archive != &quot;&quot; { serverVersion, err := dump.SessionProvider.ServerVersion() if err != nil { dump.Logger.Logvf(log.Always, &quot;warning, couldn't get version information from server: %v&quot;, err) serverVersion = &quot;unknown&quot; } dump.archive.Prelude, err = archive.NewPrelude(dump.manager, dump.OutputOptions.NumParallelCollections, serverVersion, dump.ToolOptions.VersionStr) if err != nil { return fmt.Errorf(&quot;creating archive prelude: %v&quot;, err) } err = dump.archive.Prelude.Write(dump.archive.Out) if err != nil { return fmt.Errorf(&quot;error writing metadata into archive: %v&quot;, err) } } // 备份users, roles if !dump.SkipUsersAndRoles { if dump.ToolOptions.DB == &quot;admin&quot; || dump.ToolOptions.DB == &quot;&quot; { err = dump.DumpUsersAndRoles() if err != nil { return fmt.Errorf(&quot;error dumping users and roles: %v&quot;, err) } } if dump.OutputOptions.DumpDBUsersAndRoles { dump.Logger.Logvf(log.Always, &quot;dumping users and roles for %v&quot;, dump.ToolOptions.DB) if dump.ToolOptions.DB == &quot;admin&quot; { dump.Logger.Logvf(log.Always, &quot;skipping users/roles dump, already dumped admin database&quot;) } else { err = dump.DumpUsersAndRolesForDB(dump.ToolOptions.DB) if err != nil { return fmt.Errorf(&quot;error dumping users and roles: %v&quot;, err) } } } } // 设置dump.oplogStart 和 dump.oplogCollection if dump.OutputOptions.Oplog { // set dump.oplogCollection, &quot;oplog.rs&quot;或&quot;oplog.$main&quot; err := dump.determineOplogCollectionName() if err != nil { return fmt.Errorf(&quot;error finding oplog: %v&quot;, err) } dump.Logger.Logvf(log.Info, &quot;getting most recent oplog timestamp&quot;) dump.oplogStart, err = dump.getOplogCopyStartTime() if err != nil { return fmt.Errorf(&quot;error getting oplog start: %v&quot;, err) } } /* 3. 阶段3: 备份collections */ if err := dump.DumpIntents(); err != nil { return err } /* 4. 阶段4: 备份oplog */ if dump.OutputOptions.Oplog { dump.oplogEnd, err = dump.getCurrentOplogTime() if err != nil { return fmt.Errorf(&quot;error getting oplog end: %v&quot;, err) } // 确认oplog文件是否发生了翻转(Roll over), oplog本身是个环形队列 exists, err := dump.checkOplogTimestampExists(dump.oplogStart) if !exists { return fmt.Errorf( &quot;oplog overflow: mongodump was unable to capture all new oplog entries during execution&quot;) } if err != nil { return fmt.Errorf(&quot;unable to check oplog for overflow: %v&quot;, err) } dump.Logger.Logvf(log.Always, &quot;writing captured oplog to %v&quot;, dump.manager.Oplog().Location) // 备份oplog err = dump.DumpOplogBetweenTimestamps(dump.oplogStart, dump.oplogEnd) if err != nil { return fmt.Errorf(&quot;error dumping oplog: %v&quot;, err) } // 再次确认oplog文件是否发生了翻转 dump.Logger.Logvf(log.DebugLow, &quot;checking again if oplog entry %v still exists&quot;, dump.oplogStart) exists, err = dump.checkOplogTimestampExists(dump.oplogStart) if !exists { return fmt.Errorf( &quot;oplog overflow: mongodump was unable to capture all new oplog entries during execution&quot;) } if err != nil { return fmt.Errorf(&quot;unable to check oplog for overflow: %v&quot;, err) } } // 备份完成 dump.Logger.Logvf(log.DebugLow, &quot;finishing dump&quot;) return err} 1. 备份metadata 备份metadata的逻辑比较简单, 就是将Metadata jsonMarshal后写入intent.MetadataFile (io) 源码逻辑如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192type Metadata struct { Options bson.M `bson:&quot;options,omitempty&quot;` Indexes []bson.D `bson:&quot;indexes&quot;` UUID string `bson:&quot;uuid,omitempty&quot;` CollectionName string `bson:&quot;collectionName&quot;` Type string `bson:&quot;type,omitempty&quot;`}func (dump *MongoDump) dumpMetadata(intent *intents.Intent, buffer resettableOutputBuffer) (err error) { // 1. 填充Metadata, 值取自入参`intent` meta := Metadata{ Indexes: []bson.D{}, } meta.Options = intent.Options meta.UUID = intent.UUID meta.CollectionName = intent.C if intent.Type != &quot;&quot; { meta.Type = intent.Type } session, err := dump.SessionProvider.GetSession() if err != nil { return err } // 获取源端的index并set进meta.Indexes if dump.OutputOptions.ViewsAsCollections || intent.IsView() { dump.Logger.Logvf(log.DebugLow, &quot;not dumping indexes metadata for '%v' because it is a view&quot;, intent.Namespace()) } else { // get the indexes indexesIter, err := db.GetIndexes(session.Database(intent.DB).Collection(intent.C)) if err != nil { return err } if indexesIter == nil { dump.Logger.Logvf(log.Always, &quot;the collection %v appears to have been dropped after the dump started&quot;, intent.Namespace()) return nil } defer indexesIter.Close(context.Background()) ctx := context.Background() for indexesIter.Next(ctx) { indexOpts := &amp;bson.D{} err := indexesIter.Decode(indexOpts) if err != nil { return fmt.Errorf(&quot;error converting index: %v&quot;, err) } meta.Indexes = append(meta.Indexes, *indexOpts) } if err := indexesIter.Err(); err != nil { return fmt.Errorf(&quot;error getting indexes for collection `%v`: %v&quot;, intent.Namespace(), err) } } // 2. 把Metadata写入intent.MetadataFile /* 后面就是将meta jsonMarshal后写入intent.MetadataFile 而已*/ jsonBytes, err := bson.MarshalExtJSON(meta, true, false) if err != nil { return fmt.Errorf(&quot;error marshalling metadata json for collection `%v`: %v&quot;, intent.Namespace(), err) } err = intent.MetadataFile.Open() if err != nil { return err } defer func() { closeErr := intent.MetadataFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() var f io.Writer f = intent.MetadataFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } _, err = f.Write(jsonBytes) if err != nil { err = fmt.Errorf(&quot;error writing metadata for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 2. 备份collections 源码如下, 就是从intent的manager中不断取intent分配给n条线程进行备份 一个intent对应一个collection的备份任务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 并发备份collections, NumParallelCollections条线程func (dump *MongoDump) DumpIntents() error { resultChan := make(chan error) // 线程数 = jobs = dump.OutputOptions.NumParallelCollections jobs := dump.OutputOptions.NumParallelCollections if numIntents := len(dump.manager.Intents()); jobs &gt; numIntents { jobs = numIntents } // 设置intents的Pop顺序策略 if jobs &gt; 1 { dump.manager.Finalize(intents.LongestTaskFirst) } else { dump.manager.Finalize(intents.Legacy) } // 多线程从dump.manager中Pop出Intent, 进行dump for i := 0; i &lt; jobs; i++ { go func(id int) { buffer := dump.getResettableOutputBuffer() dump.Logger.Logvf(log.DebugHigh, &quot;starting dump routine with id=%v&quot;, id) for { intent := dump.manager.Pop() if intent == nil { dump.Logger.Logvf(log.DebugHigh, &quot;ending dump routine with id=%v, no more work to do&quot;, id) resultChan &lt;- nil return } if intent.BSONFile != nil { err := dump.DumpIntent(intent, buffer) if err != nil { resultChan &lt;- err return } } dump.manager.Finish(intent) } }(i) } // 等待所有intents dump完 for i := 0; i &lt; jobs; i++ { if err := &lt;-resultChan; err != nil { return err } } return nil} 3. 备份oplog 备份oplog的逻辑比较简单, 将查询oplog的结果写入oplog对应的intent.BSONFile 如果对oplog不熟悉可以看下官方文档: https://www.mongodb.com/zh-cn/docs/manual/core/replica-set-oplog/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// 查询start与end之间的oplog, 写入 dump.manager.Oplog().BSONFilefunc (dump *MongoDump) DumpOplogBetweenTimestamps(start, end primitive.Timestamp) error { session, err := dump.SessionProvider.GetSession() if err != nil { return err } queryObj := bson.M{&quot;$and&quot;: []bson.M{ {&quot;ts&quot;: bson.M{&quot;$gte&quot;: start}}, {&quot;ts&quot;: bson.M{&quot;$lte&quot;: end}}, }} oplogQuery := &amp;db.DeferredQuery{ // &quot;local.oplog.rs&quot;(replset)或&quot;local.oplog.$main&quot;(m/s) Coll: session.Database(&quot;local&quot;).Collection(dump.oplogCollection), Filter: queryObj, LogReplay: true, } // 执行上面的`oplogQuery`, 将结果写入`dump.manager.Oplog().BSONFile` (dump.manager.Oplog()是oplog的intent) oplogCount, err := dump.dumpValidatedQueryToIntent(oplogQuery, dump.manager.Oplog(), dump.getResettableOutputBuffer(), oplogDocumentValidator) if err == nil { dump.Logger.Logvf(log.Always, &quot;\\tdumped %v oplog %v&quot;, oplogCount, util.Pluralize(int(oplogCount), &quot;entry&quot;, &quot;entries&quot;)) } return err}// 把`query`的查询结果写入`intent.BSONFile` (这是一个公共函数, 上边引用到了就顺便贴下, 不关键)func (dump *MongoDump) dumpValidatedQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer, validator documentValidator) (dumpCount int64, err error) { err = intent.BSONFile.Open() if err != nil { return 0, err } defer func() { closeErr := intent.BSONFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() // don't dump any data for views being dumped as views if intent.IsView() &amp;&amp; !dump.OutputOptions.ViewsAsCollections { return 0, nil } total, err := dump.getCount(query, intent) if err != nil { return 0, err } dumpProgressor := progress.NewCounter(total) if dump.ProgressManager != nil { dump.ProgressManager.Attach(intent.Namespace(), dumpProgressor) defer dump.ProgressManager.Detach(intent.Namespace()) } var f io.Writer f = intent.BSONFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } cursor, err := query.Iter() if err != nil { return } // 将cursor查询结果写入f err = dump.dumpValidatedIterToWriter(cursor, f, dumpProgressor, validator) dumpCount, _ = dumpProgressor.Progress() if err != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 备份单元: Intent 备份任务单元, 可以简单理解1个collection的备份任务就叫intent, 拆分是为了多线程执行 Intent相关的结构和关键方法: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177type Intent struct { // Destination namespace info DB string C string // collection // File locations as absolute paths BSONFile file BSONSize int64 MetadataFile file // Indicates where the intent will be read from or written to Location string MetadataLocation string // Collection options Options bson.M // UUID (for MongoDB 3.6+) as a big-endian hex string UUID string // File/collection size, for some prioritizer implementations. // Units don't matter as long as they are consistent for a given use case. Size int64 // Either view or timeseries. Empty string &quot;&quot; is a regular collection. Type string}// 查询collection(intent.C) 的数据, 写入intent.BSONFile, 仅此而已func (dump *MongoDump) DumpIntent(intent *intents.Intent, buffer resettableOutputBuffer) error { session, err := dump.SessionProvider.GetSession() if err != nil { return err } intendedDB := session.Database(intent.DB) var coll *mongo.Collection if intent.IsTimeseries() { coll = intendedDB.Collection(&quot;system.buckets.&quot; + intent.C) } else { coll = intendedDB.Collection(intent.C) } isView := true collInfo, err := db.GetCollectionInfo(coll) if err != nil { return err } else if collInfo != nil { isView = collInfo.IsView() } // 推断并设置dump.storageEngine if dump.storageEngine == storageEngineUnknown &amp;&amp; !isView { if err != nil { return err } dump.storageEngine = storageEngineModern isMMAPV1, err := db.IsMMAPV1(intendedDB, intent.C) if err != nil { dump.Logger.Logvf(log.Always, &quot;failed to determine storage engine, an mmapv1 storage engine could result in&quot;+ &quot; inconsistent dump results, error was: %v&quot;, err) } else if isMMAPV1 { dump.storageEngine = storageEngineMMAPV1 } } findQuery := &amp;db.DeferredQuery{Coll: coll} switch { case len(dump.query) &gt; 0: if intent.IsTimeseries() { metaKey, ok := intent.Options[&quot;timeseries&quot;].(bson.M)[&quot;metaField&quot;].(string) if !ok { return fmt.Errorf(&quot;could not determine the metaField for %s&quot;, intent.Namespace()) } for i, predicate := range dump.query { splitPredicateKey := strings.SplitN(predicate.Key, &quot;.&quot;, 2) if splitPredicateKey[0] != metaKey { return fmt.Errorf(&quot;cannot process query %v for timeseries collection %s. &quot;+ &quot;mongodump only processes queries on metadata fields for timeseries collections.&quot;, dump.query, intent.Namespace()) } if len(splitPredicateKey) &gt; 1 { dump.query[i].Key = &quot;meta.&quot; + splitPredicateKey[1] } else { dump.query[i].Key = &quot;meta&quot; } } } findQuery.Filter = dump.query case dump.storageEngine == storageEngineMMAPV1 &amp;&amp; !dump.InputOptions.TableScan &amp;&amp; !isView &amp;&amp; !intent.IsSpecialCollection() &amp;&amp; !intent.IsOplog(): autoIndexId, found := intent.Options[&quot;autoIndexId&quot;] if !found || autoIndexId == true { findQuery.Hint = bson.D{{&quot;_id&quot;, 1}} } } var dumpCount int64 if dump.OutputOptions.Out == &quot;-&quot; { // 初始化阶段有 &quot;intent.BSONFile = &amp;stdoutFile{Writer: dump.OutputWriter}&quot;, 可以搜下源码 // 所以这里虽然也是写到intent.BSONFile, 但实际写到dump.OutputWriter了 dump.Logger.Logvf(log.Always, &quot;writing %v to stdout&quot;, intent.DataNamespace()) dumpCount, err = dump.dumpQueryToIntent(findQuery, intent, buffer) if err == nil { // on success, print the document count dump.Logger.Logvf(log.Always, &quot;dumped %v %v&quot;, dumpCount, docPlural(dumpCount)) } return err } // 将findQuery查到的写入intent.BSONFile if dumpCount, err = dump.dumpQueryToIntent(findQuery, intent, buffer); err != nil { return err } return nil}// 将query查到的写入intent.BSONFilefunc (dump *MongoDump) dumpQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer) (dumpCount int64, err error) { return dump.dumpValidatedQueryToIntent(query, intent, buffer, nil)}func (dump *MongoDump) dumpValidatedQueryToIntent( query *db.DeferredQuery, intent *intents.Intent, buffer resettableOutputBuffer, validator documentValidator) (dumpCount int64, err error) { err = intent.BSONFile.Open() if err != nil { return 0, err } defer func() { closeErr := intent.BSONFile.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() // don't dump any data for views being dumped as views if intent.IsView() &amp;&amp; !dump.OutputOptions.ViewsAsCollections { return 0, nil } total, err := dump.getCount(query, intent) if err != nil { return 0, err } dumpProgressor := progress.NewCounter(total) if dump.ProgressManager != nil { dump.ProgressManager.Attach(intent.Namespace(), dumpProgressor) defer dump.ProgressManager.Detach(intent.Namespace()) } var f io.Writer f = intent.BSONFile if buffer != nil { buffer.Reset(f) f = buffer defer func() { closeErr := buffer.Close() if err == nil &amp;&amp; closeErr != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), closeErr) } }() } cursor, err := query.Iter() if err != nil { return } // 将cursor查到的东西写入f err = dump.dumpValidatedIterToWriter(cursor, f, dumpProgressor, validator) dumpCount, _ = dumpProgressor.Progress() if err != nil { err = fmt.Errorf(&quot;error writing data for collection `%v` to disk: %v&quot;, intent.Namespace(), err) } return} 多路读写模块: archive.Writer/Reader 这是MongoDump唯一比较复杂的模块, 因为只讲备份, 所以只讲archive.Writer, archive.Reader是恢复时用到, 原理一样 多路读写核心Multiplexer, 里面有个核心组件MuxIn, 这东西其实就是上面提到的BSONFile的实现, 每次BSONFile.Open就相当于New一个NuxIn然后塞给Multiplexer管理, MuxIn就是多路读写里面的路 多路读写怎么实现? 其实本质是多路In, 一路Out, 上面提到的MuxIn是实现多路In, 而一路Out的关键逻辑在Multiplexer.formatBody, 这里可以看看下面的源码, 其实就是利用写入header和namespace来做数据隔离, 配合Multiplexer的select channel这样就实现了多路读写. 这个思想是值得学习的 概念那么多是不是看了头晕? 我们将所有概念都关联起来捋一下: 在1次备份中, 只有1个archive.Writer, 也意味着只有1个Multiplexer, 1个Multiplexer管理了n个MuxIn, n又等于Intent的个数, Intent有多少个? Intent的个数为len(collections) + 1 + 1, 这里的两个1分别是metadata和oplog Multiplexer源码的几个核心方法: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150type Writer struct { Out io.WriteCloser Prelude *Prelude Mux *Multiplexer}type Multiplexer struct { Out io.WriteCloser Control chan *MuxIn Completed chan error shutdownInputs notifier // ins and selectCases are correlating slices ins []*MuxIn selectCases []reflect.SelectCase currentNamespace string}type notifier interface { Notify()}func NewMultiplexer(out io.WriteCloser, shutdownInputs notifier) *Multiplexer { mux := &amp;Multiplexer{ Out: out, Control: make(chan *MuxIn), Completed: make(chan error), shutdownInputs: shutdownInputs, ins: []*MuxIn{ nil, // There is no MuxIn for the Control case }, } // 反射实现channel select, 非常少见的玩法! mux.selectCases = []reflect.SelectCase{ { Dir: reflect.SelectRecv, Chan: reflect.ValueOf(mux.Control), Send: reflect.Value{}, }, } return mux}// 核心事件循环: 处理MuxIn的增删事件和来自MuxIn的写数据事件func (mux *Multiplexer) Run() { var err, completionErr error for { // select的反射玩法, 学到了 index, value, notEOF := reflect.Select(mux.selectCases) EOF := !notEOF if index == 0 { // index 0 为 mux.Control, 用于接收新的MuxIn if EOF { log.Logvf(log.DebugLow, &quot;Mux finish&quot;) mux.Out.Close() if completionErr != nil { mux.Completed &lt;- completionErr } else if len(mux.selectCases) != 1 { mux.Completed &lt;- fmt.Errorf(&quot;Mux ending but selectCases still open %v&quot;, len(mux.selectCases)) } else { mux.Completed &lt;- nil } return } muxIn, ok := value.Interface().(*MuxIn) if !ok { mux.Completed &lt;- fmt.Errorf(&quot;non MuxIn received on Control chan&quot;) // one for the MuxIn.Open return } log.Logvf(log.DebugLow, &quot;Mux open namespace %v&quot;, muxIn.Intent.DataNamespace()) mux.selectCases = append(mux.selectCases, reflect.SelectCase{ Dir: reflect.SelectRecv, Chan: reflect.ValueOf(muxIn.writeChan), Send: reflect.Value{}, }) mux.ins = append(mux.ins, muxIn) } else { // index &gt; 0 为 MuxIn.writeChan, 用于接收MuxIn.Write的data if EOF { mux.ins[index].writeCloseFinishedChan &lt;- struct{}{} err = mux.formatEOF(index, mux.ins[index]) if err != nil { mux.shutdownInputs.Notify() mux.Out = &amp;nopCloseNopWriter{} completionErr = err } log.Logvf(log.DebugLow, &quot;Mux close namespace %v&quot;, mux.ins[index].Intent.DataNamespace()) mux.currentNamespace = &quot;&quot; mux.selectCases = append(mux.selectCases[:index], mux.selectCases[index+1:]...) mux.ins = append(mux.ins[:index], mux.ins[index+1:]...) } else { bsonBytes, ok := value.Interface().([]byte) if !ok { mux.Completed &lt;- fmt.Errorf(&quot;multiplexer received a value that wasn't a []byte&quot;) return } // format bsonBytes, 然后 mux.Out.Write(bsonBytes) err = mux.formatBody(mux.ins[index], bsonBytes) if err != nil { mux.shutdownInputs.Notify() mux.Out = &amp;nopCloseNopWriter{} completionErr = err } } } }}// 核心逻辑, 这个里的header用于隔离不同namespace的数据, 已达到多路的效果, 恢复的时候也是根据header来恢复的// mux.Out.Write header和bsonBytes, 这里的Out其实就是dump.archive.Outfunc (mux *Multiplexer) formatBody(in *MuxIn, bsonBytes []byte) error { var err error var length int defer func() { in.writeLenChan &lt;- length }() if in.Intent.DataNamespace() != mux.currentNamespace { // Handle the change of which DB/Collection we're writing docs for // If mux.currentNamespace then we need to terminate the current block if mux.currentNamespace != &quot;&quot; { l, err := mux.Out.Write(terminatorBytes) if err != nil { return err } if l != len(terminatorBytes) { return io.ErrShortWrite } } header, err := bson.Marshal(NamespaceHeader{ Database: in.Intent.DB, Collection: in.Intent.DataCollection(), }) if err != nil { return err } l, err := mux.Out.Write(header) if err != nil { return err } if l != len(header) { return io.ErrShortWrite } } mux.currentNamespace = in.Intent.DataNamespace() length, err = mux.Out.Write(bsonBytes) if err != nil { return err } return nil} MuxIn源码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273type MuxIn struct { writeChan chan []byte writeLenChan chan int writeCloseFinishedChan chan struct{} buf []byte hash hash.Hash64 Intent *intents.Intent Mux *Multiplexer}func (muxIn *MuxIn) Read([]byte) (int, error) { return 0, nil}func (muxIn *MuxIn) Pos() int64 { return 0}// 关闭muxIn内部的所有chan, 最后multiplexer会收到关闭信号并返回formatEOF, 同时multiplexer也会发信号muxIn.writeCloseFinishedChanfunc (muxIn *MuxIn) Close() error { // the mux side of this gets closed in the mux when it gets an eof on the read log.Logvf(log.DebugHigh, &quot;MuxIn close %v&quot;, muxIn.Intent.DataNamespace()) if bufferWrites { muxIn.writeChan &lt;- muxIn.buf length := &lt;-muxIn.writeLenChan if length != len(muxIn.buf) { return io.ErrShortWrite } muxIn.buf = nil } close(muxIn.writeChan) close(muxIn.writeLenChan) &lt;-muxIn.writeCloseFinishedChan return nil}// 初始化muxIn, 然后把自己发给 muxIn.Muxfunc (muxIn *MuxIn) Open() error { log.Logvf(log.DebugHigh, &quot;MuxIn open %v&quot;, muxIn.Intent.DataNamespace()) muxIn.writeChan = make(chan []byte) muxIn.writeLenChan = make(chan int) muxIn.writeCloseFinishedChan = make(chan struct{}) muxIn.buf = make([]byte, 0, bufferSize) muxIn.hash = crc64.New(crc64.MakeTable(crc64.ECMA)) if bufferWrites { muxIn.buf = make([]byte, 0, db.MaxBSONSize) } muxIn.Mux.Control &lt;- muxIn return nil}// buf写入muxIn.buf, 满了就把muxIn.buf写入muxIn.writeChan, 然后清空muxIn.buffunc (muxIn *MuxIn) Write(buf []byte) (int, error) { if bufferWrites { // 固定true if len(muxIn.buf)+len(buf) &gt; cap(muxIn.buf) { muxIn.writeChan &lt;- muxIn.buf length := &lt;-muxIn.writeLenChan if length != len(muxIn.buf) { return 0, io.ErrShortWrite } muxIn.buf = muxIn.buf[:0] } muxIn.buf = append(muxIn.buf, buf...) } else { muxIn.writeChan &lt;- buf length := &lt;-muxIn.writeLenChan if length != len(buf) { return 0, io.ErrShortWrite } } muxIn.hash.Write(buf) return len(buf), nil}","link":"/2023/04/15/mongoDump%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Mongodb数据迁移:MongoShake源码阅读","text":"简述 近期用到了MongoShake做数据迁移, 顺便看看源码, 本篇为阅读源码的笔记 本文只讲数据迁移这块相关的 原理及架构 架构 mongoshake-arch 源码信息 源码版本: v2.8.1 源码仓库: https://github.com/alibaba/MongoShake 本文贴的源码片段为了只关注核心逻辑, 无关紧要的代码段会干掉或注释代替 一. 进程入口: collector的func main为进程的入口, 核心源码如下: 简单来讲做了3件事情 初始化和校验配置参数 加进程锁 使用了第三方工具github.com/nightlyone/lockfile实现的 简单讲就是在conf.Options.LogDirectory目录下建一个{conf.Options.Id}.pid文件来实现进程锁; 所以你可以扫描该目录下有多少个pid文件来确认当前有多少MongoShake进程在运行 开始跑数据迁移(startup) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798// cmd/collector/collector.go// 初始化ReplicationCoordinator然后Runfunc main() { var err error defer handleExit() defer LOG.Close() defer utils.Goodbye() // argument options configuration := flag.String(&quot;conf&quot;, &quot;a.conf&quot;, &quot;configure file absolute path&quot;) verbose := flag.Int(&quot;verbose&quot;, 0, &quot;where log goes to: 0 - file，1 - file+stdout，2 - stdout&quot;) version := flag.Bool(&quot;version&quot;, false, &quot;show version&quot;) flag.Parse() var file *os.File if file, err = os.Open(*configuration); err != nil { crash(fmt.Sprintf(&quot;Configure file open failed. %v&quot;, err), -1) } defer file.Close() // read fcv and do comparison if _, err := conf.CheckFcv(*configuration, utils.FcvConfiguration.FeatureCompatibleVersion); err != nil { crash(err.Error(), -5) } configure := nimo.NewConfigLoader(file) configure.SetDateFormat(utils.GolangSecurityTime) if err := configure.Load(&amp;conf.Options); err != nil { crash(fmt.Sprintf(&quot;Configure file %s parse failed. %v&quot;, *configuration, err), -2) } // verify collector options and revise if err = SanitizeOptions(); err != nil { crash(fmt.Sprintf(&quot;Conf.Options check failed: %s&quot;, err.Error()), -4) } conf.Options.Version = utils.BRANCH utils.Mkdirs(conf.Options.LogDirectory) // get exclusive process lock and write pid if utils.WritePidById(conf.Options.LogDirectory, conf.Options.Id) { startup() }}func startup() { // leader election at the beginning selectLeader() // ReplicationCoordinator即迁移任务, 包含全量和增量 coordinator := &amp;coordinator.ReplicationCoordinator{ MongoD: make([]*utils.MongoSource, len(conf.Options.MongoUrls)), } // init for i, src := range conf.Options.MongoUrls { coordinator.MongoD[i] = new(utils.MongoSource) coordinator.MongoD[i].URL = src if len(conf.Options.IncrSyncOplogGIDS) != 0 { coordinator.MongoD[i].Gids = conf.Options.IncrSyncOplogGIDS } } if conf.Options.MongoSUrl != &quot;&quot; { coordinator.MongoS = &amp;utils.MongoSource{ URL: conf.Options.MongoSUrl, ReplicaName: &quot;mongos&quot;, } coordinator.RealSourceFullSync = []*utils.MongoSource{coordinator.MongoS} coordinator.RealSourceIncrSync = []*utils.MongoSource{coordinator.MongoS} if conf.Options.IncrSyncMongoFetchMethod == utils.VarIncrSyncMongoFetchMethodOplog { coordinator.RealSourceIncrSync = coordinator.MongoD } } else { coordinator.RealSourceFullSync = coordinator.MongoD coordinator.RealSourceIncrSync = coordinator.MongoD } if conf.Options.MongoCsUrl != &quot;&quot; { coordinator.MongoCS = &amp;utils.MongoSource{ URL: conf.Options.MongoCsUrl, } } // start mongodb replication if err := coordinator.Run(); err != nil { // initial or connection established failed LOG.Critical(fmt.Sprintf(&quot;run replication failed: %v&quot;, err)) crash(err.Error(), -6) } // if the sync mode is &quot;document&quot;, mongoshake should exit here. if conf.Options.SyncMode == utils.VarSyncModeFull { return } // do not exit select {}} 上面的startup里面的coordinator.Run()会根据syncMode走全量,增量或全量+增量12345678910111213141516171819202122232425switch syncMode {case utils.VarSyncModeAll: // 全量+增量 if conf.Options.FullSyncReaderOplogStoreDisk { LOG.Info(&quot;run parallel document oplog&quot;) if err := coordinator.parallelDocumentOplog(fullBeginTs); err != nil { return err } } else { LOG.Info(&quot;run serialize document oplog&quot;) if err := coordinator.serializeDocumentOplog(fullBeginTs); err != nil { return err } }case utils.VarSyncModeFull: // 全量 if err := coordinator.startDocumentReplication(); err != nil { return err }case utils.VarSyncModeIncr: // 增量 if err := coordinator.startOplogReplication(int64(0), int64(0), startTsMap); err != nil { return err }default: LOG.Critical(&quot;unknown sync mode %v&quot;, conf.Options.SyncMode) return errors.New(&quot;unknown sync mode &quot; + conf.Options.SyncMode)} 二. 全量同步 根据上面可知道, 全量同步的入口在ReplicationCoordinator.startDocumentReplication, 主要分为3步: 获取同步源的所有表, 删除同步目标对应的表 上面说的表实际在源码中是ns(namespace), 格式为f’{database}.{collection}’, 后面会经常提到 同步所有索引数据 (Option为后台索引时是放在这一步, 如果是前台索引则是同步db后才执行同步) 查询源库getIndexes, 然后在目标库createIndex 同步db(collection)数据 多线程同步, 线程数取决源数据地址和架构 mongos只算一个地址即一条线程, 多线程是针对mongod的情况 dbSyncer是同步的核心, 一条线程对应一个dbSyncer, 负责单个db实例的全量同步工作, 所以通常情况下只有1个dbSyncer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177// collector/coordinator/full.gofunc (coordinator *ReplicationCoordinator) startDocumentReplication() error { fromIsSharding := coordinator.SourceIsSharding() var shardingChunkMap sharding.ShardingChunkMap var err error // init orphan sharding chunk map if source is mongod(get data directly from mongod) if fromIsSharding &amp;&amp; coordinator.MongoS == nil { LOG.Info(&quot;source is mongod, need to fetching chunk map&quot;) shardingChunkMap, err = fetchChunkMap(fromIsSharding) if err != nil { LOG.Critical(&quot;fetch chunk map failed[%v]&quot;, err) return err } } else { LOG.Info(&quot;source is replica or mongos, no need to fetching chunk map&quot;) } filterList := filter.NewDocFilterList() // get all namespace need to sync nsSet, _, err := utils.GetAllNamespace(coordinator.RealSourceFullSync, filterList.IterateFilter, conf.Options.MongoSslRootCaFile) if err != nil { return err } LOG.Info(&quot;all namespace: %v&quot;, nsSet) var ckptMap map[string]utils.TimestampNode if conf.Options.SpecialSourceDBFlag != utils.VarSpecialSourceDBFlagAliyunServerless &amp;&amp; len(coordinator.MongoD) &gt; 0 { // get current newest timestamp ckptMap, err = getTimestampMap(coordinator.MongoD, conf.Options.MongoSslRootCaFile) if err != nil { return err } } // create target client toUrl := conf.Options.TunnelAddress[0] var toConn *utils.MongoCommunityConn if !conf.Options.FullSyncExecutorDebug { if toConn, err = utils.NewMongoCommunityConn(toUrl, utils.VarMongoConnectModePrimary, true, utils.ReadWriteConcernLocal, utils.ReadWriteConcernDefault, conf.Options.TunnelMongoSslRootCaFile); err != nil { return err } defer toConn.Close() } // create namespace transform trans := transform.NewNamespaceTransform(conf.Options.TransformNamespace) // drop target collection if possible if err := docsyncer.StartDropDestCollection(nsSet, toConn, trans); err != nil { return err } // enable shard if sharding -&gt; sharding shardingSync := docsyncer.IsShardingToSharding(fromIsSharding, toConn) if shardingSync { var connString string if len(conf.Options.MongoSUrl) &gt; 0 { connString = conf.Options.MongoSUrl } else { connString = conf.Options.MongoCsUrl } if err := docsyncer.StartNamespaceSpecSyncForSharding(connString, toConn, trans); err != nil { return err } } // 同步所有索引数据 // fetch all indexes var indexMap map[utils.NS][]bson.D // Ns即namespace, 说白了就是 f'{database}.{collection}' if conf.Options.FullSyncCreateIndex != utils.VarFullSyncCreateIndexNone { if indexMap, err = fetchIndexes(coordinator.RealSourceFullSync, filterList.IterateFilter); err != nil { return fmt.Errorf(&quot;fetch index failed[%v]&quot;, err) } // print LOG.Info(&quot;index list below: ----------&quot;) for ns, index := range indexMap { // LOG.Info(&quot;collection[%v] -&gt; %s&quot;, ns, utils.MarshalStruct(index)) LOG.Info(&quot;collection[%v] -&gt; %v&quot;, ns, index) } LOG.Info(&quot;index list above: ----------&quot;) // 后台索引, 就是简单的查询源库getIndexes, 然后在目标库createIndex if conf.Options.FullSyncCreateIndex == utils.VarFullSyncCreateIndexBackground { if err := docsyncer.StartIndexSync(indexMap, toUrl, trans, true); err != nil { return fmt.Errorf(&quot;create background index failed[%v]&quot;, err) } } } // global qps limit, all dbsyncer share only 1 Qos qos := utils.StartQoS(0, int64(conf.Options.FullSyncReaderDocumentBatchSize), &amp;utils.FullSentinelOptions.TPS) // 同步db数据, 配置文件填了几个源地址就开几条线程(mongos只算一个地址即一条线程, 多线程是针对mongod的情况) // start sync each db var wg sync.WaitGroup var replError error for i, src := range coordinator.RealSourceFullSync { var orphanFilter *filter.OrphanFilter if conf.Options.FullSyncExecutorFilterOrphanDocument &amp;&amp; shardingChunkMap != nil { dbChunkMap := make(sharding.DBChunkMap) if chunkMap, ok := shardingChunkMap[src.ReplicaName]; ok { dbChunkMap = chunkMap } else { LOG.Warn(&quot;document syncer %v has no chunk map&quot;, src.ReplicaName) } orphanFilter = filter.NewOrphanFilter(src.ReplicaName, dbChunkMap) } dbSyncer := docsyncer.NewDBSyncer(i, src.URL, src.ReplicaName, toUrl, trans, orphanFilter, qos, fromIsSharding) dbSyncer.Init() LOG.Info(&quot;document syncer-%d do replication for url=%v&quot;, i, src.URL) wg.Add(1) nimo.GoRoutine(func() { defer wg.Done() if err := dbSyncer.Start(); err != nil { LOG.Critical(&quot;document replication for url=%v failed. %v&quot;, utils.BlockMongoUrlPassword(src.URL, &quot;***&quot;), err) replError = err } dbSyncer.Close() }) } wg.Wait() if replError != nil { return replError } // 完成同步后创建前台索引, 后台索引是同步数据前创建 // create index if == foreground if conf.Options.FullSyncCreateIndex == utils.VarFullSyncCreateIndexForeground { if err := docsyncer.StartIndexSync(indexMap, toUrl, trans, false); err != nil { return fmt.Errorf(&quot;create forground index failed[%v]&quot;, err) } } // update checkpoint after full sync // do not update checkpoint when source is &quot;aliyun_serverless&quot; if conf.Options.SyncMode != utils.VarSyncModeFull &amp;&amp; conf.Options.SpecialSourceDBFlag != utils.VarSpecialSourceDBFlagAliyunServerless { // need merge to one when from mongos and fetch_mothod==&quot;change_stream&quot; if coordinator.MongoS != nil &amp;&amp; conf.Options.IncrSyncMongoFetchMethod == utils.VarIncrSyncMongoFetchMethodChangeStream { var smallestNew int64 = math.MaxInt64 for _, val := range ckptMap { if smallestNew &gt; val.Newest { smallestNew = val.Newest } } ckptMap = map[string]utils.TimestampNode{ coordinator.MongoS.ReplicaName: { Newest: smallestNew, }, } } /* eg: map[map[shard1_servers:{Oldest:7329804871119405057 Newest:7431129274255409154} shard2_servers:{Oldest:7398473718780919810 Newest:7431129274255409153} shard3_servers:{Oldest:7417453196441813035 Newest:7431129278550376449}]] 这个长数字其实时间戳, 只不过是 `ts &lt;&lt; 32 + cnt`, oplog就是这样的格式的 */ LOG.Info(&quot;try to set checkpoint with map[%v]&quot;, ckptMap) if err := docsyncer.Checkpoint(ckptMap); err != nil { return err } } LOG.Info(&quot;document syncer sync end&quot;) return nil} 全量同步的核心: dbSyncer dbSyncer负责单个db的全量同步工作, 同步逻辑如下的Start: 将同步任务细分为nsList, ns即namespace, 其实就是collection 开n条线程, n=conf.Options.FullSyncReaderCollectionParallel 将上面细分的任务nsList丢给这些线程去运行, 即1个collection的同步是一个任务单元, 所以多个collection是做到了并行同步的 任务单元的执行逻辑在dbSyncer.collectionSync 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 每个collection为1个任务(nsList), 分配给n条线程执行func (syncer *DBSyncer) Start() (syncError error) { syncer.startTime = time.Now() var wg sync.WaitGroup filterList := filter.NewDocFilterList() // get all namespace (就是f'{database}.{collection}') nsList, _, err := utils.GetDbNamespace(syncer.FromMongoUrl, filterList.IterateFilter, conf.Options.MongoSslRootCaFile) if err != nil { return err } collExecutorParallel := conf.Options.FullSyncReaderCollectionParallel namespaces := make(chan utils.NS, collExecutorParallel) wg.Add(len(nsList)) nimo.GoRoutine(func() { for _, ns := range nsList { namespaces &lt;- ns } }) // run collection sync in parallel var nsDoneCount int32 = 0 for i := 0; i &lt; collExecutorParallel; i++ { collExecutorId := GenerateCollExecutorId() nimo.GoRoutine(func() { for { ns, ok := &lt;-namespaces if !ok { break } // 每个ns本质就是一个collection toNS := utils.NewNS(syncer.nsTrans.Transform(ns.Str())) LOG.Info(&quot;%s collExecutor-%d sync ns %v to %v begin&quot;, syncer, collExecutorId, ns, toNS) err := syncer.collectionSync(collExecutorId, ns, toNS) // from collection to collection atomic.AddInt32(&amp;nsDoneCount, 1) if err != nil { LOG.Critical(&quot;%s collExecutor-%d sync ns %v to %v failed. %v&quot;, syncer, collExecutorId, ns, toNS, err) syncError = fmt.Errorf(&quot;document syncer sync ns %v to %v failed. %v&quot;, ns, toNS, err) } else { process := int(atomic.LoadInt32(&amp;nsDoneCount)) * 100 / len(nsList) LOG.Info(&quot;%s collExecutor-%d sync ns %v to %v successful. db syncer-%d progress %v%%&quot;, syncer, collExecutorId, ns, toNS, syncer.id, process) } wg.Done() } LOG.Info(&quot;%s collExecutor-%d finish&quot;, syncer, collExecutorId) }) } wg.Wait() close(namespaces) return syncError} collectionSync的逻辑如下, 在collection这个粒度下还在splitter中做了细分任务, 这个是根据splitKeys去做分割的, 一般很少用到 splitter负责从源读数据, 数据reader入队到splitter.readerChan splitSync那上面出队的reader进行读数据, 这里就是读数据的终点了 读到的真实数据(BSON)会交给colExecutor去同步给目标 colExecutor也是一个生产-消费模型, 上面读到的数据会推给colExecutor.docBatch (chan []*bson.Raw), 在colExecutor.Start()中处理队列数据 colExecutor又又又细分了DocExecutor, DocExecutor拿到docBatch里面的数据后, 才真正的进行同步exec.doSync(docs), 这里就是写数据的终点了 写数据用的BulkWrite去批量写入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163// 单个collection的全量同步, splitter负责从FromMongoUrl-ns读, colExecutor负责往ToMongoUrl-toNS写// start sync single collectionfunc (syncer *DBSyncer) collectionSync(collExecutorId int, ns utils.NS, toNS utils.NS) error { // writer colExecutor := NewCollectionExecutor(collExecutorId, syncer.ToMongoUrl, toNS, syncer, conf.Options.TunnelMongoSslRootCaFile) if err := colExecutor.Start(); err != nil { return fmt.Errorf(&quot;start collectionSync failed: %v&quot;, err) } // splitter reader: splitter.Run()会生成n个reader(DocumentReader), 往splitter.readerChan灌; // 每个reader相当于一个任务, 在下面走多线程执行, 属于多个reader往一个writer(colExecutor)写的模式; // 全量数据可以放心并发无序写入; splitter := NewDocumentSplitter(syncer.FromMongoUrl, conf.Options.MongoSslRootCaFile, ns) if splitter == nil { return fmt.Errorf(&quot;create splitter failed&quot;) } defer splitter.Close() // run in several pieces var wg sync.WaitGroup wg.Add(conf.Options.FullSyncReaderParallelThread) for i := 0; i &lt; conf.Options.FullSyncReaderParallelThread; i++ { go func() { defer wg.Done() for { reader, ok := &lt;-splitter.readerChan if !ok || reader == nil { break } // 从reader里面读取docs, 然后把bson结果往chan colExecutor.docBatch灌 if err := syncer.splitSync(reader, colExecutor, collectionMetric); err != nil { LOG.Crashf(&quot;%v&quot;, err) } } }() } wg.Wait() LOG.Info(&quot;%s all readers finish, wait all writers finish&quot;, syncer) // close writer if err := colExecutor.Wait(); err != nil { return fmt.Errorf(&quot;close writer failed: %v&quot;, err) } // set collection finish collectionMetric.CollectionStatus = StatusFinish return nil}// 从reader读docs数据, 然后写到colExecutorfunc (syncer *DBSyncer) splitSync(reader *DocumentReader, colExecutor *CollectionExecutor, collectionMetric *CollectionMetric) error { bufferSize := conf.Options.FullSyncReaderDocumentBatchSize buffer := make([]*bson.Raw, 0, bufferSize) bufferByteSize := 0 for { // 里面就是docCursor.Next(), 只是做了层简单的封装 doc, err := reader.NextDoc() if err != nil { return fmt.Errorf(&quot;splitter reader[%v] get next document failed: %v&quot;, reader, err) } else if doc == nil { atomic.AddUint64(&amp;collectionMetric.FinishCount, uint64(len(buffer))) colExecutor.Sync(buffer) // colExecutor.docBatch &lt;- buffer syncer.replMetric.AddSuccess(uint64(len(buffer))) // only used to calculate the tps which is extract from &quot;success&quot; break } syncer.replMetric.AddGet(1) if bufferByteSize+len(doc) &gt; MAX_BUFFER_BYTE_SIZE || len(buffer) &gt;= bufferSize { atomic.AddUint64(&amp;collectionMetric.FinishCount, uint64(len(buffer))) colExecutor.Sync(buffer) syncer.replMetric.AddSuccess(uint64(len(buffer))) // only used to calculate the tps which is extract from &quot;success&quot; buffer = make([]*bson.Raw, 0, bufferSize) bufferByteSize = 0 } // transform dbref for document if len(conf.Options.TransformNamespace) &gt; 0 &amp;&amp; conf.Options.IncrSyncDBRef { var docData bson.D if err := bson.Unmarshal(doc, &amp;docData); err != nil { LOG.Error(&quot;splitter reader[%v] do bson unmarshal %v failed. %v&quot;, reader, doc, err) } else { docData = transform.TransformDBRef(docData, reader.ns.Database, syncer.nsTrans) if v, err := bson.Marshal(docData); err != nil { LOG.Warn(&quot;splitter reader[%v] do bson marshal %v failed. %v&quot;, reader, docData, err) } else { doc = v } } } buffer = append(buffer, &amp;doc) bufferByteSize += len(doc) } LOG.Info(&quot;splitter reader finishes: %v&quot;, reader) reader.Close() // reader.CloseMgo() return nil}// 接上面collectionSync的colExecutor.Start()func (colExecutor *CollectionExecutor) Start() error { var err error if !conf.Options.FullSyncExecutorDebug { writeConcern := utils.ReadWriteConcernDefault if conf.Options.FullSyncExecutorMajorityEnable { writeConcern = utils.ReadWriteConcernMajority } if colExecutor.conn, err = utils.NewMongoCommunityConn(colExecutor.mongoUrl, utils.VarMongoConnectModePrimary, true, utils.ReadWriteConcernDefault, writeConcern, colExecutor.sslRootFile); err != nil { return err } } parallel := conf.Options.FullSyncReaderWriteDocumentParallel colExecutor.docBatch = make(chan []*bson.Raw, parallel) executors := make([]*DocExecutor, parallel) for i := 0; i != len(executors); i++ { executors[i] = NewDocExecutor(GenerateDocExecutorId(), colExecutor, colExecutor.conn, colExecutor.syncer) go executors[i].start() } colExecutor.executors = executors return nil}// 接上面的go executors[i].start()func (exec *DocExecutor) start() { if !conf.Options.FullSyncExecutorDebug { defer exec.conn.Close() } for { // 读取源端的数据 docs, ok := &lt;-exec.colExecutor.docBatch if !ok { break } if exec.error == nil { // 里面就是往目标 if err := exec.doSync(docs); err != nil { exec.error = err // since v2.4.11: panic directly if meets error LOG.Crashf(&quot;%s sync failed: %v&quot;, exec, err) } } exec.colExecutor.wg.Done() // atomic.AddInt64(&amp;exec.colExecutor.batchCount, -1) }}// 终于到头了, 这里便是连接目标库进行BulkWritefunc (exec *DocExecutor) doSync(docs []*bson.Raw) error { // 省略大量逻辑 opts := options.BulkWrite().SetOrdered(false) res, err := exec.conn.Client.Database(ns.Database).Collection(ns.Collection).BulkWrite(nil, models, opts) // 省略大量逻辑} 看完了全量同步流程, 很明显有个缺陷, 全量同步期间源端有数据更新, 两边数据就不一致了, 当然这个限制条件官方也是有指出的 三. 增量同步 增量同步逻辑在OplogSyncer OplogSyncer和DbSyncer类似 多线程同步, 线程数取决源数据地址和架构 mongos只算一个地址即一条线程, 多线程是针对mongod的情况 一条线程对应一个OplogSyncer, 负责单个db实例的增量同步工作, 所以通常情况下只有1个OplogSyncer 依旧是生产-消费模型 worker是消费者, Batcher是生产者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// collector/coordinator/incr.go// 增量同步func (coordinator *ReplicationCoordinator) startOplogReplication(oplogStartPosition interface{}, fullSyncFinishPosition int64, startTsMap map[string]int64) error { // prepare all syncer. only one syncer while source is ReplicaSet // otherwise one syncer connects to one shard LOG.Info(&quot;start incr replication&quot;) for i, src := range coordinator.RealSourceIncrSync { var syncerTs interface{} if val, ok := oplogStartPosition.(int64); ok &amp;&amp; val == 0 { if v, ok := startTsMap[src.ReplicaName]; !ok { return fmt.Errorf(&quot;replia[%v] not exists on startTsMap[%v]&quot;, src.ReplicaName, startTsMap) } else { syncerTs = v } } else { syncerTs = oplogStartPosition // fullBeginTs } LOG.Info(&quot;RealSourceIncrSync[%d]: %s, startTimestamp[%v]&quot;, i, src, syncerTs) syncer := collector.NewOplogSyncer(src.ReplicaName, syncerTs, fullSyncFinishPosition, src.URL, src.Gids) // syncerGroup http api registry syncer.Init() coordinator.syncerGroup = append(coordinator.syncerGroup, syncer) } // set to group 0 as a leader coordinator.syncerGroup[0].SyncGroup = coordinator.syncerGroup // prepare worker routine and bind it to syncer for i := 0; i &lt; conf.Options.IncrSyncWorker; i++ { syncer := coordinator.syncerGroup[i%len(coordinator.syncerGroup)] w := collector.NewWorker(syncer, uint32(i)) if !w.Init() { return errors.New(&quot;worker initialize error&quot;) } w.SetInitSyncFinishTs(fullSyncFinishPosition) syncer.Bind(w) go w.StartWorker() // worker是消费者, Batcher是生产者(在syncer.Start()里面) } for _, syncer := range coordinator.syncerGroup { go syncer.Start() } return nil} 附oplog实例及字段意义 oplog各字段的解析 1234567891011121314{ &quot;ts&quot; : Timestamp(1582277156, 1), // 操作的时间戳，64位表示，高32位是时间戳，低32位是计数累加 &quot;t&quot; : NumberLong(1), // 对应raft协议里面的term，每次发生节点down掉，新节点加入，主从切换，term都会自增 &quot;h&quot; : NumberLong(0), // 操作的全局唯一id的hash结果 &quot;v&quot; : 2, // oplog的版本字段 &quot;op&quot; : &quot;i&quot;, // &quot;i&quot;表示插入，&quot;d&quot;表示删除，&quot;u&quot;表示更新，&quot;c&quot;表示DDL操作，&quot;n&quot;表示心跳 &quot;ns&quot; : &quot;zz.test&quot;, // 命名空间。操作发生在哪个表上面 &quot;ui&quot; : UUID(&quot;20d9f949-cfc7-496e-a80e-32ba633701a8&quot;), // 表的uuid &quot;wall&quot; : ISODate(&quot;2020-02-21T09:25:56.570Z&quot;), &quot;o&quot; : { // 具体的操作指令字段, 跟&quot;op&quot;一对 &quot;_id&quot; : ObjectId(&quot;5e4fa224a6717632d6ee2e85&quot;), &quot;kick&quot; : 1 }}","link":"/2023/04/09/mongoShake%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"OTP动态口令及底层实现","text":"背景 最近用到了OTP, 遂mark一下 我们常用的那种倒计时验证码就是TOTP, 既不是叫OTP也不是叫MFA, 经常听有人这么说所以提一嘴 OTP 动态口令验证可以看作是服务端和客户端之间通过约定相同的算法来实现验证功能, 也即你在客户端看到的动态口令是客户端通过算法生成的无需请求服务端获取 TOTP 平时用的google动态口令用的就是TOTP(Time-based One-Time Password), TOTP基于HOTP 原理: 假设用的是30秒间隔的六位口令, 精简版伪代码:12345678// secret为密码, timestamp为时间戳, 返回口令GetOTPCode(secret, timestamp) { hs = hmac(secret, timestamp/30) // hsToInt是对hs这个[]byte进行各种&amp;与偏移操作然后转为int intHs = hsToInt(hs) code = intHs % 1000000 return code} HOTP TOTP本质就是一个套皮HOTP, 核心是HOTP 我们从TOTP的视角来理解HOTP做了什么, 见代码和注释:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647type HOTP struct { secret []byte // 密钥, 每个人都有属于自己的一串密钥 digits int // 验证码的位数}/* counter就是`当前时间戳/设定的倒计时长`, return的就是平时看到的那串数字(通常为6位); eg: 假设设定的倒计时长为60: counter为(1600000000 / 60) 返回 519365 counter为((1600000000 + 5) / 60) 返回 519365 counter为((1600000000 + 60) / 60)返回 797425*/func (h HOTP) At(counter uint64) string { counterBytes := make([]byte, 8) binary.BigEndian.PutUint64(counterBytes, counter) hash := hmac.New(sha1.New, h.secret) hash.Write(counterBytes) hs := hash.Sum(nil) offset := hs[19] &amp; 0x0f binCodeBytes := make([]byte, 4) binCodeBytes[0] = hs[offset] &amp; 0x7f binCodeBytes[1] = hs[offset+1] &amp; 0xff binCodeBytes[2] = hs[offset+2] &amp; 0xff binCodeBytes[3] = hs[offset+3] &amp; 0xff binCode := binary.BigEndian.Uint32(binCodeBytes) mod := uint32(1) for i := 0; i &lt; h.digits; i++ { mod *= 10 } code := binCode % mod codeString := strconv.FormatUint(uint64(code), 10) if len(codeString) &lt; h.digits { paddingByteLength := h.digits - len(codeString) paddingBytes := make([]byte, paddingByteLength) for i := 0; i &lt; paddingByteLength; i++ { paddingBytes[i] = '0' } codeString = string(paddingBytes) + codeString } return codeString}// 验证阶段就简单了, 就是拿用户的输入code和上面的At结果对比func (h HOTP) Verify(code string, counter uint64) bool { return h.At(counter) == code} 库 golang-gootp python-pyotp 参考 RFC 4226 (HOTP) RFC 6238 (TOTP) 动态令牌-(OTP,HOTP,TOTP)-基本原理","link":"/2020/06/06/otp/"},{"title":"Py小玩具-绘图转线稿","text":"背景 临摹大佬的作品的时候丰富的颜色会反而产生莫名的干扰, 所以希望能转成干净的线稿! PS的滤镜效果不理想, 也找不到其他合适的工具, 故自己撸一个试试 源码 https://github.com/shuoGG1239/Paint2Sketch 版本一(分支:no-ml) 原理就是直接用opencv各种骚操作, 效果如下图 右下角几个滑条用于调参, 参数主要有canny卷积核和开闭操作的几个阈值参数 效果就是这样…参数不管怎么调都很辣鸡 版本二(分支:master) 版本一的效果太辣鸡, 没啥思路, 弃坑搁置了一年; 无意瞎逛逛到了一个lllyasviel/sketchKeras的项目, 看似效果不错? lllyasviel/sketchKeras是走了神经网络的路线, 开源了test_code和训练好的mod, 总之先抄过来试试 效果如下: 牛逼!机器学习逆天改命! 使用 如果想用的话直接clone下来, 再把release的mod.h5下载完丢根目录, 跑main.py就行了","link":"/2019/06/30/paint2sketch/"},{"title":"Py小玩具-MarkDown图床助手","text":"简介 现在markdown越用越频繁了, md好用是好用, 但就是贴图片的时候有些麻烦: 要截图-&gt;上传图片-&gt;复制图片url, 于是做了个简单的工具: 截图-&gt;传图-&gt;生成图片url三合一, 三…三分之一倍的快乐呀! 效果 截图上传(也可以用快捷键ctrl+shift+alt+F8) 上传后自动将图片url复制到剪贴板, 直接粘贴即可 也可以选择图片上传 也可以直接拖到框中上传 相关地址 exe下载: https://github.com/shuoGG1239/PicPong/releases 源码仓库: https://github.com/shuoGG1239/PicPong 技术相关 纯pyQt5开发 图床是用了sm.ms, 简单好用 后期也许会加一些其他的图床吧(也许…)","link":"/2018/10/25/picpong/"},{"title":"go-爬点Pixiv画师图","text":"背景 刚好需要某个画师的插画, 故写了个简单无需登录的爬图工具 根据Pixiv画师ID, 爬完直接保存在当前目录的 使用1234567// Fetch完后自动建一个画师ID的目录然后图片就放里面cli := PixivClient{ Cli: http.Client{}, Proxy: defaultProxy(), // 不需要代理则nil即可}ctx, _ := context.WithTimeout(context.Background(), time.Second*600)cli.Fetch(ctx, &quot;4462&quot;, -1) // 画师ID:4462 完整实现 代码很简单, 就100多行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166package pixivimport ( &quot;context&quot; &quot;encoding/json&quot; &quot;fmt&quot; &quot;golang.org/x/net/proxy&quot; &quot;io&quot; &quot;io/ioutil&quot; &quot;math/rand&quot; &quot;net/http&quot; &quot;os&quot; &quot;strconv&quot; &quot;strings&quot; &quot;sync&quot; &quot;time&quot;)type PixivClient struct { Cli http.Client Proxy proxy.Dialer // 不为nil则走proxy RandSleep bool // 随机sleep, 防止爬太快}// limit为-1则无限func (pcli PixivClient) Fetch(ctx context.Context, userId string, limit int) { wg := sync.WaitGroup{} tasksDone := make(chan struct{}) illusts, err := pcli.ListIllustByUser(userId) if err != nil { return } for i, illust := range illusts { if i &gt; limit &amp;&amp; limit != -1 { break } wg.Add(1) go func(imgUrl, imgId string) { defer wg.Done() if pcli.RandSleep { randSleep(time.Second * 5) } err := pcli.fetchOne(userId, imgUrl, imgId) if err != nil { fmt.Printf(&quot;fetchOne(%s, %s): %s\\n&quot;, userId, imgId, err.Error()) return } fmt.Printf(&quot;%s finish!\\n&quot;, imgId) }(illust.ImageUrls.Large, strconv.Itoa(illust.Id)) } go func() { wg.Wait() tasksDone &lt;- struct{}{} }() select { case &lt;-tasksDone: case &lt;-ctx.Done(): fmt.Println(ctx.Err()) }}type illustInfo struct { Id int `json:&quot;id&quot;` ImageUrls struct { Large string `json:&quot;large&quot;` } `json:&quot;image_urls&quot;`}func (pcli PixivClient) ListIllustByUser(userId string) ([]illustInfo, error) { type pagination struct { Pages int `json:&quot;pages&quot;` Current int `json:&quot;current&quot;` PerPage int `json:&quot;per_page&quot;` Total int `json:&quot;total&quot;` } type imjad struct { Response []illustInfo `json:&quot;response&quot;` Pagination pagination `json:&quot;pagination&quot;` } pcli.Cli.Transport = nil // 国内的api无需代理 getFn := func(pageNo int) (*imjad, error) { u := fmt.Sprintf(&quot;https://api.imjad.cn/pixiv/v1/?type=member_illust&amp;id=%s&amp;page=%d&quot;, userId, pageNo) req, _ := http.NewRequest(&quot;GET&quot;, u, nil) req.Header.Set(&quot;user-agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&quot;) resp, err := pcli.Cli.Do(req) if err != nil { return nil, err } data, err := ioutil.ReadAll(resp.Body) if err != nil { return nil, err } var im imjad err = json.Unmarshal(data, &amp;im) if err != nil { return nil, err } return &amp;im, nil } im, err := getFn(1) if err != nil { return nil, err } illusts := make([]illustInfo, 0) for p := 1; p &lt;= im.Pagination.Pages; p++ { im, err := getFn(p) if err != nil { return nil, err } illusts = append(illusts, im.Response...) } return illusts, nil}func (pcli PixivClient) fetchOne(userId, imgUrl, imgId string) error { u := imgUrl cli := pcli.Cli if pcli.Proxy != nil { cli.Transport = pcli.socks5Transport() } req, _ := http.NewRequest(&quot;GET&quot;, u, nil) req.Header.Set(&quot;user-agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&quot;) req.Header.Set(&quot;Referer&quot;, &quot;http://www.pixiv.net/&quot;) resp, err := cli.Do(req) if err != nil { return err } _ = os.Mkdir(userId, os.ModePerm) idx := strings.LastIndexFunc(imgUrl, func(r rune) bool { if r == '/' { return true } return false }) f, err := os.Create(fmt.Sprintf(&quot;%s/%s&quot;, userId, imgUrl[idx+1:])) if err != nil { return err } defer func() { _ = f.Close() }() if _, err = io.Copy(f, resp.Body); err != nil { return err } return nil}func (pcli PixivClient) socks5Transport() *http.Transport { httpTransport := &amp;http.Transport{} httpTransport.Dial = pcli.Proxy.Dial return httpTransport}func randSleep(max time.Duration) { t := rand.Int63n(int64(max)) time.Sleep(time.Duration(t))}func defaultProxy() proxy.Dialer { d, err := proxy.SOCKS5(&quot;tcp&quot;, &quot;127.0.0.1:1080&quot;, nil, proxy.Direct) if err != nil { panic(err) } return d}","link":"/2020/06/21/pixiv_spider/"},{"title":"总结Pyinstaller的坑及终极解决方法","text":"一. 首先要有个稳定环境 下面是博主经测试的觉得坑比较少的环境搭配 Python3.4 + PyQt5.4 + Pyinstaller3.2.1 Python3.5 + PyQt5.8 + Pyinstaller3.2.1 二. Pyinstaller遇到坑没必要换打包工具 博主好几次用Pyinstaller遇到坑时都有考虑换工具如py2exe或cx-freeze之类的, 依旧无法解决 (最后还是用pyinstaller解决了) 所以没必要换其他工具, pyinstaller就够了 三. 坑1: 打包不了, 连exe都生成不出来解决方法 直接换Pyinstaller的版本, 即卸掉重装, 推荐用3.2.1 四. 坑2: exe生成了, 但是跑不了 大多数情况都是被坑在这里 解决方法 遇到这种问题不管弹出什么样的错误提示, 在输出exe时参数加个’-d’即debug模式, 然后打开的时候能看到打印的错误信息了, 这招很好用 留意一下程序依赖的一些资源文件, 检查下路径是否正确, 特别是程序里有相对路径的; 还有一些涉及到依赖系统默认资源的如默认字体啥的, 也得留意 换下打包方式, 如onefile模式和onedir模式 (之前出现过onedir打包可以但onefile打包不行的情况) 环境变量PATH中加上PyQt5的plugins的路径 依旧不行则换个Pyinstaller的版本, 即卸掉重装, 推荐用3.2.1 再不行则换操作系统试试, 有win10跑得了但到了win7就跑不了的情况 (弄个虚拟机测下找下问题在哪) 五. 错误码集锦main return -1 这种错误基本都是自己的问题, 只能在输出exe时参数加个’-d’即debug模式, 然后再查下打印的错误信息 Failed to execute script pyi_rth_pkgres 可以先换Pyinstaller的版本, 这个错误会消失, 但会弹出其他的错误信息, 然并卵 这种错误基本都是自己的问题, 只能在输出exe时参数加个’-d’即debug模式, 然后再查下打印的错误信息 Failed to execute script xxxx 这种错误基本都是自己的问题, 只能在输出exe时参数加个’-d’即debug模式, 然后再查下打印的错误信息 This application failed to start … Qt platform plugin … 这种错误先配下PyQt5的plugins的环境变量, 如博主的是C:\\Python34\\Lib\\site-packages\\PyQt5\\plugins 不行再换Pyinstaller的版本 (貌似3.0.0这个版本有问题, 后来换3.2.1就没事了)","link":"/2018/07/06/pyinstaller_keng/"},{"title":"快速美化PyQt应用--QCandyUi","text":"QCandy-UI 快速美化PyQt应用 项目地址: https://github.com/shuoGG1239/QCandyUi 使用方法 pip install QCandyUi 仅需在需要美化的窗口类上加上@colorful装饰器即可 也可以调用CandyWindow.creatWindow()返回经美化的QWidget (推荐用这种) 实例 原味窗口 12345678910# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyclass TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝绿色主题的窗口(使用@colorful) 12345678910111213# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyfrom QCandyUi.CandyWindow import colorful@colorful('blueGreen')class TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝色主题的窗口(使用@colorful) 12345678910111213# 窗口类为TcpUdpSerialPortTool# TcpUdpSerialPortTool.pyfrom QCandyUi.CandyWindow import colorful@colorful('blue')class TcpUdpSerialPortTool(QWidget): ... ...# main.pyapp = QApplication(sys.argv)mainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow.show()sys.exit(app.exec_()) 加了蓝色主题的窗口(使用CandyWindow.createWindow) 12345from QCandyUi import CandyWindowmainWindow = TcpUdpSerialportTool.TcpUdpSerialPortTool()mainWindow = CandyWindow.createWindow(mainWindow, 'blue')mainWindow.show() Ps: 想自己新增颜色主题可以在theme.json里面配, 按照theme.json里的格式配即可 py模块的安装包在/python-version/dist中","link":"/2018/07/11/qcandyui/"},{"title":"Redis数据迁移:RedisShake源码阅读","text":"简述 近期用到了RedisShake做数据迁移, 源码代码量不多于是看了一遍, 本篇为阅读源码的笔记 本篇重点讲解RedisShake的数据迁移功能, 其他几个功能dump,decode,restore,rump只简单提及 原理及架构 原理用一句话概述就是: 假装成源Redis的slave, 利用pSync接收来自源Redis的数据再回放到目标Redis redisShake架构图cluster版 redisShake架构图standalone版 源码信息 源码版本: v1.6 源码仓库: https://github.com/tair-opensource/RedisShake 一. 进程入口:main func main为进程的入口, 核心源码如下: (为了只关注核心逻辑, 无关紧要的代码片段会用// ... + 注释代替) 简单来讲做了3件事情 初始化和校验配置参数 加进程锁 使用了第三方工具github.com/nightlyone/lockfile实现的 简单讲就是在conf.Options.PidPath目录下建一个{conf.Options.Id}.pid文件来实现进程锁; 所以你可以扫描该目录下有多少个pid文件来确认当前有多少RedisShake进程在运行 跑数据迁移 12345678910111213141516171819202122232425262728// main/main.gofunc main() { // ... 各种初始化和校验, 如配置文件, 入参等 // 加进程锁, 进程id为`conf.Options.Id`, 入参时指定 if err = utils.WritePidById(conf.Options.Id, conf.Options.PidPath); err != nil { crash(fmt.Sprintf(&quot;write pid failed. %v&quot;, err), -5) } // 根据需求选择对应的命令, 我们这里只关注数据迁移, 所以走run.CmdSync var runner base.Runner switch *tp { case conf.TypeDecode: runner = new(run.CmdDecode) // 见decode.go case conf.TypeRestore: runner = new(run.CmdRestore) // 见restore.go case conf.TypeDump: runner = new(run.CmdDump) // 见dump.go case conf.TypeSync: runner = new(run.CmdSync) // 见sync.go case conf.TypeRump: runner = new(run.CmdRump) // 见rump.go } // 运行迁移任务 runner.Main()} 二. 数据迁移入口: CmdSync 数据迁移的具体实现在 sync.go/CmdSync.Main, 核心源码如下: 简单来讲做了2件事情 建n个dbSyncer, 分配给m条线程 默认情况下n = m = len(SourceAddressList), 即和源地址相关, 但和源架构无关 对于迁移目标为cluster架构, 所有dbSyncer用同一个dst, 即TargetAddressList 对于迁移目标为standalone架构, 用roundRobin将TargetAddressList分配给各个DbSyncer 用dbSyncer完成数据迁移, 所以dbSyncer才是数据迁移的核心, 下小节讲解 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// sync.go// 这个Main()就是上小节的`runner.Main()`func (cmd *CmdSync) Main() { syncChan := make(chan syncNode, total) // dbSyncer是数据迁移的核心, 相当于worker cmd.dbSyncers = make([]*dbSyncer, total) for i, source := range conf.Options.SourceAddressList { var target []string if conf.Options.TargetType == conf.RedisTypeCluster { target = conf.Options.TargetAddressList } else { // round-robin pick pick := utils.PickTargetRoundRobin(len(conf.Options.TargetAddressList)) target = []string{conf.Options.TargetAddressList[pick]} } nd := syncNode{ id: i, source: source, sourcePassword: conf.Options.SourcePasswordRaw, target: target, targetPassword: conf.Options.TargetPasswordRaw, } syncChan &lt;- nd } var wg sync.WaitGroup wg.Add(len(conf.Options.SourceAddressList)) for i := 0; i &lt; int(conf.Options.SourceRdbParallel); i++ { go func() { for { nd, ok := &lt;-syncChan if !ok { break } ds := NewDbSyncer(nd.id, nd.source, nd.sourcePassword, nd.target, nd.targetPassword, conf.Options.HttpProfile+i) cmd.dbSyncers[nd.id] = ds go ds.sync() &lt;-ds.waitFull // 阻塞直至全量同步完成 wg.Done() } }() } wg.Wait() // 阻塞直至全量同步完成 close(syncChan) // 永远阻塞 select {}} 三. 数据迁移核心: dbSyncer 核心方法为3个: sendPSyncCmd: 读取全量+增量数据(pSync), 返回reader syncRDBFile: 从reader读取全量数据, 并同步(restore key) syncCommand: 从reader读取增量数据, 并同步(回放cmd) 这三个核心方法的具体逻辑如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223// sync.go// 不阻塞// 跑Psync, 跑一条全量+增量同步的goroutine, 用于不断读取全量+增量的数据,// 全量和增量同步获取的实体数据最终都会Pipe到piper, 即return的那个pipe.Readerfunc (ds *dbSyncer) sendPSyncCmd(master, auth_type, passwd string, tlsEnable bool) (pipe.Reader, int64) { // 1. 执行pSync c := utils.OpenNetConn(master, auth_type, passwd, tlsEnable) utils.SendPSyncListeningPort(c, conf.Options.HttpProfile) br := bufio.NewReaderSize(c, utils.ReaderBufferSize) bw := bufio.NewWriterSize(c, utils.WriterBufferSize) // 2. 首次pSync获取runid, offset, nsize, 后面的rdb数据会写入bw, 我们只需关注从br读取即可 runid, offset, wait := utils.SendPSyncFullsync(br, bw) ds.targetOffset.Set(offset) var nsize int64 // nsize为rdb的大小 for nsize == 0 { // 获取nsize为耗时操作, 通过wait管道通知获取(变量名换成waitForRdbSize好些) select { case nsize = &lt;-wait: if nsize == 0 { log.Infof(&quot;dbSyncer[%v] +&quot;, ds.id) } case &lt;-time.After(time.Second): log.Infof(&quot;dbSyncer[%v] -&quot;, ds.id) } } // br -&gt; pipew -&gt; piper(这个返回出去,作为ds.syncRDBFile的reader) piper, pipew := pipe.NewSize(utils.ReaderBufferSize) go func() { defer pipew.Close() p := make([]byte, 8192) // 3. 全量: 读取psync的数据直至读了nsize的数据, 最终写到piper(return出去那个) for rdbsize := int(nsize); rdbsize != 0; { // br -&gt; pipew rdbsize -= utils.Iocopy(br, pipew, p, rdbsize) } // 4. 增量: 不断从psync读数据, 最终写到piper(return出去那个), 是一个死循环(除非异常) for { n, err := ds.pSyncPipeCopy(c, br, bw, offset, pipew) // 正常会在这里永远阻塞 if err != nil { log.PanicErrorf(err, &quot;dbSyncer[%v] psync runid = %s, offset = %d, pipe is broken&quot;, ds.id, runid, offset) } // ... 后面是失败重试相关的操作, 这里不展示 } }() return piper, nsize}// 阻塞至全量同步完成// 从reader(这个就是上面sendPSyncCmd返回的reader)读出BinEntry, 再OpenRedisConn(target), 然后RestoreRdbEntry到target的redis节点func (ds *dbSyncer) syncRDBFile(reader *bufio.Reader, target []string, auth_type, passwd string, nsize int64, tlsEnable bool) { pipe := utils.NewRDBLoader(reader, &amp;ds.rbytes, base.RDBPipeSize) wait := make(chan struct{}) go func() { defer close(wait) var wg sync.WaitGroup wg.Add(conf.Options.Parallel) // restore的时候可以并发, 从pipe里面取, 因为是全量的(按key进行restore), 所以没有顺序之分, 可以并发执行 for i := 0; i &lt; conf.Options.Parallel; i++ { go func() { defer wg.Done() c := utils.OpenRedisConn(target, auth_type, passwd, conf.Options.TargetType == conf.RedisTypeCluster, tlsEnable) defer c.Close() var lastdb uint32 = 0 for e := range pipe { // e是BinEntry, 全量同步的单位数据 // filterDB控制src, targetDB控制dst if filter.FilterDB(int(e.DB)) { } else { // 这里执行selectDB选择同步的目标DB // selectDB, 写这么多只是为了防止重复selectDB浪费性能 if conf.Options.TargetDB != -1 { if conf.Options.TargetDB != int(lastdb) { lastdb = uint32(conf.Options.TargetDB) utils.SelectDB(c, uint32(conf.Options.TargetDB)) } } else { // 如果不指定targetDB, 则源DB是啥, targetDB就是啥 if e.DB != lastdb { lastdb = e.DB utils.SelectDB(c, lastdb) } } // 根据BinEntry的Key进行过滤 if filter.FilterKey(string(e.Key)) == true { // key白名单 ds.ignore.Incr() continue } else { slot := int(utils.KeyToSlot(string(e.Key))) if filter.FilterSlot(slot) == true { // slot白名单 ds.ignore.Incr() continue } } utils.RestoreRdbEntry(c, e) // restore 到 target } } }() } wg.Wait() }() // 这会阻塞至&lt;-wait信号, 即全量同步完成 for done := false; !done; { select { case &lt;-wait: done = true case &lt;-time.After(time.Second): } // ... 后面都是统计和打日志逻辑, 这里不展示 }}// 永远阻塞func (ds *dbSyncer) syncCommand(reader *bufio.Reader, target []string, auth_type, passwd string, tlsEnable bool) { c := utils.OpenRedisConnWithTimeout(target, auth_type, passwd, readeTimeout, writeTimeout, isCluster, tlsEnable) defer c.Close() // ... 一大段FakeSlaveOffset相关逻辑, 给不支持pSync的用的, 这里不展示 // ... 一大段统计相关逻辑, 这里不展示 go func() { var ( lastdb int32 = 0 bypass = false isselect = false scmd string argv, newArgv [][]byte err error reject bool ) decoder := redis.NewDecoder(reader) // 1. 读取reader, 解析出cmdDetail, 然后发送到sendBuf for { ignoresentinel:= false ignorecmd := false isselect = false resp := redis.MustDecodeOpt(decoder) // 这里是我精简后的代码 // 根据scmd做一些过滤逻辑, 以及对当scmd为Select db时, 对target db的一些处理 if scmd, argv, err = redis.ParseArgs(resp); err != nil { } else { if scmd != &quot;ping&quot; { if strings.EqualFold(scmd, &quot;select&quot;) { s := string(argv[0]) n, err := strconv.Atoi(s) bypass = filter.FilterDB(n) isselect = true } else if filter.FilterCommands(scmd) { ignorecmd = true } if strings.EqualFold(scmd, &quot;publish&quot;) &amp;&amp; strings.EqualFold(string(argv[0]), &quot;__sentinel__:hello&quot;){ ignoresentinel = true } if bypass || ignorecmd || ignoresentinel{ ds.nbypass.Incr() continue } } newArgv, reject = filter.HandleFilterKeyWithCommand(scmd, argv) if bypass || ignorecmd || reject { continue } } if isselect &amp;&amp; conf.Options.TargetDB != -1 { if conf.Options.TargetDB != int(lastdb) { lastdb = int32(conf.Options.TargetDB) ds.sendBuf &lt;- cmdDetail{Cmd: &quot;SELECT&quot;, Args: [][]byte{[]byte(strconv.FormatInt(int64(lastdb), 10))}} } continue } ds.sendBuf &lt;- cmdDetail{Cmd: scmd, Args: newArgv} } }() // 2. 从sendBuf读取出cmd, 回放到target (默认5000个cmd flush一次) go func() { var noFlushCount uint var cachedSize uint64 for item := range ds.sendBuf { length := len(item.Cmd) data := make([]interface{}, len(item.Args)) for i := range item.Args { data[i] = item.Args[i] length += len(item.Args[i]) } err := c.Send(item.Cmd, data...) // 回放cmd noFlushCount += 1 if noFlushCount &gt;= conf.Options.SenderCount || cachedSize &gt;= conf.Options.SenderSize || len(ds.sendBuf) == 0 { // 5000 ds in a batch err := c.Flush() noFlushCount = 0 cachedSize = 0 } } }() // 3. 永远阻塞, 每1s做一次统计 for lstat := ds.Stat(); ; { time.Sleep(time.Second) nstat := ds.Stat() var b bytes.Buffer fmt.Fprintf(&amp;b, &quot;dbSyncer[%v] sync: &quot;, ds.id) fmt.Fprintf(&amp;b, &quot; +forwardCommands=%-6d&quot;, nstat.forward-lstat.forward) fmt.Fprintf(&amp;b, &quot; +filterCommands=%-6d&quot;, nstat.nbypass-lstat.nbypass) fmt.Fprintf(&amp;b, &quot; +writeBytes=%d&quot;, nstat.wbytes-lstat.wbytes) log.Info(b.String()) lstat = nstat }} 四. 总结 简单来说, 这个迁移原理就是假装为slave (利用pSync), 接收源redis的rdb然后restore到目标redis实现全量同步,然后继续接收源redis的增量cmd然后回放到目标redis实现增量同步","link":"/2022/07/09/redisShake%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Redis源码阅读-事件模型ae","text":"源码文件 src/ae.c 入口函数 src/ae.c下的void aeMain(aeEventLoop *eventLoop)函数; 推荐从这个函数开始阅读1234567891011121314/* * 事件处理器的主循环 */void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { // 如果有需要在事件处理前执行的函数，那么运行它 if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); // 开始处理事件 aeProcessEvents(eventLoop, AE_ALL_EVENTS); }} 我们着重看下aeMain里面aeProcessEvents(eventLoop, AE_ALL_EVENTS)做了什么; 这里我们留意一下里面的aeApiPoll函数, 该函数用于获取可执行的事件, 获取之后在下面的for循环中处理事件, 执行事件处理器 fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask) aeApiPoll函数是ae模块提供的一个接口, 在ae_epoll.c ae_kqueue.c ae_select.c ae_evport.c都做了相应的具体实现, 也是所谓IO多路复用各平台的具体实现, 目的为了兼容不同平台 备注: 也许你会好奇为啥IO多路复用没有iocp的实现难道windows就没人权吗, 其实redis的官方版本是不支持windows的, windows版本在https://github.com/microsoftarchive/redis由微软团队自己维护, 里面就有ae_wsiocp.c即iocp版的实现1234567891011121314151617181920212223242526int aeProcessEvents(aeEventLoop *eventLoop, int flags) { ... ... // 处理文件事件 numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j &lt; numevents; j++) { // 从已就绪数组中获取事件 aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; // 读事件 if (fe-&gt;mask &amp; mask &amp; AE_READABLE) { // rfired 确保读/写事件只能执行其中一个 rfired = 1; fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask); } // 写事件 if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) { if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask); } processed++; } ... ...} 通常说的redis的reactor模型(反应堆)其实说的就是aeMain的大循环中aeProcessEvents做的那些事情: 监听网络连接的FD的文件事件---&gt; 获取事件---&gt; 执行事件回调 剩下具体细节不多赘述, 顺着思路看源码即可 参考 Redis 和 IO 多路复用 redis的事件模型详解(结合Reactor设计模式)","link":"/2020/05/05/redis_ae/"},{"title":"sd1.5微调实践","text":"微调实践 目前sd1.5我主要试了3种微调, dreamBooth, lora, lycoris","link":"/2023/12/23/sd1.5%E5%BE%AE%E8%B0%83%E5%AE%9E%E8%B7%B5/"},{"title":"Py小玩具-Tcp&amp;Udp&amp;串口调试工具","text":"简介 以前捣鼓嵌入式的时候写的, 就是那种网上搜就一大把的UDP+TCP+串口调试工具, 为了方便定制些功能于是自己搞了一个, 平时自己用, 功能没啥问题 效果如下 界面 十分眼熟的那四种模式…Udp+TcpClient+TcpServer+串口通信 代码https://github.com/shuoGG1239/TcpUdpSerialPortTool 补充 纯PyQt开发 界面右下角的AOP功能目的主要用于拦截发送和接收的数据, 然后在脚本中做额外处理, 一般场景用不到可无视…","link":"/2019/03/31/tcpudp_tool/"},{"title":"博客主题icarus样式魔改","text":"简介 这几天把博客主题从next换成了icarus, 官方默认样式不大满意, 我做了些调整, 这里记下调整过程 预览(前者官方, 后者魔改) 小组件调整 自带的小组件太杂了, 这里我只保留了总览, 分类, 标签, 近期文章 修改_config.icarus.yml的widgets, 仅保留type为profile, categories, recent_posts, tags这4个, 其他删掉或注释掉即可 代码样式调整 修改_config.icarus.yml的article/highlight/theme, 默认是用atom-one-light, 我改用vs即vscode的样式 标题样式调整 原大标题字体没加粗处理, 跟文章content看起来不好区分, 所以做了加粗处理 修改include/style/article.styl文件, 在h1到h5项分别加上font-weight: 600, 如下:12345678910111213141516171819202122232425262728293031323334&amp;.article .article-meta, .article-tags color: $text-light .article-meta overflow-x: auto margin-bottom: .5rem .article-more @extend .button.is-light .content word-wrap: break-word font-size: $article-font-size h1 font-weight: 600 font-size: 1.75em h2 font-weight: 600 font-size: 1.5em h3 font-weight: 600 font-size: 1.25em h4 font-weight: 600 font-size: 1.125em h5 font-weight: 600 font-size: 1em 文章宽度和留空区宽度调整 默认的文章宽度太小, 两边的留空区太大, 文章内部就显得拥挤 调整include/style/base.styl的基础变量$gap, 由64px改为32px 12345$gap ?= 32px$tablet ?= 769px$desktop ?= 1088px$widescreen ?= 1280px$fullhd ?= 1472px 调整include/style/responsive.styl里面containter的width, container其实就是文章容器, 这里把默认值2 * $gap改成1 * $gap 12345678910111213+widescreen() .is-1-column .container, .is-2-column .container max-width: $widescreen - 1 * $gap width: $widescreen - 1 * $gap+fullhd() .is-2-column .container max-width: $fullhd - 1 * $gap width: $fullhd - 1 * $gap .is-1-column .container max-width: $desktop - 1 * $gap width: $desktop - 1 * $gap 调整layout/common/widgets.jsx的getColumnSizeClass, 将case 2的is-4-tablet is-4-desktop is-4-widescreen替换为is-2-tablet is-2-desktop is-2-widescreen 123456789function getColumnSizeClass(columnCount) { switch (columnCount) { case 2: return 'is-2-tablet is-2-desktop is-2-widescreen'; case 3: return 'is-3-tablet is-3-desktop is-3-widescreen'; } return '';} 调整layout/layout.jsx, 将'is-8-tablet is-8-desktop is-8-widescreen': columnCount === 2替换为'is-8-tablet is-9-desktop is-9-widescreen': columnCount === 2 12345678&lt;div class={classname({ column: true, 'order-2': true, 'column-main': true, 'is-12': columnCount === 1, 'is-8-tablet is-9-desktop is-9-widescreen': columnCount === 2, 'is-8-tablet is-9-desktop is-6-widescreen': columnCount === 3})} dangerouslySetInnerHTML={{ __html: body }}&gt;&lt;/div&gt; 侧边栏的profile 由于侧边栏被我调窄了, 头像显得太大, 所以这里调下layout/widget/profile.jsx的figure, 将128x128替换为64x641234567891011&lt;div&gt; &lt;figure class=&quot;image is-64x64 mx-auto mb-2&quot;&gt; &lt;img class={'avatar' + (avatarRounded ? ' is-rounded' : '')} src={avatar} alt={author} /&gt; &lt;/figure&gt; {author ? &lt;p class=&quot;title is-size-4 is-block&quot; style={{'line-height': 'inherit'}}&gt;{author}&lt;/p&gt; : null} {authorTitle ? &lt;p class=&quot;is-size-6 is-block&quot;&gt;{authorTitle}&lt;/p&gt; : null} {location ? &lt;p class=&quot;is-size-6 is-flex justify-content-center&quot;&gt; &lt;i class=&quot;fas fa-map-marker-alt mr-1&quot;&gt;&lt;/i&gt; &lt;span&gt;{location}&lt;/span&gt; &lt;/p&gt; : null}&lt;/div&gt;","link":"/2024/11/27/%E5%8D%9A%E5%AE%A2%E4%B8%BB%E9%A2%98icarus%E6%A0%B7%E5%BC%8F%E9%AD%94%E6%94%B9/"},{"title":"梯度下降笔记","text":"梯度下降迭代公式$$\\omega_{t+1} = \\omega_t - \\alpha \\nabla f(\\omega_t)$$ $\\alpha$为学习率 $\\nabla f(\\omega_t)$为梯度 通俗理解梯度下降迭代公式 首先可以确定的是我们的任务是找loss极小值(不是极值而是极小值) 对于f(x)的一阶导可以认为是f(x)随着x的增长而增长的趋势, 所以为了找极小值的x坐标必定和一阶导的反方向来找 在此例子中, $f(x) = x^2-2x+3$, 一阶导$f’(x) = 2x -2$, 在x=2的导数为2, 可以通俗的理解为当x增长1则f(x)会增长2,而我们的任务是找f(x)的极小值, 所以极小值的x坐标必定是跟导数2是反方向;同理在x=0的导数为-2, 即x增长1则f(x)会增长-2, 极小值的x坐标方向确实和导数-2为反向 在这个例子中我们也可以用斜率的投影方向来理解","link":"/2024/01/06/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"tls","slug":"tls","link":"/tags/tls/"},{"name":"grpc","slug":"grpc","link":"/tags/grpc/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"Qt","slug":"Qt","link":"/tags/Qt/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"}],"categories":[{"name":"画画","slug":"画画","link":"/categories/%E7%94%BB%E7%94%BB/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Ai","slug":"Ai","link":"/categories/Ai/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"PC端","slug":"PC端","link":"/categories/PC%E7%AB%AF/"},{"name":"db","slug":"db","link":"/categories/db/"},{"name":"后端","slug":"后端","link":"/categories/%E5%90%8E%E7%AB%AF/"}],"pages":[{"title":"文章分类","text":"","link":"/categories/index.html"},{"title":"文章分类","text":"","link":"/tags/index.html"}]}